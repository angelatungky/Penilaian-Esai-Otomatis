{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10667568,"sourceType":"datasetVersion","datasetId":6606717},{"sourceId":10669607,"sourceType":"datasetVersion","datasetId":6608283},{"sourceId":10448883,"sourceType":"datasetVersion","datasetId":6467689},{"sourceId":250503,"sourceType":"modelInstanceVersion","modelInstanceId":196387,"modelId":218296}],"dockerImageVersionId":30839,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from gensim.models import Word2Vec\n\n# Load pre-trained Word2Vec model\nword2vec_model = Word2Vec.load('/kaggle/input/id-w2v-model/pytorch/default/2/idwiki_word2vec_200_new_lower.model')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-02-09T13:25:13.321010Z","iopub.execute_input":"2025-02-09T13:25:13.321409Z","iopub.status.idle":"2025-02-09T13:25:44.303211Z","shell.execute_reply.started":"2025-02-09T13:25:13.321367Z","shell.execute_reply":"2025-02-09T13:25:44.301984Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"import warnings\nimport numpy as np\nimport pandas as pd\nimport random\nfrom sklearn.model_selection import train_test_split\nfrom gensim.models import Word2Vec\nfrom nltk.tokenize import word_tokenize\nimport nltk","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T13:25:44.304461Z","iopub.execute_input":"2025-02-09T13:25:44.304830Z","iopub.status.idle":"2025-02-09T13:25:45.259955Z","shell.execute_reply.started":"2025-02-09T13:25:44.304801Z","shell.execute_reply":"2025-02-09T13:25:45.258811Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# Ensure required NLTK data is downloaded\nnltk.download('punkt')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T13:25:45.260987Z","iopub.execute_input":"2025-02-09T13:25:45.261298Z","iopub.status.idle":"2025-02-09T13:25:45.428520Z","shell.execute_reply.started":"2025-02-09T13:25:45.261271Z","shell.execute_reply":"2025-02-09T13:25:45.427325Z"}},"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n","output_type":"stream"},{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"# ====== Dataset Loading and Splitting ======\ndef load_indo_dataset(filename):\n    df = pd.read_csv(filename)\n    data = [\n        (row['answer'], row['response'], row['label'] / 5.0)  # Normalize label\n        for _, row in df.iterrows()\n    ]\n    return data","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T13:25:45.431479Z","iopub.execute_input":"2025-02-09T13:25:45.431841Z","iopub.status.idle":"2025-02-09T13:25:45.437205Z","shell.execute_reply.started":"2025-02-09T13:25:45.431812Z","shell.execute_reply":"2025-02-09T13:25:45.436149Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"def split_dataset(data, valid_percentage=0.15, test_percentage=0.15):\n    random.shuffle(data)\n    train_size = int(len(data) * (1 - valid_percentage - test_percentage))\n    valid_size = int(len(data) * valid_percentage)\n    train = data[:train_size]\n    valid = data[train_size:train_size + valid_size]\n    test = data[train_size + valid_size:]\n    return train, valid, test\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T13:25:45.439316Z","iopub.execute_input":"2025-02-09T13:25:45.439746Z","iopub.status.idle":"2025-02-09T13:25:45.457346Z","shell.execute_reply.started":"2025-02-09T13:25:45.439713Z","shell.execute_reply":"2025-02-09T13:25:45.456027Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"import pandas as pd\nimport os\n\n# Save datasets to CSV files\ndef save_splits_to_csv(train_data, val_data, test_data, output_dir):\n    os.makedirs(output_dir, exist_ok=True)\n\n    train_df = pd.DataFrame(train_data, columns=[\"response\", \"answer\", \"label\"])\n    val_df = pd.DataFrame(val_data, columns=[\"response\", \"answer\", \"label\"])\n    test_df = pd.DataFrame(test_data, columns=[\"response\", \"answer\", \"label\"])\n\n    train_df.to_csv(os.path.join(output_dir, \"train_data.csv\"), index=False)\n    val_df.to_csv(os.path.join(output_dir, \"val_data.csv\"), index=False)\n    test_df.to_csv(os.path.join(output_dir, \"test_data.csv\"), index=False)\n\n    print(f\"Data saved to {output_dir} successfully.\")\n\n# Load datasets from CSV files\ndef load_splits_from_csv(output_dir):\n    train_df = pd.read_csv(os.path.join(output_dir, \"train_data.csv\"))\n    val_df = pd.read_csv(os.path.join(output_dir, \"val_data.csv\"))\n    test_df = pd.read_csv(os.path.join(output_dir, \"test_data.csv\"))\n\n    # Convert dataframes back to lists of tuples\n    train_data = list(train_df.itertuples(index=False, name=None))\n    val_data = list(val_df.itertuples(index=False, name=None))\n    test_data = list(test_df.itertuples(index=False, name=None))\n\n    print(f\"Data loaded from {output_dir} successfully.\")\n    return train_data, val_data, test_data","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T13:25:45.458748Z","iopub.execute_input":"2025-02-09T13:25:45.459441Z","iopub.status.idle":"2025-02-09T13:25:45.473387Z","shell.execute_reply.started":"2025-02-09T13:25:45.459403Z","shell.execute_reply":"2025-02-09T13:25:45.471428Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import re\nfrom nltk.tokenize import word_tokenize\n\ndef preprocess_text_indo(text):\n    # Normalize text (convert to lowercase)\n    text = text.lower()\n\n    # Remove new lines and extra spaces\n    text = re.sub(r\"\\s+\", \" \", text.strip())\n\n    # Remove punctuation using regex\n    text = re.sub(r\"[^\\w\\s]\", \"\", text)  # Removes all punctuation but keeps words and spaces\n\n    # Tokenize text\n    tokens = word_tokenize(text)\n\n    return \" \".join(tokens)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T13:25:45.474862Z","iopub.execute_input":"2025-02-09T13:25:45.475248Z","iopub.status.idle":"2025-02-09T13:25:45.497046Z","shell.execute_reply.started":"2025-02-09T13:25:45.475219Z","shell.execute_reply":"2025-02-09T13:25:45.495364Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"def preprocess_data(data):\n    sentence1 = [preprocess_text_indo(item[0]) for item in data]\n    sentence2 = [preprocess_text_indo(item[1]) for item in data]\n    labels = [item[2] for item in data]\n    return sentence1, sentence2, labels","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T13:25:45.498220Z","iopub.execute_input":"2025-02-09T13:25:45.499043Z","iopub.status.idle":"2025-02-09T13:25:45.516816Z","shell.execute_reply.started":"2025-02-09T13:25:45.498999Z","shell.execute_reply":"2025-02-09T13:25:45.515447Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# ====== Load Pre-Trained Word2Vec Model ======\ndef load_pretrained_word_embedding(path):\n    print(f\"Loading pre-trained Word2Vec model from: {path}\")\n    return Word2Vec.load(path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T13:25:45.518005Z","iopub.execute_input":"2025-02-09T13:25:45.518404Z","iopub.status.idle":"2025-02-09T13:25:45.535986Z","shell.execute_reply.started":"2025-02-09T13:25:45.518369Z","shell.execute_reply":"2025-02-09T13:25:45.534889Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"# Define your custom cosine similarity function\ndef cosine_similarity_custom(vec1, vec2):\n    \"\"\"\n    Custom implementation of cosine similarity.\n    \"\"\"\n    dot_product = np.dot(vec1, vec2)\n    norm_vec1 = np.linalg.norm(vec1)\n    norm_vec2 = np.linalg.norm(vec2)\n    \n    if norm_vec1 == 0 or norm_vec2 == 0:  # Handle zero-vector case\n        return 0.0\n    \n    return dot_product / (norm_vec1 * norm_vec2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T13:25:45.537270Z","iopub.execute_input":"2025-02-09T13:25:45.537799Z","iopub.status.idle":"2025-02-09T13:25:45.560422Z","shell.execute_reply.started":"2025-02-09T13:25:45.537742Z","shell.execute_reply":"2025-02-09T13:25:45.559121Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"def average_cosine_similarity(kalimat1, kalimat2, embedding, method=\"w2v\"):\n    \"\"\"\n    Generate sentence features using Word2Vec, FastText.\n    \"\"\"\n    if method in [\"w2v\", \"fast\"]:\n        similarities = []\n        for text1, text2 in zip(kalimat1, kalimat2):\n            tokens1 = [word for word in text1.split() if word in embedding.wv]\n            tokens2 = [word for word in text2.split() if word in embedding.wv]\n            \n            if not tokens1 or not tokens2:\n                similarities.append(0)\n                continue\n            \n            vec1 = np.mean([embedding.wv[word] for word in tokens1], axis=0).reshape(1, -1)\n            vec2 = np.mean([embedding.wv[word] for word in tokens2], axis=0).reshape(1, -1)\n            \n            similarities.append(cosine_similarity_custom(vec1.flatten(), vec2.flatten()))\n        return np.array(similarities).reshape(-1, 1)\n\n    else:\n        raise ValueError(\"Unsupported method.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T13:25:45.561520Z","iopub.execute_input":"2025-02-09T13:25:45.562157Z","iopub.status.idle":"2025-02-09T13:25:45.579301Z","shell.execute_reply.started":"2025-02-09T13:25:45.562116Z","shell.execute_reply":"2025-02-09T13:25:45.578139Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"def word_distance(text_group1, text_group2, embed_model):\n    similarity_scores = []\n    for group1, group2 in zip(text_group1, text_group2):\n        group1_tokens = [word for word in group1.split() if word in embed_model.wv]\n        group2_tokens = [word for word in group2.split() if word in embed_model.wv]\n\n        if not group1_tokens or not group2_tokens:\n            similarity_scores.append(0)\n        else:\n            similarity_scores.append(-embed_model.wv.wmdistance(group1_tokens, group2_tokens))\n\n    return np.array(similarity_scores).reshape(-1, 1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T13:25:45.580468Z","iopub.execute_input":"2025-02-09T13:25:45.580870Z","iopub.status.idle":"2025-02-09T13:25:45.592408Z","shell.execute_reply.started":"2025-02-09T13:25:45.580841Z","shell.execute_reply":"2025-02-09T13:25:45.591220Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"# for SIF\nfrom sklearn.decomposition import TruncatedSVD\n    \ndef eliminate_first_component(matrix):\n    svd_model = TruncatedSVD(n_components=1, random_state=42)\n    svd_model.fit(matrix)\n    principal_component = svd_model.components_\n    return matrix - matrix.dot(principal_component.T) * principal_component\n    \ndef sif_cos(text_group1, text_group2, embed_model, frequency_map, smoothing_factor=0.001):\n    freq_sum = sum(frequency_map.values())\n    all_embeddings = []\n\n    for group1, group2 in zip(text_group1, text_group2):\n        tokens1 = [word for word in group1.split() if word in embed_model.wv]\n        tokens2 = [word for word in group2.split() if word in embed_model.wv]\n\n        if not tokens1 or not tokens2:\n            all_embeddings.extend([np.zeros(embed_model.vector_size), np.zeros(embed_model.vector_size)])\n            continue\n\n        weights1 = [smoothing_factor / (smoothing_factor + frequency_map.get(word, 1e-5) / freq_sum) for word in tokens1]\n        weights2 = [smoothing_factor / (smoothing_factor + frequency_map.get(word, 1e-5) / freq_sum) for word in tokens2]\n\n        embedding1 = np.average([embed_model.wv[word] for word in tokens1], axis=0, weights=weights1)\n        embedding2 = np.average([embed_model.wv[word] for word in tokens2], axis=0, weights=weights2)\n\n        all_embeddings.extend([embedding1, embedding2])\n\n    all_embeddings = np.array(all_embeddings)\n    all_embeddings = eliminate_first_component(all_embeddings)\n\n    similarities = [\n        (\n            np.dot(all_embeddings[i], all_embeddings[i + 1]) /\n            (np.linalg.norm(all_embeddings[i]) * np.linalg.norm(all_embeddings[i + 1]))\n            if np.linalg.norm(all_embeddings[i]) > 0 and np.linalg.norm(all_embeddings[i + 1]) > 0 else 0\n        )\n        for i in range(0, len(all_embeddings), 2)\n    ]\n\n    return np.array(similarities).reshape(-1, 1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T13:25:45.596803Z","iopub.execute_input":"2025-02-09T13:25:45.597179Z","iopub.status.idle":"2025-02-09T13:25:45.692014Z","shell.execute_reply.started":"2025-02-09T13:25:45.597148Z","shell.execute_reply":"2025-02-09T13:25:45.690904Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"def feature_extraction(train_set1, train_set2, val_set1, val_set2, test_set, embed_model, frequency_map, method):\n    if method == \"averageCosine\":\n        train_similarities = average_cosine_similarity(train_set1, train_set2, embed_model)\n        val_similarities = average_cosine_similarity(val_set1, val_set2, embed_model)\n        test_similarities = average_cosine_similarity(test_set[\"sentence1\"], test_set[\"sentence2\"], embed_model)\n    elif method == \"wordDis\":\n        train_similarities = word_distance(train_set1, train_set2, embed_model)\n        val_similarities = word_distance(val_set1, val_set2, embed_model)\n        test_similarities = word_distance(test_set[\"sentence1\"], test_set[\"sentence2\"], embed_model)\n    elif method == \"sifCos\":\n        train_similarities = sif_cos(train_set1, train_set2, embed_model, frequency_map)\n        val_similarities = sif_cos(val_set1, val_set2, embed_model, frequency_map)\n        test_similarities = sif_cos(test_set[\"sentence1\"], test_set[\"sentence2\"], embed_model, frequency_map)\n    else:\n        raise ValueError(f\"Feature extraction method '{method}' is not supported.\")\n\n    return np.array(train_similarities), np.array(val_similarities), np.array(test_similarities)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T13:25:45.693445Z","iopub.execute_input":"2025-02-09T13:25:45.693851Z","iopub.status.idle":"2025-02-09T13:25:45.701388Z","shell.execute_reply.started":"2025-02-09T13:25:45.693805Z","shell.execute_reply":"2025-02-09T13:25:45.699955Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"# ====== Regression Model ======\nclass RegressionModel:\n    def __init__(self, model_type=\"linear\"):\n        if model_type == \"linear\":\n            self.model = self.LinearRegressionCustom()\n        elif model_type == \"svr\":\n            self.model = SVR(kernel=\"linear\")\n        elif model_type == \"rfr\":\n            self.model = self.RandomForestCustom()\n        else:\n            raise ValueError(\"Unsupported model type.\")\n\n    class LinearRegressionCustom:\n        def __init__(self):\n            self.weights = None\n\n        def fit(self, X, y):\n            X = np.c_[np.ones(X.shape[0]), X]  # Add bias term\n            self.weights = np.linalg.pinv(X.T @ X) @ X.T @ y\n        def fit(self, X, y):\n            # Convert y to NumPy and ensure matching rows\n            y = np.array(y)\n            if X.shape[0] != y.shape[0]:\n                raise ValueError(f\"Shape mismatch: X has {X.shape[0]} rows but y has {y.shape[0]} rows.\")\n            \n            X = np.c_[np.ones(X.shape[0]), X]  # Add bias term\n            self.weights = np.linalg.pinv(X.T @ X) @ X.T @ y\n\n        def predict(self, X):\n            X = np.c_[np.ones(X.shape[0]), X]  # Add bias term\n            return X @ self.weights\n\n    class RandomForestCustom:\n        def __init__(self, n_estimators=100, max_depth=None):\n            self.n_estimators = n_estimators\n            self.max_depth = max_depth\n            self.trees = []\n    \n        def fit(self, X, y):\n            from sklearn.tree import DecisionTreeRegressor\n    \n            # Ensure y is a NumPy array\n            y = np.array(y)\n    \n            n_samples = X.shape[0]\n    \n            for _ in range(self.n_estimators):\n                # Ensure indices are integers for proper indexing\n                indices = np.random.choice(range(n_samples), size=n_samples, replace=True)\n                X_sample = X[indices]\n                y_sample = y[indices]\n                tree = DecisionTreeRegressor(max_depth=self.max_depth)\n                tree.fit(X_sample, y_sample)\n                self.trees.append(tree)\n    \n        def predict(self, X):\n            # Aggregate predictions from all trees\n            predictions = np.array([tree.predict(X) for tree in self.trees])\n            return np.mean(predictions, axis=0)\n\n    @staticmethod\n    def mean_squared_error(y_true, y_pred):\n        squared_errors = [(true - pred) ** 2 for true, pred in zip(y_true, y_pred)]\n        return sum(squared_errors) / len(squared_errors)\n\n    @staticmethod\n    def mean_absolute_error(y_true, y_pred):\n        absolute_errors = [abs(true - pred) for true, pred in zip(y_true, y_pred)]\n        return sum(absolute_errors) / len(absolute_errors)\n\n    @staticmethod\n    def pearsonr(x, y):\n        mean_x = sum(x) / len(x)\n        mean_y = sum(y) / len(y)\n        numerator = sum((xi - mean_x) * (yi - mean_y) for xi, yi in zip(x, y))\n        denominator = ((sum((xi - mean_x) ** 2 for xi in x) * sum((yi - mean_y) ** 2 for yi in y)) ** 0.5)\n        return (numerator / denominator if denominator != 0 else 0.0, None)\n\n    def train(self, x_train, y_train):\n        self.model.fit(x_train, y_train)\n\n    def evaluate(self, x, y):\n        predictions = self.model.predict(x)\n        mse = self.mean_squared_error(y, predictions)\n        mae = self.mean_absolute_error(y, predictions)\n        pearson_corr, _ = self.pearsonr(y, predictions)\n        return mse, mae, pearson_corr\n\n    def predict(self, x):\n        return self.model.predict(x)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T13:25:45.702784Z","iopub.execute_input":"2025-02-09T13:25:45.703205Z","iopub.status.idle":"2025-02-09T13:25:45.726567Z","shell.execute_reply.started":"2025-02-09T13:25:45.703158Z","shell.execute_reply":"2025-02-09T13:25:45.725429Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"pip install cvxopt","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T13:25:45.727732Z","iopub.execute_input":"2025-02-09T13:25:45.728075Z","iopub.status.idle":"2025-02-09T13:25:51.327153Z","shell.execute_reply.started":"2025-02-09T13:25:45.728016Z","shell.execute_reply":"2025-02-09T13:25:51.325322Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: cvxopt in /usr/local/lib/python3.10/dist-packages (1.3.2)\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"pip install POT","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T13:25:51.328478Z","iopub.execute_input":"2025-02-09T13:25:51.328960Z","iopub.status.idle":"2025-02-09T13:25:56.430918Z","shell.execute_reply.started":"2025-02-09T13:25:51.328905Z","shell.execute_reply":"2025-02-09T13:25:56.429448Z"}},"outputs":[{"name":"stdout","text":"Collecting POT\n  Downloading POT-0.9.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (34 kB)\nRequirement already satisfied: numpy>=1.16 in /usr/local/lib/python3.10/dist-packages (from POT) (1.26.4)\nRequirement already satisfied: scipy>=1.6 in /usr/local/lib/python3.10/dist-packages (from POT) (1.13.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.16->POT) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.16->POT) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.16->POT) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.16->POT) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.16->POT) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.16->POT) (2.4.1)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.16->POT) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.16->POT) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.16->POT) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.16->POT) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.16->POT) (2024.2.0)\nDownloading POT-0.9.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (865 kB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m865.6/865.6 kB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: POT\nSuccessfully installed POT-0.9.5\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"# ====== Load Dataset ======\n# Load train, validation, and test datasets from CSV files\ntrain_file = \"/kaggle/input/70-indo-dataset/70-train_data.csv\"\nvalid_file = \"/kaggle/input/70-indo-dataset/15-val_data.csv\"\ntest_file = \"/kaggle/input/70-indo-dataset/15-test_data.csv\"\n\n# Read datasets\ntrain_data = pd.read_csv(train_file).values\nvalid_data = pd.read_csv(valid_file).values\ntest_data = pd.read_csv(test_file).values","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T13:25:56.432251Z","iopub.execute_input":"2025-02-09T13:25:56.432605Z","iopub.status.idle":"2025-02-09T13:25:56.499748Z","shell.execute_reply.started":"2025-02-09T13:25:56.432544Z","shell.execute_reply":"2025-02-09T13:25:56.498755Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"# ====== Preprocessing ======\nx_train1, x_train2, y_train = preprocess_data(train_data)\nx_valid1, x_valid2, y_valid = preprocess_data(valid_data)\nx_test1, x_test2, y_test = preprocess_data(test_data)\n\n# Output shapes for verification\nprint(f\"Train data: {len(x_train1)} pairs, {len(y_train)} labels\")\nprint(f\"Validation data: {len(x_valid1)} pairs, {len(y_valid)} labels\")\nprint(f\"Test data: {len(x_test1)} pairs, {len(y_test)} labels\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T13:25:56.500860Z","iopub.execute_input":"2025-02-09T13:25:56.501194Z","iopub.status.idle":"2025-02-09T13:25:57.074511Z","shell.execute_reply.started":"2025-02-09T13:25:56.501167Z","shell.execute_reply":"2025-02-09T13:25:57.073405Z"}},"outputs":[{"name":"stdout","text":"Train data: 1291 pairs, 1291 labels\nValidation data: 276 pairs, 276 labels\nTest data: 278 pairs, 278 labels\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"import pandas as pd\n\n# ====== Frequency Computation for SIF ======\nfrom collections import Counter\nall_sentences = x_train1 + x_train2 + x_valid1 + x_valid2 + x_test1 + x_test2\n# Compute word frequencies for SIF\nall_tokens = [token for sentence in all_sentences for token in sentence.split()]\nfreqs = Counter(all_tokens)\n\n# ====== Feature Extraction and Model Evaluation ======\nmethods = [\"averageCosine\", \"wordDis\", \"sifCos\"]\n# Store results for all methods and models\nresults = []\nword2vec_model_path = '/kaggle/input/id-w2v-model/pytorch/default/2/idwiki_word2vec_200_new_lower.model'\n# Load pre-trained Word2Vec model\nembedding = load_pretrained_word_embedding(word2vec_model_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T13:25:57.075550Z","iopub.execute_input":"2025-02-09T13:25:57.075956Z","iopub.status.idle":"2025-02-09T13:26:01.900269Z","shell.execute_reply.started":"2025-02-09T13:25:57.075927Z","shell.execute_reply":"2025-02-09T13:26:01.899095Z"}},"outputs":[{"name":"stdout","text":"Loading pre-trained Word2Vec model from: /kaggle/input/id-w2v-model/pytorch/default/2/idwiki_word2vec_200_new_lower.model\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"from sklearn.svm import SVR\nimport pickle\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T13:26:01.901290Z","iopub.execute_input":"2025-02-09T13:26:01.901609Z","iopub.status.idle":"2025-02-09T13:26:01.906781Z","shell.execute_reply.started":"2025-02-09T13:26:01.901565Z","shell.execute_reply":"2025-02-09T13:26:01.905328Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"for method in methods:\n    current_method = method\n    print(f\"Using feature extraction method: {current_method}\")\n    \n    x_train_features, x_valid_features, x_test_features = feature_extraction(\n        x_train1, x_train2, x_valid1, x_valid2, {\"sentence1\": x_test1, \"sentence2\": x_test2}, embedding, freqs, current_method\n    )\n\n    for reg_model in [\"linear\", \"svr\", \"rfr\"]:\n        model = RegressionModel(model_type=reg_model)\n        model.train(x_train_features, y_train)\n        # Save trained model\n        model_filename = f\"trained_model_{method}_{reg_model}.pkl\"\n        with open(model_filename, \"wb\") as file:\n            pickle.dump(model, file)\n        print(f\"Saved {reg_model} model using {method} feature extraction: {model_filename}\")\n        print(\"Trained regression model saved successfully!\")\n        # Evaluate on validation and test sets\n        val_mse, val_mae, val_pearson = model.evaluate(x_valid_features, y_valid)\n        test_mse, test_mae, test_pearson = model.evaluate(x_test_features, y_test)\n\n        print(f\"Validation Performance ({current_method}, {reg_model}):\")\n        print(f\"MSE: {val_mse:.4f}, MAE: {val_mae:.4f}, Pearson Correlation: {val_pearson:.4f}\")\n\n        print(f\"Test Performance ({current_method}, {reg_model}):\")\n        print(f\"MSE: {test_mse:.4f}, MAE: {test_mae:.4f}, Pearson Correlation: {test_pearson:.4f}\")\n\n        # Store results\n        results.append({\n            \"method\": current_method,\n            \"model\": reg_model,\n            \"pearson\": test_pearson,\n            \"test_predictions\": model.predict(x_test_features),\n            \"test_features\": x_test_features,\n            \"x_test1\": x_test1,\n            \"x_test2\": x_test2,\n            \"y_test\": y_test\n        })\n\n# Save all results as CSV files (for both validation and test)\nfor i, result in enumerate(results, start=1):\n    # Retrieve original raw sentences for validation and test\n    raw_valid1 = [item[0] for item in valid_data]  # Original raw Sentence 1 for validation\n    raw_valid2 = [item[1] for item in valid_data]  # Original raw Sentence 2 for validation\n    raw_test1 = [item[0] for item in test_data]  # Original raw Sentence 1 for test\n    raw_test2 = [item[1] for item in test_data]  # Original raw Sentence 2 for test\n    val_predictions = model.predict(x_valid_features)  # Prediksi untuk data validasi\n    # Create DataFrame for validation predictions\n    val_df = pd.DataFrame({\n        \"Original Sentence 1\": raw_valid1,  # Append raw sentence 1\n        \"Original Sentence 2\": raw_valid2,  # Append raw sentence 2\n        \"Preprocessed Sentence 1\": x_valid1,  # Preprocessed validation sentence 1\n        \"Preprocessed Sentence 2\": x_valid2,  # Preprocessed validation sentence 2\n        \"True Similarity Score\": [y * 5 for y in y_valid],  # Rescale validation true scores\n        \"Predicted Similarity Score\": [y * 5 for y in val_predictions]  # Validation predictions rescaled\n    })\n\n    # Save validation result CSV\n    val_filename = f\"val_result_{i}_{result['method']}_{result['model']}.csv\"\n    val_df.to_csv(val_filename, index=False)\n    print(f\"Saved validation result: {val_filename}\")\n\n    # Create DataFrame for test predictions\n    test_df = pd.DataFrame({\n        \"Original Sentence 1\": raw_test1,  # Append raw sentence 1\n        \"Original Sentence 2\": raw_test2,  # Append raw sentence 2\n        \"Preprocessed Sentence 1\": result[\"x_test1\"],  # Preprocessed test sentence 1\n        \"Preprocessed Sentence 2\": result[\"x_test2\"],  # Preprocessed test sentence 2\n        \"True Similarity Score\": [y * 5 for y in result[\"y_test\"]],  # Rescale to [0, 5]\n        \"Predicted Similarity Score\": [y * 5 for y in result[\"test_predictions\"]]  # Predicted scores rescaled\n    })\n\n    # Save test result CSV\n    test_filename = f\"test_result_{i}_{result['method']}_{result['model']}.csv\"\n    test_df.to_csv(test_filename, index=False)\n    print(f\"Saved test result: {test_filename}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T13:26:01.908264Z","iopub.execute_input":"2025-02-09T13:26:01.908745Z","iopub.status.idle":"2025-02-09T13:26:27.314039Z","shell.execute_reply.started":"2025-02-09T13:26:01.908699Z","shell.execute_reply":"2025-02-09T13:26:27.312851Z"}},"outputs":[{"name":"stdout","text":"Using feature extraction method: averageCosine\nSaved linear model using averageCosine feature extraction: trained_model_averageCosine_linear.pkl\nTrained regression model saved successfully!\nValidation Performance (averageCosine, linear):\nMSE: 0.0960, MAE: 0.2634, Pearson Correlation: 0.5620\nTest Performance (averageCosine, linear):\nMSE: 0.0828, MAE: 0.2438, Pearson Correlation: 0.6552\nSaved svr model using averageCosine feature extraction: trained_model_averageCosine_svr.pkl\nTrained regression model saved successfully!\nValidation Performance (averageCosine, svr):\nMSE: 0.1082, MAE: 0.2593, Pearson Correlation: 0.5620\nTest Performance (averageCosine, svr):\nMSE: 0.0868, MAE: 0.2297, Pearson Correlation: 0.6552\nSaved rfr model using averageCosine feature extraction: trained_model_averageCosine_rfr.pkl\nTrained regression model saved successfully!\nValidation Performance (averageCosine, rfr):\nMSE: 0.0379, MAE: 0.1219, Pearson Correlation: 0.8542\nTest Performance (averageCosine, rfr):\nMSE: 0.0759, MAE: 0.1970, Pearson Correlation: 0.7040\nUsing feature extraction method: wordDis\nSaved linear model using wordDis feature extraction: trained_model_wordDis_linear.pkl\nTrained regression model saved successfully!\nValidation Performance (wordDis, linear):\nMSE: 0.0513, MAE: 0.1777, Pearson Correlation: 0.7970\nTest Performance (wordDis, linear):\nMSE: 0.0493, MAE: 0.1755, Pearson Correlation: 0.8121\nSaved svr model using wordDis feature extraction: trained_model_wordDis_svr.pkl\nTrained regression model saved successfully!\nValidation Performance (wordDis, svr):\nMSE: 0.0512, MAE: 0.1745, Pearson Correlation: 0.7970\nTest Performance (wordDis, svr):\nMSE: 0.0490, MAE: 0.1717, Pearson Correlation: 0.8121\nSaved rfr model using wordDis feature extraction: trained_model_wordDis_rfr.pkl\nTrained regression model saved successfully!\nValidation Performance (wordDis, rfr):\nMSE: 0.0223, MAE: 0.0995, Pearson Correlation: 0.9169\nTest Performance (wordDis, rfr):\nMSE: 0.0716, MAE: 0.1909, Pearson Correlation: 0.7333\nUsing feature extraction method: sifCos\nSaved linear model using sifCos feature extraction: trained_model_sifCos_linear.pkl\nTrained regression model saved successfully!\nValidation Performance (sifCos, linear):\nMSE: 0.0665, MAE: 0.2050, Pearson Correlation: 0.7263\nTest Performance (sifCos, linear):\nMSE: 0.0625, MAE: 0.1995, Pearson Correlation: 0.7548\nSaved svr model using sifCos feature extraction: trained_model_sifCos_svr.pkl\nTrained regression model saved successfully!\nValidation Performance (sifCos, svr):\nMSE: 0.0664, MAE: 0.2000, Pearson Correlation: 0.7263\nTest Performance (sifCos, svr):\nMSE: 0.0619, MAE: 0.1950, Pearson Correlation: 0.7548\nSaved rfr model using sifCos feature extraction: trained_model_sifCos_rfr.pkl\nTrained regression model saved successfully!\nValidation Performance (sifCos, rfr):\nMSE: 0.0902, MAE: 0.2122, Pearson Correlation: 0.6398\nTest Performance (sifCos, rfr):\nMSE: 0.0880, MAE: 0.2133, Pearson Correlation: 0.6627\nSaved validation result: val_result_1_averageCosine_linear.csv\nSaved test result: test_result_1_averageCosine_linear.csv\nSaved validation result: val_result_2_averageCosine_svr.csv\nSaved test result: test_result_2_averageCosine_svr.csv\nSaved validation result: val_result_3_averageCosine_rfr.csv\nSaved test result: test_result_3_averageCosine_rfr.csv\nSaved validation result: val_result_4_wordDis_linear.csv\nSaved test result: test_result_4_wordDis_linear.csv\nSaved validation result: val_result_5_wordDis_svr.csv\nSaved test result: test_result_5_wordDis_svr.csv\nSaved validation result: val_result_6_wordDis_rfr.csv\nSaved test result: test_result_6_wordDis_rfr.csv\nSaved validation result: val_result_7_sifCos_linear.csv\nSaved test result: test_result_7_sifCos_linear.csv\nSaved validation result: val_result_8_sifCos_svr.csv\nSaved test result: test_result_8_sifCos_svr.csv\nSaved validation result: val_result_9_sifCos_rfr.csv\nSaved test result: test_result_9_sifCos_rfr.csv\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"import pickle\nimport numpy as np\nimport pandas as pd\nimport random\nimport nltk\nfrom gensim.models import Word2Vec\nfrom sklearn.svm import SVR\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error\nfrom scipy.stats import pearsonr\n\n# Ensure required NLTK data is downloaded\nnltk.download('punkt')\n\n# Load pre-trained Word2Vec model\nword2vec_model_path = \"/kaggle/input/id-w2v-model/pytorch/default/2/idwiki_word2vec_200_new_lower.model\"\nembedding = Word2Vec.load(word2vec_model_path)\n\n# Load dataset\ndef load_dataset(filename):\n    df = pd.read_csv(filename)\n    return [(row['answer'], row['response'], row['label'] / 5.0) for _, row in df.iterrows()]\n\n# Preprocessing\ndef preprocess_text_indo(text):\n    text = text.lower()\n    tokens = nltk.word_tokenize(text)\n    tokens = [word for word in tokens if word.isalpha()]\n    return \" \".join(tokens)\n\n# Feature Extraction Methods\ndef avg_cosine_similarity(sentences1, sentences2, embedding):\n    similarities = []\n    for sent1, sent2 in zip(sentences1, sentences2):\n        tokens1 = [token for token in sent1.split() if token in embedding.wv]\n        tokens2 = [token for token in sent2.split() if token in embedding.wv]\n\n        if not tokens1 or not tokens2:\n            similarities.append(0)\n            continue\n\n        vec1 = np.mean([embedding.wv[token] for token in tokens1], axis=0)\n        vec2 = np.mean([embedding.wv[token] for token in tokens2], axis=0)\n\n        similarities.append(np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2)))\n    return np.array(similarities).reshape(-1, 1)\n\ndef word_distance(sentences1, sentences2, embedding):\n    similarities = []\n    for sent1, sent2 in zip(sentences1, sentences2):\n        tokens1 = [word for word in sent1.split() if word in embedding.wv]\n        tokens2 = [word for word in sent2.split() if word in embedding.wv]\n\n        if not tokens1 or not tokens2:\n            similarities.append(0)\n        else:\n            similarities.append(-embedding.wv.wmdistance(tokens1, tokens2))\n    return np.array(similarities).reshape(-1, 1)\n\ndef sif_cos(sentences1, sentences2, embedding):\n    from sklearn.decomposition import TruncatedSVD\n\n    def eliminate_first_component(matrix):\n        svd_model = TruncatedSVD(n_components=1, random_state=42)\n        svd_model.fit(matrix)\n        principal_component = svd_model.components_\n        return matrix - matrix.dot(principal_component.T) * principal_component\n\n    similarities = []\n    all_embeddings = []\n    \n    for sent1, sent2 in zip(sentences1, sentences2):\n        tokens1 = [word for word in sent1.split() if word in embedding.wv]\n        tokens2 = [word for word in sent2.split() if word in embedding.wv]\n\n        if not tokens1 or not tokens2:\n            similarities.append(0)\n            continue\n\n        embedding1 = np.mean([embedding.wv[word] for word in tokens1], axis=0)\n        embedding2 = np.mean([embedding.wv[word] for word in tokens2], axis=0)\n\n        all_embeddings.extend([embedding1, embedding2])\n\n    all_embeddings = np.array(all_embeddings)\n    all_embeddings = eliminate_first_component(all_embeddings)\n\n    for i in range(0, len(all_embeddings), 2):\n        similarities.append(\n            np.dot(all_embeddings[i], all_embeddings[i + 1]) /\n            (np.linalg.norm(all_embeddings[i]) * np.linalg.norm(all_embeddings[i + 1]))\n        )\n    \n    return np.array(similarities).reshape(-1, 1)\n\n# Compute Metrics\ndef compute_metrics(y_true, y_pred):\n    mae = mean_absolute_error(y_true, y_pred)\n    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n    pearson_corr, _ = pearsonr(y_true, y_pred) if len(y_true) > 1 else (0, None)\n\n    print(f\"üìä Evaluation Metrics:\")\n    print(f\"‚úÖ MAE: {mae:.4f}, RMSE: {rmse:.4f}, Pearson Correlation: {pearson_corr:.4f}\")\n    \n    return mae, rmse, pearson_corr\n\n# Predict on Unseen Data\ndef predict_unseen_data(unseen_file_path):\n    unseen_data = pd.read_csv(unseen_file_path)\n\n    # Get true labels (if available)\n    if \"label\" in unseen_data.columns:\n        y_true = unseen_data[\"label\"].values / 5.0\n    else:\n        y_true = None\n\n    unseen_sentence1 = [preprocess_text_indo(item) for item in unseen_data[\"answer\"]]\n    unseen_sentence2 = [preprocess_text_indo(item) for item in unseen_data[\"response\"]]\n\n    results = []\n\n    for method in [\"averageCosine\", \"wordDis\", \"sifCos\"]:\n        if method == \"averageCosine\":\n            unseen_features = avg_cosine_similarity(unseen_sentence1, unseen_sentence2, embedding)\n        elif method == \"wordDis\":\n            unseen_features = word_distance(unseen_sentence1, unseen_sentence2, embedding)\n        elif method == \"sifCos\":\n            unseen_features = sif_cos(unseen_sentence1, unseen_sentence2, embedding)\n\n        for model_type in [\"linear\", \"svr\", \"rfr\"]:\n            model_filename = f\"trained_model_{method}_{model_type}.pkl\"\n\n            with open(model_filename, \"rb\") as file:\n                model = pickle.load(file)\n            \n            predictions = model.predict(unseen_features)\n            predictions_rescaled = predictions * 5  # Scale predictions to [0, 5]\n\n            # Save Predictions\n            df = pd.DataFrame({\n                \"Original Sentence 1\": unseen_data[\"answer\"],\n                \"Original Sentence 2\": unseen_data[\"response\"],\n                \"True Label\": unseen_data[\"label\"] if y_true is not None else \"N/A\",  # Keep original scale\n                \"Predicted Similarity Score\": predictions_rescaled\n            })\n\n            output_filename = f\"unseen_predictions_{method}_{model_type}.csv\"\n            df.to_csv(output_filename, index=False)\n            print(f\"‚úÖ Saved unseen predictions to {output_filename}\")\n\n            # Compute Metrics if True Labels Exist\n            if y_true is not None:\n                mae, rmse, pearson_corr = compute_metrics(y_true * 5, predictions_rescaled)\n                results.append({\n                    \"Method\": method,\n                    \"Model\": model_type,\n                    \"MAE\": mae,\n                    \"RMSE\": rmse,\n                    \"Pearson\": pearson_corr\n                })\n\n    # Save All Metrics to CSV\n    if results:\n        results_df = pd.DataFrame(results)\n        results_df.to_csv(\"unseen_metrics_summary.csv\", index=False)\n        print(\"üìå Saved all evaluation metrics to unseen_metrics_summary.csv\")\n\n# Run predictions and evaluation on Unseen Data\nunseen_file_path = \"/kaggle/input/test-data/test-BuIng.csv\"\npredict_unseen_data(unseen_file_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T13:26:27.315037Z","iopub.execute_input":"2025-02-09T13:26:27.315320Z","iopub.status.idle":"2025-02-09T13:26:32.773527Z","shell.execute_reply.started":"2025-02-09T13:26:27.315291Z","shell.execute_reply":"2025-02-09T13:26:32.772435Z"}},"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n‚úÖ Saved unseen predictions to unseen_predictions_averageCosine_linear.csv\nüìä Evaluation Metrics:\n‚úÖ MAE: 1.2451, RMSE: 1.5673, Pearson Correlation: -0.1286\n‚úÖ Saved unseen predictions to unseen_predictions_averageCosine_svr.csv\nüìä Evaluation Metrics:\n‚úÖ MAE: 1.2809, RMSE: 1.5551, Pearson Correlation: -0.1286\n‚úÖ Saved unseen predictions to unseen_predictions_averageCosine_rfr.csv\nüìä Evaluation Metrics:\n‚úÖ MAE: 2.0692, RMSE: 2.3695, Pearson Correlation: 0.1061\n‚úÖ Saved unseen predictions to unseen_predictions_wordDis_linear.csv\nüìä Evaluation Metrics:\n‚úÖ MAE: 1.9066, RMSE: 2.2281, Pearson Correlation: -0.0211\n‚úÖ Saved unseen predictions to unseen_predictions_wordDis_svr.csv\nüìä Evaluation Metrics:\n‚úÖ MAE: 2.0874, RMSE: 2.3969, Pearson Correlation: -0.0211\n‚úÖ Saved unseen predictions to unseen_predictions_wordDis_rfr.csv\nüìä Evaluation Metrics:\n‚úÖ MAE: 2.1058, RMSE: 2.5782, Pearson Correlation: -0.0916\n‚úÖ Saved unseen predictions to unseen_predictions_sifCos_linear.csv\nüìä Evaluation Metrics:\n‚úÖ MAE: 4.6298, RMSE: 4.7865, Pearson Correlation: 0.1895\n‚úÖ Saved unseen predictions to unseen_predictions_sifCos_svr.csv\nüìä Evaluation Metrics:\n‚úÖ MAE: 5.0137, RMSE: 5.1653, Pearson Correlation: 0.1895\n‚úÖ Saved unseen predictions to unseen_predictions_sifCos_rfr.csv\nüìä Evaluation Metrics:\n‚úÖ MAE: 3.0000, RMSE: 3.2618, Pearson Correlation: -0.1377\nüìå Saved all evaluation metrics to unseen_metrics_summary.csv\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"import pickle\nimport pandas as pd\nimport numpy as np\nimport nltk\nfrom gensim.models import Word2Vec\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error\nfrom scipy.stats import pearsonr\n\n# Load pre-trained Word2Vec model\nword2vec_model_path = \"/kaggle/input/id-w2v-model/pytorch/default/2/idwiki_word2vec_200_new_lower.model\"\nembedding = Word2Vec.load(word2vec_model_path)\n\n# Load trained regression model\nwith open(\"/kaggle/working/trained_model_sifCos_rfr.pkl\", \"rb\") as file:\n    model = pickle.load(file)\n\nprint(\"Trained regression model loaded successfully!\")\n\n\ndef preprocess_text_indo(text):\n    # Normalize text (convert to lowercase)\n    text = text.lower()\n\n    # Remove new lines and extra spaces\n    text = re.sub(r\"\\s+\", \" \", text.strip())\n\n    # Remove punctuation using regex\n    text = re.sub(r\"[^\\w\\s]\", \"\", text)  # Removes all punctuation but keeps words and spaces\n\n    # Tokenize text\n    tokens = word_tokenize(text)\n\n    return \" \".join(tokens)\n\n# ====== Cosine Similarity Feature Extraction ======\ndef avg_cosine_similarity(sentences1, sentences2, embedding):\n    similarities = []\n    for sent1, sent2 in zip(sentences1, sentences2):\n        tokens1 = [token for token in sent1.split() if token in embedding.wv]\n        tokens2 = [token for token in sent2.split() if token in embedding.wv]\n        \n        if not tokens1 or not tokens2:\n            similarities.append(0)\n            continue\n        \n        vec1 = np.mean([embedding.wv[token] for token in tokens1], axis=0).reshape(1, -1)\n        vec2 = np.mean([embedding.wv[token] for token in tokens2], axis=0).reshape(1, -1)\n        \n        similarities.append(np.dot(vec1.flatten(), vec2.flatten()) / \n                            (np.linalg.norm(vec1) * np.linalg.norm(vec2)))\n    return np.array(similarities).reshape(-1, 1)\n\n# ====== Compute Metrics ======\ndef compute_metrics(y_true, y_pred):\n    \"\"\"\n    Compute MAE, RMSE, and Pearson correlation.\n    \"\"\"\n    mae = mean_absolute_error(y_true, y_pred)\n    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n    pearson_corr, _ = pearsonr(y_true, y_pred)\n\n    print(f\"\\nüìä Evaluation Metrics on Unseen Data:\")\n    print(f\"‚úÖ Mean Absolute Error (MAE): {mae:.4f}\")\n    print(f\"‚úÖ Root Mean Squared Error (RMSE): {rmse:.4f}\")\n    print(f\"‚úÖ Pearson Correlation: {pearson_corr:.4f}\")\n\n    return mae, rmse, pearson_corr\n\n# ====== Predict on Unseen Data and Compute Metrics ======\ndef predict_unseen_data(model, embedding, unseen_file_path, output_file):\n    \"\"\"\n    Predict similarity scores on an unseen dataset and compute evaluation metrics.\n    \"\"\"\n    unseen_data = pd.read_csv(unseen_file_path)\n\n    # Check if labels are available for evaluation\n    if \"label\" in unseen_data.columns:\n        y_true = unseen_data[\"label\"].values / 5.0  # Normalize labels to [0, 1]\n    else:\n        y_true = None\n\n    # Preprocess unseen dataset\n    unseen_sentence1 = [preprocess_text_indo(item) for item in unseen_data[\"answer\"]]\n    unseen_sentence2 = [preprocess_text_indo(item) for item in unseen_data[\"response\"]]\n\n    # Extract features using Word2Vec\n    unseen_features = avg_cosine_similarity(unseen_sentence1, unseen_sentence2, embedding)\n\n    # Predict similarity scores\n    predictions = model.predict(unseen_features)\n\n    # Save predictions to CSV\n    df = pd.DataFrame({\n        \"Original Sentence 1\": unseen_data[\"answer\"],\n        \"Original Sentence 2\": unseen_data[\"response\"],\n        \"True Label\": unseen_data[\"label\"] if y_true is not None else \"N/A\",  # Keep original scale\n        \"Predicted Similarity Score\": predictions * 5  # Rescale to [0, 5]\n    })\n\n    # Include true labels if available\n    if y_true is not None:\n        df[\"True Similarity Score\"] = unseen_data[\"label\"]  # Keep original scale [0, 5]\n    \n    df.to_csv(output_file, index=False)\n    print(f\"‚úÖ Saved predictions to {output_file}\")\n\n    # Compute metrics if true labels are available\n    if y_true is not None:\n        y_true_rescaled = y_true *5  # Rescale true scores back to [0, 5]\n        predictions_rescaled = predictions * 5  # Rescale predictions to [0, 5]\n        return compute_metrics(y_true_rescaled, predictions_rescaled)\n    else:\n        print(\"‚ö†Ô∏è True labels not available. Skipping metric calculations.\")\n        return None, None, None\n\n# ====== Run Prediction on Unseen Data ======\nunseen_file_path = \"/kaggle/input/test-data/test-BuIng.csv\"\noutput_file = \"trial-unseen_test_predictions.csv\"\n\nprint(\"\\nüöÄ Predicting on Unseen Dataset...\")\nmae, rmse, pearson_corr = predict_unseen_data(model, embedding, unseen_file_path, output_file)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T13:34:38.615239Z","iopub.execute_input":"2025-02-09T13:34:38.615777Z","iopub.status.idle":"2025-02-09T13:34:43.443150Z","shell.execute_reply.started":"2025-02-09T13:34:38.615739Z","shell.execute_reply":"2025-02-09T13:34:43.442047Z"}},"outputs":[{"name":"stdout","text":"Trained regression model loaded successfully!\n\nüöÄ Predicting on Unseen Dataset...\n‚úÖ Saved predictions to trial-unseen_test_predictions.csv\n\nüìä Evaluation Metrics on Unseen Data:\n‚úÖ Mean Absolute Error (MAE): 1.2111\n‚úÖ Root Mean Squared Error (RMSE): 1.4775\n‚úÖ Pearson Correlation: -0.0788\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"import pickle\nimport pandas as pd\nimport numpy as np\nimport nltk\nfrom gensim.models import Word2Vec\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error\nfrom scipy.stats import pearsonr\n\n# Load pre-trained Word2Vec model\nword2vec_model_path = \"/kaggle/input/id-w2v-model/pytorch/default/2/idwiki_word2vec_200_new_lower.model\"\nembedding = Word2Vec.load(word2vec_model_path)\n\n# Load trained regression model\nwith open(\"/kaggle/working/trained_model_sifCos_svr.pkl\", \"rb\") as file:\n    model = pickle.load(file)\n\nprint(\"Trained regression model loaded successfully!\")\n\ndef preprocess_text_indo(text):\n    # Normalize text (convert to lowercase)\n    text = text.lower()\n\n    # Remove new lines and extra spaces\n    text = re.sub(r\"\\s+\", \" \", text.strip())\n\n    # Remove punctuation using regex\n    text = re.sub(r\"[^\\w\\s]\", \"\", text)  # Removes all punctuation but keeps words and spaces\n\n    # Tokenize text\n    tokens = word_tokenize(text)\n\n    return \" \".join(tokens)\n\n# ====== Cosine Similarity Feature Extraction ======\ndef avg_cosine_similarity(sentences1, sentences2, embedding):\n    similarities = []\n    for sent1, sent2 in zip(sentences1, sentences2):\n        tokens1 = [token for token in sent1.split() if token in embedding.wv]\n        tokens2 = [token for token in sent2.split() if token in embedding.wv]\n        \n        if not tokens1 or not tokens2:\n            similarities.append(0)\n            continue\n        \n        vec1 = np.mean([embedding.wv[token] for token in tokens1], axis=0).reshape(1, -1)\n        vec2 = np.mean([embedding.wv[token] for token in tokens2], axis=0).reshape(1, -1)\n        \n        similarities.append(np.dot(vec1.flatten(), vec2.flatten()) / \n                            (np.linalg.norm(vec1) * np.linalg.norm(vec2)))\n    return np.array(similarities).reshape(-1, 1)\n\n# ====== Compute Metrics ======\ndef compute_metrics(y_true, y_pred):\n    \"\"\"\n    Compute MAE, RMSE, and Pearson correlation.\n    \"\"\"\n    mae = mean_absolute_error(y_true, y_pred)\n    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n    pearson_corr, _ = pearsonr(y_true, y_pred)\n\n    print(f\"\\nüìä Evaluation Metrics on Unseen Data:\")\n    print(f\"‚úÖ Mean Absolute Error (MAE): {mae:.4f}\")\n    print(f\"‚úÖ Root Mean Squared Error (RMSE): {rmse:.4f}\")\n    print(f\"‚úÖ Pearson Correlation: {pearson_corr:.4f}\")\n\n    return mae, rmse, pearson_corr\n\n# ====== Predict on Unseen Data and Compute Metrics ======\ndef predict_unseen_data(model, embedding, unseen_file_path, output_file):\n    \"\"\"\n    Predict similarity scores on an unseen dataset and compute evaluation metrics.\n    \"\"\"\n    unseen_data = pd.read_csv(unseen_file_path)\n\n    # Check if labels are available for evaluation\n    if \"label\" in unseen_data.columns:\n        y_true = unseen_data[\"label\"].values / 5.0  # Normalize labels to [0, 1]\n    else:\n        y_true = None\n\n    # Preprocess unseen dataset\n    unseen_sentence1 = [preprocess_text_indo(item) for item in unseen_data[\"answer\"]]\n    unseen_sentence2 = [preprocess_text_indo(item) for item in unseen_data[\"response\"]]\n\n    # Extract features using Word2Vec\n    unseen_features = word_distance(unseen_sentence1, unseen_sentence2, embedding)\n\n    # Predict similarity scores\n    predictions = model.predict(unseen_features)\n\n    # Save predictions to CSV\n    df = pd.DataFrame({\n        \"Original Sentence 1\": unseen_data[\"answer\"],\n        \"Original Sentence 2\": unseen_data[\"response\"],\n        \"True Label\": unseen_data[\"label\"] if y_true is not None else \"N/A\",  # Keep original scale\n        \"Predicted Similarity Score\": predictions * 5  # Rescale to [0, 5]\n    })\n\n    # Include true labels if available\n    if y_true is not None:\n        df[\"True Similarity Score\"] = unseen_data[\"label\"]  # Keep original scale [0, 5]\n    \n    df.to_csv(output_file, index=False)\n    print(f\"‚úÖ Saved predictions to {output_file}\")\n\n    # Compute metrics if true labels are available\n    if y_true is not None:\n        y_true_rescaled = y_true *5  # Rescale true scores back to [0, 5]\n        predictions_rescaled = predictions * 5  # Rescale predictions to [0, 5]\n        return compute_metrics(y_true_rescaled, predictions_rescaled)\n    else:\n        print(\"‚ö†Ô∏è True labels not available. Skipping metric calculations.\")\n        return None, None, None\n\n# ====== Run Prediction on Unseen Data ======\nunseen_file_path = \"/kaggle/input/test-data/test-BuIng.csv\"\noutput_file = \"trial-unseen_test_predictions.csv\"\n\nprint(\"\\nüöÄ Predicting on Unseen Dataset...\")\nmae, rmse, pearson_corr = predict_unseen_data(model, embedding, unseen_file_path, output_file)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T13:26:38.242889Z","iopub.execute_input":"2025-02-09T13:26:38.243185Z","iopub.status.idle":"2025-02-09T13:26:43.157031Z","shell.execute_reply.started":"2025-02-09T13:26:38.243161Z","shell.execute_reply":"2025-02-09T13:26:43.155972Z"}},"outputs":[{"name":"stdout","text":"Trained regression model loaded successfully!\n\nüöÄ Predicting on Unseen Dataset...\n‚úÖ Saved predictions to trial-unseen_test_predictions.csv\n\nüìä Evaluation Metrics on Unseen Data:\n‚úÖ Mean Absolute Error (MAE): 7.7710\n‚úÖ Root Mean Squared Error (RMSE): 7.8616\n‚úÖ Pearson Correlation: -0.0211\n","output_type":"stream"}],"execution_count":25}]}