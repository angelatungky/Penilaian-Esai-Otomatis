{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9970536,"sourceType":"datasetVersion","datasetId":6133976},{"sourceId":10667527,"sourceType":"datasetVersion","datasetId":6606685},{"sourceId":10667568,"sourceType":"datasetVersion","datasetId":6606717},{"sourceId":10669607,"sourceType":"datasetVersion","datasetId":6608283},{"sourceId":250503,"sourceType":"modelInstanceVersion","modelInstanceId":196387,"modelId":218296}],"dockerImageVersionId":30805,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport requests\n\n# URL untuk Wikipedia Bahasa Indonesia\nurl = \"https://dumps.wikimedia.org/idwiki/latest/idwiki-latest-pages-articles.xml.bz2\"\n\n# Nama file output\noutput_file = \"idwiki-latest-pages-articles.xml.bz2\"\n\n# Periksa apakah file sudah ada\nif not os.path.exists(output_file):\n    print(\"Mengunduh file Wikipedia Bahasa Indonesia...\")\n    response = requests.get(url, stream=True)\n    total_size = int(response.headers.get('content-length', 0))\n    chunk_size = 1024 * 1024  # 1 MB per chunk\n\n    with open(output_file, \"wb\") as file:\n        for data in response.iter_content(chunk_size=chunk_size):\n            file.write(data)\n    \n    print(\"Unduhan selesai. File disimpan sebagai:\", output_file)\nelse:\n    print(\"File sudah ada:\", output_file)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install gensim\n\nimport io\nimport time\nfrom datetime import timedelta\nimport gensim\nimport os\n\nif __name__ == '__main__':\n    start_time = time.time()\n\n    # Path input file Wikipedia XML\n    input_file = 'idwiki-latest-pages-articles.xml.bz2'\n    assert os.path.exists(input_file), \"File 'idwiki-latest-pages-articles.xml.bz2' tidak ditemukan!\"\n\n    print('Streaming wiki...')\n    id_wiki = gensim.corpora.WikiCorpus(\n        input_file, dictionary={}, lower=True\n    )\n    \n    # Path output file teks\n    output_file = 'idwiki_new_lower.txt'\n    article_count = 0\n\n    with io.open(output_file, 'w', encoding='utf-8') as wiki_txt:\n        for text in id_wiki.get_texts():\n            # Menulis artikel ke file teks\n            wiki_txt.write(\" \".join(text) + '\\n')\n            article_count += 1\n\n            # Progress log setiap 10.000 artikel\n            if article_count % 10000 == 0:\n                print('{} articles processed'.format(article_count))\n        \n        print('Total: {} articles processed.'.format(article_count))\n\n    finish_time = time.time()\n    print('Elapsed time: {}'.format(timedelta(seconds=finish_time - start_time)))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T01:23:39.597037Z","iopub.execute_input":"2024-12-04T01:23:39.597422Z","iopub.status.idle":"2024-12-04T01:43:15.218169Z","shell.execute_reply.started":"2024-12-04T01:23:39.597392Z","shell.execute_reply":"2024-12-04T01:43:15.217178Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import time\nimport os\nimport multiprocessing\nfrom datetime import timedelta\nfrom gensim.models import Word2Vec\nfrom gensim.models.word2vec import LineSentence\n\nif __name__ == '__main__':\n    start_time = time.time()\n    print('Training Word2Vec Model...')\n\n    # Path input file teks hasil proses sebelumnya\n    input_file = '/kaggle/working/idwiki_new_lower.txt'\n    assert os.path.exists(input_file), \"File 'idwiki_new_lower.txt' tidak ditemukan! Pastikan proses sebelumnya berhasil.\"\n\n    # Path output model\n    output_dir = 'model'\n    output_file = os.path.join(output_dir, 'idwiki_word2vec_200_new_lower.model')\n\n    # Membuat direktori output jika belum ada\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n\n    # Melatih model Word2Vec\n    sentences = LineSentence(input_file)\n    id_w2v = Word2Vec(sentences, vector_size=200, workers=multiprocessing.cpu_count() - 1)\n\n    # Menyimpan model\n    id_w2v.save(output_file)\n\n    finish_time = time.time()\n    print(f'Finished. Elapsed time: {timedelta(seconds=finish_time - start_time)}')\n\n    # Informasi lokasi file model\n    print(f\"Model saved at: {output_file}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T02:00:43.482715Z","iopub.execute_input":"2024-12-04T02:00:43.483589Z","iopub.status.idle":"2024-12-04T02:19:02.567838Z","shell.execute_reply.started":"2024-12-04T02:00:43.483554Z","shell.execute_reply":"2024-12-04T02:19:02.566996Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from gensim.models import Word2Vec\n\n# Load pre-trained Word2Vec model\nword2vec_model = Word2Vec.load('/kaggle/input/id-w2v-model/pytorch/default/2/idwiki_word2vec_200_new_lower.model')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-05T12:44:47.628706Z","iopub.execute_input":"2025-02-05T12:44:47.629031Z","iopub.status.idle":"2025-02-05T12:45:05.394960Z","shell.execute_reply.started":"2025-02-05T12:44:47.628990Z","shell.execute_reply":"2025-02-05T12:45:05.394222Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"import nltk\nimport string\nfrom nltk.tokenize import word_tokenize\n\n# Sample Indonesian text\nsample_text = \"Ini adalah contoh kalimat dalam bahasa Indonesia.\"\n\n# Tokenize the text\nnltk.download('punkt')  # Download tokenizer data if not already downloaded\ntokenized_text = word_tokenize(sample_text.lower())  # Convert to lowercase for consistency\n\n# Filter out punctuation\ntokenized_text = [word for word in tokenized_text if word not in string.punctuation]\n\nprint(\"Tokenized words (no punctuation):\", tokenized_text)\n\n# Check for each word in the Word2Vec model\nfor word in tokenized_text:\n    if word in word2vec_model.wv:  # Check if the word exists in the model vocabulary\n        print(f\"Word: {word}, Vector: {word2vec_model.wv[word][:5]}...\")  # Show first 5 dimensions\n    else:\n        print(f\"Word '{word}' is not in the vocabulary.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-05T12:46:03.900732Z","iopub.execute_input":"2025-02-05T12:46:03.901098Z","iopub.status.idle":"2025-02-05T12:46:04.651835Z","shell.execute_reply.started":"2025-02-05T12:46:03.901064Z","shell.execute_reply":"2025-02-05T12:46:04.650951Z"}},"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\nTokenized words (no punctuation): ['ini', 'adalah', 'contoh', 'kalimat', 'dalam', 'bahasa', 'indonesia']\nWord: ini, Vector: [1.4045461 1.3396655 1.0604098 1.5734333 3.5066075]...\nWord: adalah, Vector: [-3.1721826 -1.6112142 -4.920957  -0.5853852  0.6956718]...\nWord: contoh, Vector: [ 1.2200239  -0.57646936 -0.94530183  0.19300076  2.1011887 ]...\nWord: kalimat, Vector: [ 0.89005333  1.5616243   3.1428714  -0.84587735  0.4537459 ]...\nWord: dalam, Vector: [-0.06166538  0.69772893 -1.8301325  -0.46174216 -0.14013124]...\nWord: bahasa, Vector: [-0.5654576   0.58922666  2.964573    1.6541932   3.4733458 ]...\nWord: indonesia, Vector: [-0.8432819   0.03616891  3.1263888  -3.3953204  -0.34944582]...\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"pip install Sastrawi","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-05T12:46:07.668419Z","iopub.execute_input":"2025-02-05T12:46:07.668770Z","iopub.status.idle":"2025-02-05T12:46:17.117672Z","shell.execute_reply.started":"2025-02-05T12:46:07.668742Z","shell.execute_reply":"2025-02-05T12:46:17.116371Z"}},"outputs":[{"name":"stdout","text":"Collecting Sastrawi\n  Downloading Sastrawi-1.0.1-py2.py3-none-any.whl.metadata (909 bytes)\nDownloading Sastrawi-1.0.1-py2.py3-none-any.whl (209 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.7/209.7 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: Sastrawi\nSuccessfully installed Sastrawi-1.0.1\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import warnings\nimport numpy as np\nimport pandas as pd\nimport random\nfrom sklearn.model_selection import train_test_split\nfrom gensim.models import Word2Vec\nfrom nltk.tokenize import word_tokenize\nfrom Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\nimport nltk","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-05T12:46:17.120041Z","iopub.execute_input":"2025-02-05T12:46:17.120485Z","iopub.status.idle":"2025-02-05T12:46:17.131036Z","shell.execute_reply.started":"2025-02-05T12:46:17.120443Z","shell.execute_reply":"2025-02-05T12:46:17.130193Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# Ensure required NLTK data is downloaded\nnltk.download('punkt')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-05T12:46:17.132205Z","iopub.execute_input":"2025-02-05T12:46:17.132554Z","iopub.status.idle":"2025-02-05T12:46:17.144016Z","shell.execute_reply.started":"2025-02-05T12:46:17.132518Z","shell.execute_reply":"2025-02-05T12:46:17.143034Z"}},"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n","output_type":"stream"},{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"# ====== Dataset Loading and Splitting ======\ndef load_indo_dataset(filename):\n    df = pd.read_csv(filename)\n    data = [\n        (row['answer'], row['response'], row['label'] / 5.0)  # Normalize label\n        for _, row in df.iterrows()\n    ]\n    return data","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-05T12:46:17.981130Z","iopub.execute_input":"2025-02-05T12:46:17.982075Z","iopub.status.idle":"2025-02-05T12:46:17.988040Z","shell.execute_reply.started":"2025-02-05T12:46:17.982021Z","shell.execute_reply":"2025-02-05T12:46:17.987148Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"def split_dataset(data, valid_percentage=0.2, test_percentage=0.2):\n    random.shuffle(data)\n    train_size = int(len(data) * (1 - valid_percentage - test_percentage))\n    valid_size = int(len(data) * valid_percentage)\n    train = data[:train_size]\n    valid = data[train_size:train_size + valid_size]\n    test = data[train_size + valid_size:]\n    return train, valid, test\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-05T12:46:21.841882Z","iopub.execute_input":"2025-02-05T12:46:21.842719Z","iopub.status.idle":"2025-02-05T12:46:21.848801Z","shell.execute_reply.started":"2025-02-05T12:46:21.842668Z","shell.execute_reply":"2025-02-05T12:46:21.847776Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"def preprocess_text_indo(text):\n    # Normalize text\n    text = text.lower()\n\n    # Tokenize text\n    tokens = word_tokenize(text)\n\n    # Remove stopwords only (no stemming)\n    stopword_factory = StopWordRemoverFactory()\n    stopword_remover = stopword_factory.create_stop_word_remover()\n    \n    tokens = [stopword_remover.remove(word) for word in tokens if word.isalpha()]\n    return \" \".join(tokens)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-05T12:46:27.381385Z","iopub.execute_input":"2025-02-05T12:46:27.381724Z","iopub.status.idle":"2025-02-05T12:46:27.386586Z","shell.execute_reply.started":"2025-02-05T12:46:27.381695Z","shell.execute_reply":"2025-02-05T12:46:27.385655Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"def preprocess_data(data):\n    sentence1 = [preprocess_text_indo(item[0]) for item in data]\n    sentence2 = [preprocess_text_indo(item[1]) for item in data]\n    labels = [item[2] for item in data]\n    return sentence1, sentence2, labels","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-05T12:46:32.289805Z","iopub.execute_input":"2025-02-05T12:46:32.290466Z","iopub.status.idle":"2025-02-05T12:46:32.295172Z","shell.execute_reply.started":"2025-02-05T12:46:32.290432Z","shell.execute_reply":"2025-02-05T12:46:32.294169Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"# ====== Load Pre-Trained Word2Vec Model ======\ndef load_pretrained_word_embedding(path):\n    print(f\"Loading pre-trained Word2Vec model from: {path}\")\n    return Word2Vec.load(path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-05T12:46:37.185051Z","iopub.execute_input":"2025-02-05T12:46:37.185505Z","iopub.status.idle":"2025-02-05T12:46:37.189786Z","shell.execute_reply.started":"2025-02-05T12:46:37.185473Z","shell.execute_reply":"2025-02-05T12:46:37.188840Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"# Define your custom cosine similarity function\ndef cosine_similarity_custom(vec1, vec2):\n    \"\"\"\n    Custom implementation of cosine similarity.\n    \"\"\"\n    dot_product = np.dot(vec1, vec2)\n    norm_vec1 = np.linalg.norm(vec1)\n    norm_vec2 = np.linalg.norm(vec2)\n    \n    if norm_vec1 == 0 or norm_vec2 == 0:  # Handle zero-vector case\n        return 0.0\n    \n    return dot_product / (norm_vec1 * norm_vec2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-05T12:46:39.820946Z","iopub.execute_input":"2025-02-05T12:46:39.821800Z","iopub.status.idle":"2025-02-05T12:46:39.826765Z","shell.execute_reply.started":"2025-02-05T12:46:39.821762Z","shell.execute_reply":"2025-02-05T12:46:39.825878Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"def average_cosine_similarity(kalimat1, kalimat2, embedding, method=\"w2v\"):\n    \"\"\"\n    Generate sentence features using Word2Vec, FastText.\n    \"\"\"\n    if method in [\"w2v\", \"fast\"]:\n        similarities = []\n        for text1, text2 in zip(kalimat1, kalimat2):\n            tokens1 = [word for word in text1.split() if word in embedding.wv]\n            tokens2 = [word for word in text2.split() if word in embedding.wv]\n            \n            if not tokens1 or not tokens2:\n                similarities.append(0)\n                continue\n            \n            vec1 = np.mean([embedding.wv[word] for word in tokens1], axis=0).reshape(1, -1)\n            vec2 = np.mean([embedding.wv[word] for word in tokens2], axis=0).reshape(1, -1)\n            \n            similarities.append(cosine_similarity_custom(vec1.flatten(), vec2.flatten()))\n        return np.array(similarities).reshape(-1, 1)\n\n    else:\n        raise ValueError(\"Unsupported method.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-05T12:46:41.696021Z","iopub.execute_input":"2025-02-05T12:46:41.696866Z","iopub.status.idle":"2025-02-05T12:46:41.703164Z","shell.execute_reply.started":"2025-02-05T12:46:41.696830Z","shell.execute_reply":"2025-02-05T12:46:41.702369Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"def word_distance(text_group1, text_group2, embed_model):\n    similarity_scores = []\n    for group1, group2 in zip(text_group1, text_group2):\n        group1_tokens = [word for word in group1.split() if word in embed_model.wv]\n        group2_tokens = [word for word in group2.split() if word in embed_model.wv]\n\n        if not group1_tokens or not group2_tokens:\n            similarity_scores.append(0)\n        else:\n            similarity_scores.append(-embed_model.wv.wmdistance(group1_tokens, group2_tokens))\n\n    return np.array(similarity_scores).reshape(-1, 1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-05T12:46:43.905081Z","iopub.execute_input":"2025-02-05T12:46:43.905673Z","iopub.status.idle":"2025-02-05T12:46:43.911020Z","shell.execute_reply.started":"2025-02-05T12:46:43.905639Z","shell.execute_reply":"2025-02-05T12:46:43.910074Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"# for SIF\nfrom sklearn.decomposition import TruncatedSVD\n    \ndef eliminate_first_component(matrix):\n    svd_model = TruncatedSVD(n_components=1, random_state=42)\n    svd_model.fit(matrix)\n    principal_component = svd_model.components_\n    return matrix - matrix.dot(principal_component.T) * principal_component\n    \ndef sif_cos(text_group1, text_group2, embed_model, frequency_map, smoothing_factor=0.001):\n    freq_sum = sum(frequency_map.values())\n    all_embeddings = []\n\n    for group1, group2 in zip(text_group1, text_group2):\n        tokens1 = [word for word in group1.split() if word in embed_model.wv]\n        tokens2 = [word for word in group2.split() if word in embed_model.wv]\n\n        if not tokens1 or not tokens2:\n            all_embeddings.extend([np.zeros(embed_model.vector_size), np.zeros(embed_model.vector_size)])\n            continue\n\n        weights1 = [smoothing_factor / (smoothing_factor + frequency_map.get(word, 1e-5) / freq_sum) for word in tokens1]\n        weights2 = [smoothing_factor / (smoothing_factor + frequency_map.get(word, 1e-5) / freq_sum) for word in tokens2]\n\n        embedding1 = np.average([embed_model.wv[word] for word in tokens1], axis=0, weights=weights1)\n        embedding2 = np.average([embed_model.wv[word] for word in tokens2], axis=0, weights=weights2)\n\n        all_embeddings.extend([embedding1, embedding2])\n\n    all_embeddings = np.array(all_embeddings)\n    all_embeddings = eliminate_first_component(all_embeddings)\n\n    similarities = [\n        (\n            np.dot(all_embeddings[i], all_embeddings[i + 1]) /\n            (np.linalg.norm(all_embeddings[i]) * np.linalg.norm(all_embeddings[i + 1]))\n            if np.linalg.norm(all_embeddings[i]) > 0 and np.linalg.norm(all_embeddings[i + 1]) > 0 else 0\n        )\n        for i in range(0, len(all_embeddings), 2)\n    ]\n\n    return np.array(similarities).reshape(-1, 1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-05T12:46:45.807521Z","iopub.execute_input":"2025-02-05T12:46:45.808163Z","iopub.status.idle":"2025-02-05T12:46:45.840142Z","shell.execute_reply.started":"2025-02-05T12:46:45.808126Z","shell.execute_reply":"2025-02-05T12:46:45.839283Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"def feature_extraction(train_set1, train_set2, val_set1, val_set2, test_set, embed_model, frequency_map, method):\n    if method == \"averageCosine\":\n        train_similarities = average_cosine_similarity(train_set1, train_set2, embed_model)\n        val_similarities = average_cosine_similarity(val_set1, val_set2, embed_model)\n        test_similarities = average_cosine_similarity(test_set[\"sentence1\"], test_set[\"sentence2\"], embed_model)\n    elif method == \"wordDis\":\n        train_similarities = word_distance(train_set1, train_set2, embed_model)\n        val_similarities = word_distance(val_set1, val_set2, embed_model)\n        test_similarities = word_distance(test_set[\"sentence1\"], test_set[\"sentence2\"], embed_model)\n    elif method == \"sifCos\":\n        train_similarities = sif_cos(train_set1, train_set2, embed_model, frequency_map)\n        val_similarities = sif_cos(val_set1, val_set2, embed_model, frequency_map)\n        test_similarities = sif_cos(test_set[\"sentence1\"], test_set[\"sentence2\"], embed_model, frequency_map)\n    else:\n        raise ValueError(f\"Feature extraction method '{method}' is not supported.\")\n\n    return np.array(train_similarities), np.array(val_similarities), np.array(test_similarities)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-05T12:46:47.978174Z","iopub.execute_input":"2025-02-05T12:46:47.978538Z","iopub.status.idle":"2025-02-05T12:46:47.985127Z","shell.execute_reply.started":"2025-02-05T12:46:47.978507Z","shell.execute_reply":"2025-02-05T12:46:47.983985Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"\n# ====== Regression Model ======\nclass RegressionModel:\n    def __init__(self, model_type=\"linear\"):\n        if model_type == \"linear\":\n            self.model = self.LinearRegressionCustom()\n        elif model_type == \"svr\":\n            self.model = SVR(kernel=\"linear\")\n        elif model_type == \"rfr\":\n            self.model = self.RandomForestCustom()\n        else:\n            raise ValueError(\"Unsupported model type.\")\n\n    class LinearRegressionCustom:\n        def __init__(self):\n            self.weights = None\n\n        def fit(self, X, y):\n            X = np.c_[np.ones(X.shape[0]), X]  # Add bias term\n            self.weights = np.linalg.pinv(X.T @ X) @ X.T @ y\n        def fit(self, X, y):\n            # Convert y to NumPy and ensure matching rows\n            y = np.array(y)\n            if X.shape[0] != y.shape[0]:\n                raise ValueError(f\"Shape mismatch: X has {X.shape[0]} rows but y has {y.shape[0]} rows.\")\n            \n            X = np.c_[np.ones(X.shape[0]), X]  # Add bias term\n            self.weights = np.linalg.pinv(X.T @ X) @ X.T @ y\n\n        def predict(self, X):\n            X = np.c_[np.ones(X.shape[0]), X]  # Add bias term\n            return X @ self.weights\n\n    class RandomForestCustom:\n        def __init__(self, n_estimators=100, max_depth=None):\n            self.n_estimators = n_estimators\n            self.max_depth = max_depth\n            self.trees = []\n    \n        def fit(self, X, y):\n            from sklearn.tree import DecisionTreeRegressor\n    \n            # Ensure y is a NumPy array\n            y = np.array(y)\n    \n            n_samples = X.shape[0]\n    \n            for _ in range(self.n_estimators):\n                # Ensure indices are integers for proper indexing\n                indices = np.random.choice(range(n_samples), size=n_samples, replace=True)\n                X_sample = X[indices]\n                y_sample = y[indices]\n                tree = DecisionTreeRegressor(max_depth=self.max_depth)\n                tree.fit(X_sample, y_sample)\n                self.trees.append(tree)\n    \n        def predict(self, X):\n            # Aggregate predictions from all trees\n            predictions = np.array([tree.predict(X) for tree in self.trees])\n            return np.mean(predictions, axis=0)\n\n    @staticmethod\n    def mean_squared_error(y_true, y_pred):\n        squared_errors = [(true - pred) ** 2 for true, pred in zip(y_true, y_pred)]\n        return sum(squared_errors) / len(squared_errors)\n\n    @staticmethod\n    def mean_absolute_error(y_true, y_pred):\n        absolute_errors = [abs(true - pred) for true, pred in zip(y_true, y_pred)]\n        return sum(absolute_errors) / len(absolute_errors)\n\n    @staticmethod\n    def pearsonr(x, y):\n        mean_x = sum(x) / len(x)\n        mean_y = sum(y) / len(y)\n        numerator = sum((xi - mean_x) * (yi - mean_y) for xi, yi in zip(x, y))\n        denominator = ((sum((xi - mean_x) ** 2 for xi in x) * sum((yi - mean_y) ** 2 for yi in y)) ** 0.5)\n        return (numerator / denominator if denominator != 0 else 0.0, None)\n\n    def train(self, x_train, y_train):\n        self.model.fit(x_train, y_train)\n\n    def evaluate(self, x, y):\n        predictions = self.model.predict(x)\n        mse = self.mean_squared_error(y, predictions)\n        mae = self.mean_absolute_error(y, predictions)\n        pearson_corr, _ = self.pearsonr(y, predictions)\n        return mse, mae, pearson_corr\n\n    def predict(self, x):\n        return self.model.predict(x)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-05T12:46:53.579120Z","iopub.execute_input":"2025-02-05T12:46:53.579465Z","iopub.status.idle":"2025-02-05T12:46:53.594469Z","shell.execute_reply.started":"2025-02-05T12:46:53.579433Z","shell.execute_reply":"2025-02-05T12:46:53.593486Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"pip install cvxopt","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-05T12:46:56.796657Z","iopub.execute_input":"2025-02-05T12:46:56.797219Z","iopub.status.idle":"2025-02-05T12:47:05.669319Z","shell.execute_reply.started":"2025-02-05T12:46:56.797183Z","shell.execute_reply":"2025-02-05T12:47:05.668182Z"}},"outputs":[{"name":"stdout","text":"Collecting cvxopt\n  Downloading cvxopt-1.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.3 kB)\nDownloading cvxopt-1.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.6/13.6 MB\u001b[0m \u001b[31m92.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hInstalling collected packages: cvxopt\nSuccessfully installed cvxopt-1.3.2\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"pip install POT","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-05T12:47:05.671465Z","iopub.execute_input":"2025-02-05T12:47:05.671902Z","iopub.status.idle":"2025-02-05T12:47:14.267884Z","shell.execute_reply.started":"2025-02-05T12:47:05.671849Z","shell.execute_reply":"2025-02-05T12:47:14.266767Z"}},"outputs":[{"name":"stdout","text":"Collecting POT\n  Downloading POT-0.9.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (34 kB)\nRequirement already satisfied: numpy>=1.16 in /opt/conda/lib/python3.10/site-packages (from POT) (1.26.4)\nRequirement already satisfied: scipy>=1.6 in /opt/conda/lib/python3.10/site-packages (from POT) (1.14.1)\nDownloading POT-0.9.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (865 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m865.6/865.6 kB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: POT\nSuccessfully installed POT-0.9.5\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"import random\n# ====== Main Workflow ======\n# Load and preprocess dataset\n# Load Indonesian dataset\nraw_data = load_indo_dataset(\"/kaggle/input/indo-datasets/indodata.csv\")\ntrain_data, valid_data, test_data = split_dataset(raw_data)\nx_train1, x_train2, y_train = preprocess_data(train_data)\nx_valid1, x_valid2, y_valid = preprocess_data(valid_data)\nx_test1, x_test2, y_test = preprocess_data(test_data)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-05T12:47:36.427165Z","iopub.execute_input":"2025-02-05T12:47:36.428143Z","iopub.status.idle":"2025-02-05T12:47:36.707137Z","shell.execute_reply.started":"2025-02-05T12:47:36.428097Z","shell.execute_reply":"2025-02-05T12:47:36.706055Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"# ====== Load Dataset ======\n# Load train, validation, and test datasets from CSV files\ntrain_file = \"/kaggle/input/data-w2v/train_data-w2v (1).csv\"\nvalid_file = \"/kaggle/input/data-w2v/valid_data-w2v (1).csv\"\ntest_file = \"/kaggle/input/data-w2v/test_data-w2v (1).csv\"\n\n# Read datasets\ntrain_data = pd.read_csv(train_file).values\nvalid_data = pd.read_csv(valid_file).values\ntest_data = pd.read_csv(test_file).values\n\n# ====== Preprocessing ======\nx_train1, x_train2, y_train = preprocess_data(train_data)\nx_valid1, x_valid2, y_valid = preprocess_data(valid_data)\nx_test1, x_test2, y_test = preprocess_data(test_data)\n\n# Output shapes for verification\nprint(f\"Train data: {len(x_train1)} pairs, {len(y_train)} labels\")\nprint(f\"Validation data: {len(x_valid1)} pairs, {len(y_valid)} labels\")\nprint(f\"Test data: {len(x_test1)} pairs, {len(y_test)} labels\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-05T07:50:34.296394Z","iopub.execute_input":"2025-02-05T07:50:34.296774Z","iopub.status.idle":"2025-02-05T07:50:35.091034Z","shell.execute_reply.started":"2025-02-05T07:50:34.296746Z","shell.execute_reply":"2025-02-05T07:50:35.089974Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\n\n# ====== Frequency Computation for SIF ======\nfrom collections import Counter\nall_sentences = x_train1 + x_train2 + x_valid1 + x_valid2 + x_test1 + x_test2\n# Compute word frequencies for SIF\nall_tokens = [token for sentence in all_sentences for token in sentence.split()]\nfreqs = Counter(all_tokens)\n\n# ====== Feature Extraction and Model Evaluation ======\nmethods = [\"averageCosine\", \"wordDis\", \"sifCos\"]\n# Store results for all methods and models\nresults = []\nword2vec_model_path = '/kaggle/input/id-w2v-model/pytorch/default/2/idwiki_word2vec_200_new_lower.model'\n# Load pre-trained Word2Vec model\nembedding = load_pretrained_word_embedding(word2vec_model_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-05T12:47:44.906147Z","iopub.execute_input":"2025-02-05T12:47:44.906730Z","iopub.status.idle":"2025-02-05T12:47:48.765954Z","shell.execute_reply.started":"2025-02-05T12:47:44.906696Z","shell.execute_reply":"2025-02-05T12:47:48.765195Z"}},"outputs":[{"name":"stdout","text":"Loading pre-trained Word2Vec model from: /kaggle/input/id-w2v-model/pytorch/default/2/idwiki_word2vec_200_new_lower.model\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"from sklearn.svm import SVR","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-05T12:47:53.794144Z","iopub.execute_input":"2025-02-05T12:47:53.794817Z","iopub.status.idle":"2025-02-05T12:47:53.798920Z","shell.execute_reply.started":"2025-02-05T12:47:53.794779Z","shell.execute_reply":"2025-02-05T12:47:53.797835Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"for method in methods:\n    current_method = method\n    print(f\"Using feature extraction method: {current_method}\")\n    \n    x_train_features, x_valid_features, x_test_features = feature_extraction(\n        x_train1, x_train2, x_valid1, x_valid2, {\"sentence1\": x_test1, \"sentence2\": x_test2}, embedding, freqs, current_method\n    )\n\n    for reg_model in [\"linear\", \"svr\", \"rfr\"]:\n        model = RegressionModel(model_type=reg_model)\n        model.train(x_train_features, y_train)\n\n        # Evaluate on validation and test sets\n        val_mse, val_mae, val_pearson = model.evaluate(x_valid_features, y_valid)\n        test_mse, test_mae, test_pearson = model.evaluate(x_test_features, y_test)\n\n        print(f\"Validation Performance ({current_method}, {reg_model}):\")\n        print(f\"MSE: {val_mse:.4f}, MAE: {val_mae:.4f}, Pearson Correlation: {val_pearson:.4f}\")\n\n        print(f\"Test Performance ({current_method}, {reg_model}):\")\n        print(f\"MSE: {test_mse:.4f}, MAE: {test_mae:.4f}, Pearson Correlation: {test_pearson:.4f}\")\n\n        # Store results\n        results.append({\n            \"method\": current_method,\n            \"model\": reg_model,\n            \"pearson\": test_pearson,\n            \"test_predictions\": model.predict(x_test_features),\n            \"test_features\": x_test_features,\n            \"x_test1\": x_test1,\n            \"x_test2\": x_test2,\n            \"y_test\": y_test\n        })\n\n# Save all results as CSV files (for both validation and test)\nfor i, result in enumerate(results, start=1):\n    # Retrieve original raw sentences for validation and test\n    raw_valid1 = [item[0] for item in valid_data]  # Original raw Sentence 1 for validation\n    raw_valid2 = [item[1] for item in valid_data]  # Original raw Sentence 2 for validation\n    raw_test1 = [item[0] for item in test_data]  # Original raw Sentence 1 for test\n    raw_test2 = [item[1] for item in test_data]  # Original raw Sentence 2 for test\n    val_predictions = model.predict(x_valid_features)  # Prediksi untuk data validasi\n    # Create DataFrame for validation predictions\n    val_df = pd.DataFrame({\n        \"Original Sentence 1\": raw_valid1,  # Append raw sentence 1\n        \"Original Sentence 2\": raw_valid2,  # Append raw sentence 2\n        \"Preprocessed Sentence 1\": x_valid1,  # Preprocessed validation sentence 1\n        \"Preprocessed Sentence 2\": x_valid2,  # Preprocessed validation sentence 2\n        \"True Similarity Score\": [y * 5 for y in y_valid],  # Rescale validation true scores\n        \"Predicted Similarity Score\": [y * 5 for y in val_predictions]  # Validation predictions rescaled\n    })\n\n    # Save validation result CSV\n    val_filename = f\"val_result_{i}_{result['method']}_{result['model']}.csv\"\n    val_df.to_csv(val_filename, index=False)\n    print(f\"Saved validation result: {val_filename}\")\n\n    # Create DataFrame for test predictions\n    test_df = pd.DataFrame({\n        \"Original Sentence 1\": raw_test1,  # Append raw sentence 1\n        \"Original Sentence 2\": raw_test2,  # Append raw sentence 2\n        \"Preprocessed Sentence 1\": result[\"x_test1\"],  # Preprocessed test sentence 1\n        \"Preprocessed Sentence 2\": result[\"x_test2\"],  # Preprocessed test sentence 2\n        \"True Similarity Score\": [y * 5 for y in result[\"y_test\"]],  # Rescale to [0, 5]\n        \"Predicted Similarity Score\": [y * 5 for y in result[\"test_predictions\"]]  # Predicted scores rescaled\n    })\n\n    # Save test result CSV\n    test_filename = f\"test_result_{i}_{result['method']}_{result['model']}.csv\"\n    test_df.to_csv(test_filename, index=False)\n    print(f\"Saved test result: {test_filename}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-05T12:48:05.121002Z","iopub.execute_input":"2025-02-05T12:48:05.121376Z","iopub.status.idle":"2025-02-05T12:48:21.384867Z","shell.execute_reply.started":"2025-02-05T12:48:05.121341Z","shell.execute_reply":"2025-02-05T12:48:21.383223Z"}},"outputs":[{"name":"stdout","text":"Using feature extraction method: averageCosine\nValidation Performance (averageCosine, linear):\nMSE: 0.0613, MAE: 0.2009, Pearson Correlation: 0.7154\nTest Performance (averageCosine, linear):\nMSE: 0.1833, MAE: 0.2295, Pearson Correlation: 0.4484\nValidation Performance (averageCosine, svr):\nMSE: 0.0629, MAE: 0.1983, Pearson Correlation: 0.7154\nTest Performance (averageCosine, svr):\nMSE: 0.1869, MAE: 0.2267, Pearson Correlation: 0.4484\nValidation Performance (averageCosine, rfr):\nMSE: 0.0646, MAE: 0.1811, Pearson Correlation: 0.7176\nTest Performance (averageCosine, rfr):\nMSE: 0.1787, MAE: 0.2137, Pearson Correlation: 0.4994\nUsing feature extraction method: wordDis\nValidation Performance (wordDis, linear):\nMSE: 0.0347, MAE: 0.1418, Pearson Correlation: 0.8507\nTest Performance (wordDis, linear):\nMSE: 0.1538, MAE: 0.1784, Pearson Correlation: 0.5694\nValidation Performance (wordDis, svr):\nMSE: 0.0347, MAE: 0.1399, Pearson Correlation: 0.8507\nTest Performance (wordDis, svr):\nMSE: 0.1556, MAE: 0.1769, Pearson Correlation: 0.5694\nValidation Performance (wordDis, rfr):\nMSE: 0.0503, MAE: 0.1597, Pearson Correlation: 0.7886\nTest Performance (wordDis, rfr):\nMSE: 0.1812, MAE: 0.2034, Pearson Correlation: 0.4885\nUsing feature extraction method: sifCos\nValidation Performance (sifCos, linear):\nMSE: 0.0546, MAE: 0.1852, Pearson Correlation: 0.7518\nTest Performance (sifCos, linear):\nMSE: 0.1738, MAE: 0.2233, Pearson Correlation: 0.4790\nValidation Performance (sifCos, svr):\nMSE: 0.0559, MAE: 0.1805, Pearson Correlation: 0.7518\nTest Performance (sifCos, svr):\nMSE: 0.1770, MAE: 0.2198, Pearson Correlation: 0.4790\nValidation Performance (sifCos, rfr):\nMSE: 0.0747, MAE: 0.1988, Pearson Correlation: 0.6785\nTest Performance (sifCos, rfr):\nMSE: 0.1998, MAE: 0.2441, Pearson Correlation: 0.4129\nSaved validation result: val_result_1_averageCosine_linear.csv\nSaved test result: test_result_1_averageCosine_linear.csv\nSaved validation result: val_result_2_averageCosine_svr.csv\nSaved test result: test_result_2_averageCosine_svr.csv\nSaved validation result: val_result_3_averageCosine_rfr.csv\nSaved test result: test_result_3_averageCosine_rfr.csv\nSaved validation result: val_result_4_wordDis_linear.csv\nSaved test result: test_result_4_wordDis_linear.csv\nSaved validation result: val_result_5_wordDis_svr.csv\nSaved test result: test_result_5_wordDis_svr.csv\nSaved validation result: val_result_6_wordDis_rfr.csv\nSaved test result: test_result_6_wordDis_rfr.csv\nSaved validation result: val_result_7_sifCos_linear.csv\nSaved test result: test_result_7_sifCos_linear.csv\nSaved validation result: val_result_8_sifCos_svr.csv\nSaved test result: test_result_8_sifCos_svr.csv\nSaved validation result: val_result_9_sifCos_rfr.csv\nSaved test result: test_result_9_sifCos_rfr.csv\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"import pandas as pd\nimport os\n\n# Save datasets to CSV files\ndef save_splits_to_csv(train_data, val_data, test_data, output_dir):\n    os.makedirs(output_dir, exist_ok=True)\n\n    train_df = pd.DataFrame(train_data, columns=[\"response\", \"answer\", \"label\"])\n    val_df = pd.DataFrame(val_data, columns=[\"response\", \"answer\", \"label\"])\n    test_df = pd.DataFrame(test_data, columns=[\"response\", \"answer\", \"label\"])\n\n    train_df.to_csv(os.path.join(output_dir, \"train_data.csv\"), index=False)\n    val_df.to_csv(os.path.join(output_dir, \"val_data.csv\"), index=False)\n    test_df.to_csv(os.path.join(output_dir, \"test_data.csv\"), index=False)\n\n    print(f\"Data saved to {output_dir} successfully.\")\n    \n# Save splits to CSV\noutput_directory = \"output_splits\"\nsave_splits_to_csv(train_data, valid_data, test_data, output_directory)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-05T12:52:38.996488Z","iopub.execute_input":"2025-02-05T12:52:38.996848Z","iopub.status.idle":"2025-02-05T12:52:39.013134Z","shell.execute_reply.started":"2025-02-05T12:52:38.996819Z","shell.execute_reply":"2025-02-05T12:52:39.012248Z"}},"outputs":[{"name":"stdout","text":"Data saved to output_splits successfully.\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"results = []\n# ====== Load Dataset ======\n# Load train, validation, and test datasets from CSV files\ntrain_file = \"/kaggle/working/output_splits/train_data.csv\"\nvalid_file = \"/kaggle/working/output_splits/val_data.csv\"\nunseen_file = \"/kaggle/input/test-data/test-BuIng.csv\"\n\n# Read datasets\ntrain_data = pd.read_csv(train_file).values\nvalid_data = pd.read_csv(valid_file).values\nunseen_data = pd.read_csv(unseen_file).values\n\n# ====== Preprocessing ======\nx_train1, x_train2, y_train = preprocess_data(train_data)\nx_valid1, x_valid2, y_valid = preprocess_data(valid_data)\nx_unseen1, x_unseen2, y_unseen = preprocess_data(unseen_data)\n\n# Output shapes for verification\nprint(f\"Train data: {len(x_train1)} pairs, {len(y_train)} labels\")\nprint(f\"Validation data: {len(x_valid1)} pairs, {len(y_valid)} labels\")\nprint(f\"unseen data: {len(x_unseen1)} pairs, {len(y_unseen)} labels\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-05T13:31:50.556014Z","iopub.execute_input":"2025-02-05T13:31:50.556731Z","iopub.status.idle":"2025-02-05T13:31:50.785156Z","shell.execute_reply.started":"2025-02-05T13:31:50.556695Z","shell.execute_reply":"2025-02-05T13:31:50.784239Z"}},"outputs":[{"name":"stdout","text":"Train data: 434 pairs, 434 labels\nValidation data: 144 pairs, 144 labels\nunseen data: 25 pairs, 25 labels\n","output_type":"stream"}],"execution_count":34},{"cell_type":"code","source":"for method in methods:\n    current_method = method\n    print(f\"Using feature extraction method: {current_method}\")\n    \n    x_train_features, x_valid_features, x_unseen_features = feature_extraction(\n        x_train1, x_train2, x_valid1, x_valid2, {\"sentence1\": x_unseen1, \"sentence2\": x_unseen2}, embedding, freqs, current_method\n    )\n\n    for reg_model in [\"linear\", \"svr\", \"rfr\"]:\n        model = RegressionModel(model_type=reg_model)\n        model.train(x_train_features, y_train)\n\n        # Evaluate on validation and test sets\n        val_mse, val_mae, val_pearson = model.evaluate(x_valid_features, y_valid)\n        unseen_mse, unseen_mae, unseen_pearson = model.evaluate(x_unseen_features, y_unseen)\n\n        print(f\"Validation Performance ({current_method}, {reg_model}):\")\n        print(f\"MSE: {val_mse:.4f}, MAE: {val_mae:.4f}, Pearson Correlation: {val_pearson:.4f}\")\n\n        print(f\"Test Performance ({current_method}, {reg_model}):\")\n        print(f\"MSE: {unseen_mse:.4f}, MAE: {unseen_mae:.4f}, Pearson Correlation: {unseen_pearson:.4f}\")\n\n    \n        # Ensure x_unseen1, x_unseen2, and y_unseen are properly stored\n        results.append({\n            \"method\": current_method,\n            \"model\": reg_model,\n            \"pearson\": unseen_pearson,\n            \"unseen_predictions\": list(model.predict(x_unseen_features)),  # Convert predictions to list\n            \"unseen_features\": list(x_unseen_features) if x_unseen_features is not None else [],  # Ensure it's a list\n            \"x_unseen1\": list(x_unseen1) if x_unseen1 else [],  # Ensure stored properly\n            \"x_unseen2\": list(x_unseen2) if x_unseen2 else [],  # Ensure stored properly\n            \"y_unseen\": list(y_unseen) if y_unseen else []  # Ensure stored properly\n        })\n\n\n# Save all results as CSV files (for both validation and test)\nfor i, result in enumerate(results, start=1):\n    # Create DataFrame for test predictions\n    unseen_df = pd.DataFrame({\n        \"Original Sentence 1\": raw_unseen1,\n        \"Original Sentence 2\": raw_unseen2,\n        \"Preprocessed Sentence 1\": list(result.get(\"x_unseen1\", [])),  # Use get() to avoid KeyError\n        \"Preprocessed Sentence 2\": list(result.get(\"x_unseen2\", [])),  # Use get() to avoid KeyError\n        \"True Similarity Score\": [y * 5 for y in result.get(\"y_unseen\", [])],\n        \"Predicted Similarity Score\": [y * 5 for y in result.get(\"unseen_predictions\", [])]\n    })\n\n\n    # Save test result CSV\n    test_filename = f\"unseen_result_{i}_{result['method']}_{result['model']}.csv\"\n    unseen_df.to_csv(test_filename, index=False)\n    print(f\"Saved unseen result: {test_filename}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-05T13:31:54.675168Z","iopub.execute_input":"2025-02-05T13:31:54.675859Z","iopub.status.idle":"2025-02-05T13:31:55.651036Z","shell.execute_reply.started":"2025-02-05T13:31:54.675816Z","shell.execute_reply":"2025-02-05T13:31:55.650153Z"}},"outputs":[{"name":"stdout","text":"Using feature extraction method: averageCosine\nValidation Performance (averageCosine, linear):\nMSE: 0.0613, MAE: 0.2009, Pearson Correlation: 0.7154\nTest Performance (averageCosine, linear):\nMSE: 11.4567, MAE: 3.1978, Pearson Correlation: 0.0445\nValidation Performance (averageCosine, svr):\nMSE: 0.0629, MAE: 0.1983, Pearson Correlation: 0.7154\nTest Performance (averageCosine, svr):\nMSE: 11.3523, MAE: 3.1809, Pearson Correlation: 0.0445\nValidation Performance (averageCosine, rfr):\nMSE: 0.0637, MAE: 0.1809, Pearson Correlation: 0.7227\nTest Performance (averageCosine, rfr):\nMSE: 12.2501, MAE: 3.3116, Pearson Correlation: -0.0661\nUsing feature extraction method: wordDis\nValidation Performance (wordDis, linear):\nMSE: 0.0347, MAE: 0.1418, Pearson Correlation: 0.8507\nTest Performance (wordDis, linear):\nMSE: 12.3164, MAE: 3.3321, Pearson Correlation: 0.1035\nValidation Performance (wordDis, svr):\nMSE: 0.0347, MAE: 0.1399, Pearson Correlation: 0.8507\nTest Performance (wordDis, svr):\nMSE: 12.4221, MAE: 3.3480, Pearson Correlation: 0.1035\nValidation Performance (wordDis, rfr):\nMSE: 0.0517, MAE: 0.1624, Pearson Correlation: 0.7820\nTest Performance (wordDis, rfr):\nMSE: 12.4393, MAE: 3.3509, Pearson Correlation: 0.1145\nUsing feature extraction method: sifCos\nValidation Performance (sifCos, linear):\nMSE: 0.0546, MAE: 0.1852, Pearson Correlation: 0.7518\nTest Performance (sifCos, linear):\nMSE: 14.5462, MAE: 3.6594, Pearson Correlation: 0.2823\nValidation Performance (sifCos, svr):\nMSE: 0.0559, MAE: 0.1805, Pearson Correlation: 0.7518\nTest Performance (sifCos, svr):\nMSE: 15.0068, MAE: 3.7224, Pearson Correlation: 0.2823\nValidation Performance (sifCos, rfr):\nMSE: 0.0730, MAE: 0.1959, Pearson Correlation: 0.6855\nTest Performance (sifCos, rfr):\nMSE: 12.9411, MAE: 3.4239, Pearson Correlation: 0.0839\nSaved unseen result: unseen_result_1_averageCosine_linear.csv\nSaved unseen result: unseen_result_2_averageCosine_svr.csv\nSaved unseen result: unseen_result_3_averageCosine_rfr.csv\nSaved unseen result: unseen_result_4_wordDis_linear.csv\nSaved unseen result: unseen_result_5_wordDis_svr.csv\nSaved unseen result: unseen_result_6_wordDis_rfr.csv\nSaved unseen result: unseen_result_7_sifCos_linear.csv\nSaved unseen result: unseen_result_8_sifCos_svr.csv\nSaved unseen result: unseen_result_9_sifCos_rfr.csv\n","output_type":"stream"}],"execution_count":35},{"cell_type":"code","source":"print(\"Lengths of unseen data components:\")\nprint(f\"raw_unseen1: {len(raw_unseen1)}\")\nprint(f\"raw_unseen2: {len(raw_unseen2)}\")\nprint(f\"x_unseen1: {len(result.get('x_unseen1', []))}\")\nprint(f\"x_unseen2: {len(result.get('x_unseen2', []))}\")\nprint(f\"y_unseen: {len(result.get('y_unseen', []))}\")\nprint(f\"unseen_predictions: {len(result.get('unseen_predictions', []))}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-05T13:24:19.166064Z","iopub.execute_input":"2025-02-05T13:24:19.166748Z","iopub.status.idle":"2025-02-05T13:24:19.171864Z","shell.execute_reply.started":"2025-02-05T13:24:19.166711Z","shell.execute_reply":"2025-02-05T13:24:19.170951Z"}},"outputs":[{"name":"stdout","text":"Lengths of unseen data components:\nraw_unseen1: 25\nraw_unseen2: 25\nx_unseen1: 0\nx_unseen2: 0\ny_unseen: 0\nunseen_predictions: 0\n","output_type":"stream"}],"execution_count":32},{"cell_type":"code","source":"class KeyedVectorsWrapper:\n    \"\"\"Wrapper for KeyedVectors to provide a .wv attribute.\"\"\"\n    def __init__(self, keyed_vectors):\n        self.wv = keyed_vectors\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-05T07:57:20.526432Z","iopub.execute_input":"2025-02-05T07:57:20.527146Z","iopub.status.idle":"2025-02-05T07:57:20.531318Z","shell.execute_reply.started":"2025-02-05T07:57:20.527115Z","shell.execute_reply":"2025-02-05T07:57:20.530365Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nfrom gensim.models import KeyedVectors, Word2Vec\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\nfrom scipy.stats import pearsonr\n# ====== Function to Load and Preprocess Unseen Dataset ======\ndef load_unseen_indo_dataset(filename):\n    \"\"\"Load and preprocess new unseen Indonesian dataset.\"\"\"\n    df = pd.read_csv(filename)\n    data = [(row['answer'], row['response']) for _, row in df.iterrows()]  # No labels in unseen data\n    return data\n\ndef preprocess_unseen_data(data):\n    \"\"\"Preprocess unseen data for predictions.\"\"\"\n    sentence1 = [preprocess_text_indo(item[0]) for item in data]  # Preprocess first sentence\n    sentence2 = [preprocess_text_indo(item[1]) for item in data]  # Preprocess second sentence\n    return sentence1, sentence2\n\n# ====== Load Saved Embedding Model ======\ndef load_saved_embedding_model(model_path, method):\n    \"\"\"Load saved Word2Vec or FastText model and handle KeyedVectors compatibility.\"\"\"\n    if method == \"w2v\":\n        print(f\"Loading saved Word2Vec model from {model_path}...\")\n        embedding = Word2Vec.load(model_path)\n    elif method == \"fast\":\n        print(f\"Loading saved FastText model from {model_path}...\")\n        embedding = KeyedVectors.load_word2vec_format(model_path, binary=True)\n    else:\n        raise ValueError(\"Unsupported embedding method.\")\n    print(f\"Model loaded successfully!\")\n    return embedding\n\n# ====== Compute Evaluation Metrics ======\ndef compute_evaluation_metrics(y_true, y_pred):\n    \"\"\"Compute MSE, MAE, and Pearson Correlation.\"\"\"\n    mse = mean_squared_error(y_true, y_pred)\n    mae = mean_absolute_error(y_true, y_pred)\n    pearson_corr, _ = pearsonr(y_true, y_pred) if len(y_true) > 1 else (0, None)\n    return mse, mae, pearson_corr\n    \n# ====== Corrected Predict Unseen Data Function ======\ndef predict_unseen_data(model, embedding, unseen_file_path, output_file):\n    \"\"\"\n    Predict similarity scores on unseen dataset, save results, and compute evaluation metrics.\n    \n    Parameters:\n        model: Trained regression model.\n        embedding: Trained embedding model (Word2Vec or FastText).\n        unseen_file_path: Path to the unseen dataset CSV.\n        output_file: Path to save the results.\n    \"\"\"\n    # Load unseen dataset\n    unseen_data = pd.read_csv(unseen_file_path)\n    \n    # Extract true labels\n    if \"label\" in unseen_data.columns:\n        y_true = unseen_data[\"label\"].values / 5.0  # Normalize true labels to 0-1\n    else:\n        y_true = None\n\n    # Preprocess unseen dataset\n    unseen_sentence1 = [preprocess_text_indo(item) for item in unseen_data[\"answer\"]]\n    unseen_sentence2 = [preprocess_text_indo(item) for item in unseen_data[\"response\"]]\n    \n    # Extract features using the embedding\n    unseen_features = average_cosine_similarity(unseen_sentence1, unseen_sentence2, embedding)  # Pass embedding directly\n    \n    # Make predictions\n    predictions = model.predict(unseen_features)\n    \n    # Save predictions to CSV\n    df = pd.DataFrame({\n        \"Original Sentence 1\": unseen_data[\"answer\"],\n        \"Original Sentence 2\": unseen_data[\"response\"],\n        \"Predicted Similarity Score\": predictions * 5  # Rescale the scores to 0-5\n    })\n\n    # Include true labels if available\n    if y_true is not None:\n        df[\"True Similarity Score\"] = unseen_data[\"label\"]  # Keep original scale (0-5)\n    \n    df.to_csv(output_file, index=False)\n    print(f\"Saved predictions for unseen dataset to {output_file}\")\n    \n    # If true scores are provided, compute evaluation metrics\n    if y_true is not None:\n        y_true_rescaled = [y * 5 for y in y_true]  # Rescale true scores to 0-5\n        predictions_rescaled = predictions * 5  # Rescale predictions to 0-5\n        mse, mae, pearson_corr = compute_evaluation_metrics(y_true_rescaled, predictions_rescaled)\n        print(f\"Evaluation Metrics:\\n\"\n              f\"Mean Squared Error (MSE): {mse:.4f}\\n\"\n              f\"Mean Absolute Error (MAE): {mae:.4f}\\n\"\n              f\"Pearson Correlation: {pearson_corr:.4f}\")\n        return mse, mae, pearson_corr\n    else:\n        print(\"True scores not provided. Skipping evaluation metrics.\")\n        return None, None, None\n\n# ====== Main Workflow for Testing on Unseen Data ======\nunseen_file_path = \"/kaggle/input/testi-data/test-BuIng.csv\"\noutput_file = \"w2v-unseen_test_predictions_with_true_labels.csv\"\n\n# Load the saved embedding model\nembedding_model_path = \"/kaggle/working/id-domain_w2v.model\"  # Use the correct path\nembedding = load_saved_embedding_model(embedding_model_path, method=\"w2v\")  # Or \"fast\" for FastText\n\n# Predict and evaluate on the unseen dataset\nprint(\"Predicting on unseen dataset and computing evaluation metrics...\")\nmse, mae, pearson_corr = predict_unseen_data(model, embedding, unseen_file_path, output_file)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ====== Main Workflow ======\n# Paths and configurations\nfile_path = '/kaggle/input/indo-datasets/indodata.csv'\nword2vec_model_path = '/kaggle/working/model/idwiki_word2vec_200_new_lower.model'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T02:57:50.822719Z","iopub.execute_input":"2024-12-04T02:57:50.823498Z","iopub.status.idle":"2024-12-04T02:57:50.827349Z","shell.execute_reply.started":"2024-12-04T02:57:50.823464Z","shell.execute_reply":"2024-12-04T02:57:50.826465Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load and preprocess dataset\nraw_data = load_indo_dataset(file_path)\ntrain_data, valid_data, test_data = split_dataset(raw_data, valid_percentage=0.1, test_percentage=0.1)\n\nx_train1, x_train2, y_train = preprocess_data(train_data)\nx_valid1, x_valid2, y_valid = preprocess_data(valid_data)\nx_test1, x_test2, y_test = preprocess_data(test_data)\n\n# Load pre-trained Word2Vec model\nembedding = load_pretrained_word_embedding(word2vec_model_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T03:00:51.644819Z","iopub.execute_input":"2024-12-04T03:00:51.64565Z","iopub.status.idle":"2024-12-04T03:00:55.768768Z","shell.execute_reply.started":"2024-12-04T03:00:51.645615Z","shell.execute_reply":"2024-12-04T03:00:55.767978Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\n\n# Function to save raw and preprocessed data\ndef save_raw_and_preprocessed(raw_data, preprocessed_data1, preprocessed_data2, labels, filename):\n    # Convert to DataFrame\n    df = pd.DataFrame({\n        \"Raw Sentence 1\": [item[0] for item in raw_data],\n        \"Raw Sentence 2\": [item[1] for item in raw_data],\n        \"Preprocessed Sentence 1\": preprocessed_data1,\n        \"Preprocessed Sentence 2\": preprocessed_data2,\n        \"Label\": labels\n    })\n    df.to_csv(filename, index=False)\n    print(f\"Saved dataset to {filename}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T03:00:59.720174Z","iopub.execute_input":"2024-12-04T03:00:59.720887Z","iopub.status.idle":"2024-12-04T03:00:59.726025Z","shell.execute_reply.started":"2024-12-04T03:00:59.720854Z","shell.execute_reply":"2024-12-04T03:00:59.724976Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Save raw and preprocessed datasets\nsave_raw_and_preprocessed(train_data, x_train1, x_train2, y_train, \"train_data_with_preprocessing.csv\")\nsave_raw_and_preprocessed(valid_data, x_valid1, x_valid2, y_valid, \"valid_data_with_preprocessing.csv\")\nsave_raw_and_preprocessed(test_data, x_test1, x_test2, y_test, \"test_data_with_preprocessing.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T03:01:46.731211Z","iopub.execute_input":"2024-12-04T03:01:46.73157Z","iopub.status.idle":"2024-12-04T03:01:46.761478Z","shell.execute_reply.started":"2024-12-04T03:01:46.731541Z","shell.execute_reply":"2024-12-04T03:01:46.760487Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define your custom cosine similarity function\ndef cosine_similarity_custom(vec1, vec2):\n    \"\"\"\n    Custom implementation of cosine similarity.\n    \"\"\"\n    dot_product = np.dot(vec1, vec2)\n    norm_vec1 = np.linalg.norm(vec1)\n    norm_vec2 = np.linalg.norm(vec2)\n    \n    if norm_vec1 == 0 or norm_vec2 == 0:  # Handle zero-vector case\n        return 0.0\n    \n    return dot_product / (norm_vec1 * norm_vec2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T03:22:01.505762Z","iopub.execute_input":"2024-12-04T03:22:01.506653Z","iopub.status.idle":"2024-12-04T03:22:01.511702Z","shell.execute_reply.started":"2024-12-04T03:22:01.506617Z","shell.execute_reply":"2024-12-04T03:22:01.51075Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def avgCos(sentences1, sentences2, embedding, method=\"w2v\"):\n    \"\"\"\n    Generate sentence features using Word2Vec, FastText, BoW, or TF-IDF.\n    \"\"\"\n    if method in [\"w2v\", \"fast\"]:\n        similarities = []\n        for sent1, sent2 in zip(sentences1, sentences2):\n            tokens1 = [token for token in sent1.split() if token in embedding.wv]\n            tokens2 = [token for token in sent2.split() if token in embedding.wv]\n            \n            if not tokens1 or not tokens2:\n                similarities.append(0)\n                continue\n            \n            vec1 = np.mean([embedding.wv[token] for token in tokens1], axis=0).reshape(1, -1)\n            vec2 = np.mean([embedding.wv[token] for token in tokens2], axis=0).reshape(1, -1)\n            \n            similarities.append(cosine_similarity_custom(vec1.flatten(), vec2.flatten()))\n        return np.array(similarities).reshape(-1, 1)\n    \n    elif method in [\"bow\", \"tfidf\"]:\n        # Ensure both sets of sentences are vectorized and have the same shape\n        vectorized1 = embedding.transform(sentences1)\n        vectorized2 = embedding.transform(sentences2)\n        return np.abs(vectorized1 - vectorized2)  # Feature difference as input\n\n    else:\n        raise ValueError(\"Unsupported method.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T03:22:03.074219Z","iopub.execute_input":"2024-12-04T03:22:03.074976Z","iopub.status.idle":"2024-12-04T03:22:03.082112Z","shell.execute_reply.started":"2024-12-04T03:22:03.07493Z","shell.execute_reply":"2024-12-04T03:22:03.081134Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ====== Feature Extraction ======\nx_train_features = avgCos(x_train1, x_train2, embedding)\nx_valid_features = avgCos(x_valid1, x_valid2, embedding)\nx_test_features = avgCos(x_test1, x_test2, embedding)\n\n# ====== Regression Model and Evaluation ======\nfrom sklearn.linear_model import LinearRegression\n\nclass RegressionModel:\n    def __init__(self):\n        self.model = LinearRegression()\n\n    def train(self, X, y):\n        self.model.fit(X, y)\n\n    def evaluate(self, X, y):\n        predictions = self.model.predict(X)\n        mse = np.mean((y - predictions) ** 2)\n        mae = np.mean(np.abs(y - predictions))\n        corr = np.corrcoef(y, predictions)[0, 1]\n        return mse, mae, corr\n\n    def predict(self, X):\n        return self.model.predict(X)\n\n# Train and evaluate\nmodel = RegressionModel()\nmodel.train(x_train_features, y_train)\n\nval_mse, val_mae, val_corr = model.evaluate(x_valid_features, y_valid)\ntest_mse, test_mae, test_corr = model.evaluate(x_test_features, y_test)\n\nprint(f\"Validation: MSE={val_mse:.4f}, MAE={val_mae:.4f}, Pearson Correlation={val_corr:.4f}\")\nprint(f\"Test: MSE={test_mse:.4f}, MAE={test_mae:.4f}, Pearson Correlation={test_corr:.4f}\")\n\n# ====== Save Predictions ======\npredictions = model.predict(x_test_features)\nresults = pd.DataFrame({\n    \"Sentence 1\": x_test1,\n    \"Sentence 2\": x_test2,\n    \"True Label\": [y * 5 for y in y_test],\n    \"Predicted Label\": [y * 5 for y in predictions]\n})\nresults.to_csv(\"test_results.csv\", index=False)\nprint(\"Results saved to test_results.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T03:22:05.181517Z","iopub.execute_input":"2024-12-04T03:22:05.182243Z","iopub.status.idle":"2024-12-04T03:22:05.264122Z","shell.execute_reply.started":"2024-12-04T03:22:05.182208Z","shell.execute_reply":"2024-12-04T03:22:05.263249Z"}},"outputs":[],"execution_count":null}]}