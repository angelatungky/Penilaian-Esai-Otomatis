{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9948346,"sourceType":"datasetVersion","datasetId":6117586},{"sourceId":10481411,"sourceType":"datasetVersion","datasetId":6490086}],"dockerImageVersionId":30839,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import warnings\nimport numpy as np\nimport pandas as pd\nimport random\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.svm import SVR\nfrom gensim.models import Word2Vec, FastText\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\nimport nltk","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-01-20T04:25:29.882169Z","iopub.execute_input":"2025-01-20T04:25:29.882403Z","iopub.status.idle":"2025-01-20T04:25:50.687832Z","shell.execute_reply.started":"2025-01-20T04:25:29.882383Z","shell.execute_reply":"2025-01-20T04:25:50.686817Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# Ensure required NLTK data is downloaded\nnltk.download('punkt')\nnltk.download('stopwords')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-20T04:25:50.688906Z","iopub.execute_input":"2025-01-20T04:25:50.689627Z","iopub.status.idle":"2025-01-20T04:25:50.865203Z","shell.execute_reply.started":"2025-01-20T04:25:50.689589Z","shell.execute_reply":"2025-01-20T04:25:50.864059Z"}},"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n","output_type":"stream"},{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"# ====== Dataset Loading and Splitting ======\ndef load_custom_dataset(filename):\n    data = []\n    with open(filename, \"r\") as file:\n        for line in file:\n            question, response, answer, label = line.strip().split('\\t')\n            label = float(label) / 5.0  # Normalize to [0, 1]\n            data.append((response, answer, label))\n    return data","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-20T04:25:50.866224Z","iopub.execute_input":"2025-01-20T04:25:50.866514Z","iopub.status.idle":"2025-01-20T04:25:50.872207Z","shell.execute_reply.started":"2025-01-20T04:25:50.866490Z","shell.execute_reply":"2025-01-20T04:25:50.871021Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"def split_dataset(data, valid_percentage, test_percentage):\n    length = len(data)\n    random.shuffle(data)\n    train = data[:int(length * (1 - valid_percentage - test_percentage))]\n    valid = data[int(length * (1 - valid_percentage - test_percentage)):int(length * (1 - test_percentage))]\n    test = data[int(length * (1 - test_percentage)):]\n    return train, valid, test","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-16T02:28:02.818861Z","iopub.execute_input":"2025-01-16T02:28:02.819317Z","iopub.status.idle":"2025-01-16T02:28:02.82551Z","shell.execute_reply.started":"2025-01-16T02:28:02.819284Z","shell.execute_reply":"2025-01-16T02:28:02.824203Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ====== Text Preprocessing ======\ndef preprocess_text(text):\n    # Normalize the text by replacing curly apostrophes with straight ones\n    text = text.replace(\"‘\", \"'\").replace(\"’\", \"'\").lower()  # Case folding and normalization\n    \n    # Tokenize the text\n    tokens = nltk.word_tokenize(text)\n    \n    # Remove non-alphabetic tokens and stopwords\n    stop_words = set(nltk.corpus.stopwords.words(\"english\"))\n    tokens = [word for word in tokens if word.isalpha() and word not in stop_words]\n    return \" \".join(tokens)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-20T04:25:50.873349Z","iopub.execute_input":"2025-01-20T04:25:50.873670Z","iopub.status.idle":"2025-01-20T04:25:50.887873Z","shell.execute_reply.started":"2025-01-20T04:25:50.873613Z","shell.execute_reply":"2025-01-20T04:25:50.886884Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"def preprocess_data(data):\n    sentence1 = [preprocess_text(item[0]) for item in data]\n    sentence2 = [preprocess_text(item[1]) for item in data]\n    labels = [item[2] for item in data]\n    return sentence1, sentence2, labels","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-20T04:25:50.888859Z","iopub.execute_input":"2025-01-20T04:25:50.889232Z","iopub.status.idle":"2025-01-20T04:25:50.906592Z","shell.execute_reply.started":"2025-01-20T04:25:50.889206Z","shell.execute_reply":"2025-01-20T04:25:50.905521Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"import pandas as pd\n\n# Function to save raw and preprocessed data\ndef save_raw_and_preprocessed(raw_data, preprocessed_data1, preprocessed_data2, labels, filename):\n    # Convert to DataFrame\n    df = pd.DataFrame({\n        \"Raw Sentence 1\": [item[0] for item in raw_data],\n        \"Raw Sentence 2\": [item[1] for item in raw_data],\n        \"Preprocessed Sentence 1\": preprocessed_data1,\n        \"Preprocessed Sentence 2\": preprocessed_data2,\n        \"Label\": labels\n    })\n    df.to_csv(filename, index=False)\n    print(f\"Saved dataset to {filename}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-16T02:28:09.023719Z","iopub.execute_input":"2025-01-16T02:28:09.02414Z","iopub.status.idle":"2025-01-16T02:28:09.031571Z","shell.execute_reply.started":"2025-01-16T02:28:09.024102Z","shell.execute_reply":"2025-01-16T02:28:09.029665Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import random\n# ====== Main Workflow ======\n# Load and preprocess dataset\nraw_data = load_custom_dataset(\"/kaggle/input/dataset/expand.txt\")\ntrain_data, valid_data, test_data = split_dataset(raw_data, valid_percentage=0.1, test_percentage=0.1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-16T02:28:14.654259Z","iopub.execute_input":"2025-01-16T02:28:14.654672Z","iopub.status.idle":"2025-01-16T02:28:17.098015Z","shell.execute_reply.started":"2025-01-16T02:28:14.654643Z","shell.execute_reply":"2025-01-16T02:28:17.096722Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ====== Load Dataset ======\n# Load train, validation, and test datasets from CSV files\ntrain_file = \"/kaggle/input/w2v-dataset/train_data_ENG-W2V.csv\"\nvalid_file = \"/kaggle/input/w2v-dataset/valid_data_ENG-W2V.csv\"\ntest_file = \"/kaggle/input/w2v-dataset/test_data_ENG-W2V.csv\"\n\n# Read datasets\ntrain_data = pd.read_csv(train_file).values\nvalid_data = pd.read_csv(valid_file).values\ntest_data = pd.read_csv(test_file).values\n\n# ====== Preprocessing ======\nx_train1, x_train2, y_train = preprocess_data(train_data)\nx_valid1, x_valid2, y_valid = preprocess_data(valid_data)\nx_test1, x_test2, y_test = preprocess_data(test_data)\n\n# Output shapes for verification\nprint(f\"Train data: {len(x_train1)} pairs, {len(y_train)} labels\")\nprint(f\"Validation data: {len(x_valid1)} pairs, {len(y_valid)} labels\")\nprint(f\"Test data: {len(x_test1)} pairs, {len(y_test)} labels\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-20T04:27:50.484207Z","iopub.execute_input":"2025-01-20T04:27:50.484554Z","iopub.status.idle":"2025-01-20T04:27:52.831128Z","shell.execute_reply.started":"2025-01-20T04:27:50.484529Z","shell.execute_reply":"2025-01-20T04:27:52.829976Z"}},"outputs":[{"name":"stdout","text":"Train data: 2916 pairs, 2916 labels\nValidation data: 365 pairs, 365 labels\nTest data: 365 pairs, 365 labels\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"# Save raw and preprocessed datasets\nsave_raw_and_preprocessed(train_data, x_train1, x_train2, y_train, \"train_data_with_preprocessing.csv\")\nsave_raw_and_preprocessed(valid_data, x_valid1, x_valid2, y_valid, \"valid_data_with_preprocessing.csv\")\nsave_raw_and_preprocessed(test_data, x_test1, x_test2, y_test, \"test_data_with_preprocessing.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-16T02:28:19.669868Z","iopub.execute_input":"2025-01-16T02:28:19.670299Z","iopub.status.idle":"2025-01-16T02:28:19.743383Z","shell.execute_reply.started":"2025-01-16T02:28:19.670269Z","shell.execute_reply":"2025-01-16T02:28:19.742156Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Check a few random samples from the preprocessed training data\nprint(\"Preprocessed x_train1 Samples:\")\nfor i in random.sample(range(len(x_train1)), 5):  # Randomly select 5 indices\n    print(f\"Original Sentence 1: {train_data[i][0]}\")\n    print(f\"Preprocessed Sentence 1: {x_train1[i]}\")\n    print()\n\nprint(\"Preprocessed x_train2 Samples:\")\nfor i in random.sample(range(len(x_train2)), 5):  # Randomly select 5 indices\n    print(f\"Original Sentence 2: {train_data[i][1]}\")\n    print(f\"Preprocessed Sentence 2: {x_train2[i]}\")\n    print()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-20T04:28:00.630545Z","iopub.execute_input":"2025-01-20T04:28:00.631093Z","iopub.status.idle":"2025-01-20T04:28:00.641431Z","shell.execute_reply.started":"2025-01-20T04:28:00.631053Z","shell.execute_reply":"2025-01-20T04:28:00.639029Z"}},"outputs":[{"name":"stdout","text":"Preprocessed x_train1 Samples:\nOriginal Sentence 1: A data structure in c plus plus which is a collection of data that is kept in order First in first out\nPreprocessed Sentence 1: data structure c plus plus collection data kept order first first\n\nOriginal Sentence 1: Data members - LRB - attributes - RRB - and member functions\nPreprocessed Sentence 1: data members lrb attributes rrb member functions\n\nOriginal Sentence 1: enqueue which adds data to the queue and dequeue which deletes data from the queue\nPreprocessed Sentence 1: enqueue adds data queue dequeue deletes data queue\n\nOriginal Sentence 1: You implement a list in which the head pointer points to the element most recently pushed onto the list and the pop function changes the head pointer to point to the next to last element in the list and removes the element head pointer previously pointed to\nPreprocessed Sentence 1: implement list head pointer points element recently pushed onto list pop function changes head pointer point next last element list removes element head pointer previously pointed\n\nOriginal Sentence 1: a static array will store the new values that were assigned to each of its elements meaning if you call a function twice it will use the last values that were returned the first time if you do not declare it static then the new values will not be stored and will be reset to their original value\nPreprocessed Sentence 1: static array store new values assigned elements meaning call function twice use last values returned first time declare static new values stored reset original value\n\nPreprocessed x_train2 Samples:\nOriginal Sentence 2: it treats them as the same function\nPreprocessed Sentence 2: treats function\n\nOriginal Sentence 2: If the node is a leaf just set it is parent pointer to null and delete it if it has a single child set the parent pointer to the child and delete; if it has two children set the node to one of the middle children and remove that child from its previous position as a leaf - LRB - rightmost child of the left subtree or leftmost child of the right subtree - RRB -\nPreprocessed Sentence 2: node leaf set parent pointer null delete single child set parent pointer child delete two children set node one middle children remove child previous position leaf lrb rightmost child left subtree leftmost child right subtree rrb\n\nOriginal Sentence 2: the head object is passed to the function\nPreprocessed Sentence 2: head object passed function\n\nOriginal Sentence 2: Binary search trees are a fundamental data structure used to construct more abstract data structures such as sets multisets and associative arrays\nPreprocessed Sentence 2: binary search trees fundamental data structure used construct abstract data structures sets multisets associative arrays\n\nOriginal Sentence 2: A data type that contains a pointer to at least the next element in a list\nPreprocessed Sentence 2: data type contains pointer least next element list\n\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"def build_domain_specific_word_embedding(sentences, method=\"w2v\", epochs=30):\n    tokenized_sentences = [sentence.split() for sentence in sentences]\n    \n    if method == \"w2v\":\n        save_path = \"new-param-domain_w2v.model\"\n        model = Word2Vec(\n            vector_size=200,\n            window=4,\n            min_count=1,\n            workers=4,\n            sg=1,  # CBOW (0), set to 1 for Skip-Gram\n            sample=6e-5,\n            alpha=0.03,\n            min_alpha=0.0007,\n            negative=15\n        )\n    elif method == \"fast\":\n        save_path = \"new-param-domain_fasttext.model\"  # Different filename for FastText\n        model = FastText(\n            vector_size=300,\n            window=3,\n            min_count=1,\n            workers=4\n        )\n    else:\n        raise ValueError(\"Unsupported embedding method.\")\n\n    print(f\"Building vocabulary with {len(tokenized_sentences)} sentences\")\n    model.build_vocab(tokenized_sentences)\n\n    print(f\"Training the model for {epochs} epochs\")\n    model.train(\n        tokenized_sentences,\n        total_examples=model.corpus_count,\n        epochs=epochs\n    )\n\n    model.init_sims(replace=True)\n    model.save(save_path)\n    print(f\"Model saved at {save_path}\")\n    \n    return model\n\ndef load_pretrained_word_embedding(load_path=\"new-param-domain_w2v.model\"):\n    return Word2Vec.load(load_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-20T04:37:00.617760Z","iopub.execute_input":"2025-01-20T04:37:00.618156Z","iopub.status.idle":"2025-01-20T04:37:00.625385Z","shell.execute_reply.started":"2025-01-20T04:37:00.618125Z","shell.execute_reply":"2025-01-20T04:37:00.624221Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"# Define your custom cosine similarity function\ndef cosine_similarity_custom(vec1, vec2):\n    \"\"\"\n    Custom implementation of cosine similarity.\n    \"\"\"\n    dot_product = np.dot(vec1, vec2)\n    norm_vec1 = np.linalg.norm(vec1)\n    norm_vec2 = np.linalg.norm(vec2)\n    \n    if norm_vec1 == 0 or norm_vec2 == 0:  # Handle zero-vector case\n        return 0.0\n    \n    return dot_product / (norm_vec1 * norm_vec2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-20T04:37:06.340190Z","iopub.execute_input":"2025-01-20T04:37:06.340504Z","iopub.status.idle":"2025-01-20T04:37:06.346415Z","shell.execute_reply.started":"2025-01-20T04:37:06.340481Z","shell.execute_reply":"2025-01-20T04:37:06.345379Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"def average_cosine_similarity(kalimat1, kalimat2, embedding, method=\"w2v\"):\n    \"\"\"\n    Generate sentence features using Word2Vec, FastText.\n    \"\"\"\n    if method in [\"w2v\", \"fast\"]:\n        similarities = []\n        for text1, text2 in zip(kalimat1, kalimat2):\n            tokens1 = [word for word in text1.split() if word in embedding.wv]\n            tokens2 = [word for word in text2.split() if word in embedding.wv]\n            \n            if not tokens1 or not tokens2:\n                similarities.append(0)\n                continue\n            \n            vec1 = np.mean([embedding.wv[word] for word in tokens1], axis=0).reshape(1, -1)\n            vec2 = np.mean([embedding.wv[word] for word in tokens2], axis=0).reshape(1, -1)\n            \n            similarities.append(cosine_similarity_custom(vec1.flatten(), vec2.flatten()))\n        return np.array(similarities).reshape(-1, 1)\n\n    else:\n        raise ValueError(\"Unsupported method.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-20T05:38:19.011643Z","iopub.execute_input":"2025-01-20T05:38:19.012020Z","iopub.status.idle":"2025-01-20T05:38:19.019129Z","shell.execute_reply.started":"2025-01-20T05:38:19.011989Z","shell.execute_reply":"2025-01-20T05:38:19.017839Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"def word_distance(text_group1, text_group2, embed_model):\n    similarity_scores = []\n    for group1, group2 in zip(text_group1, text_group2):\n        group1_tokens = [word for word in group1.split() if word in embed_model.wv]\n        group2_tokens = [word for word in group2.split() if word in embed_model.wv]\n\n        if not group1_tokens or not group2_tokens:\n            similarity_scores.append(0)\n        else:\n            similarity_scores.append(-embed_model.wv.wmdistance(group1_tokens, group2_tokens))\n\n    return np.array(similarity_scores).reshape(-1, 1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-20T04:45:07.470982Z","iopub.execute_input":"2025-01-20T04:45:07.471317Z","iopub.status.idle":"2025-01-20T04:45:07.477053Z","shell.execute_reply.started":"2025-01-20T04:45:07.471293Z","shell.execute_reply":"2025-01-20T04:45:07.475908Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"# for SIF\nfrom sklearn.decomposition import TruncatedSVD\n    \ndef eliminate_first_component(matrix):\n    svd_model = TruncatedSVD(n_components=1, random_state=42)\n    svd_model.fit(matrix)\n    principal_component = svd_model.components_\n    return matrix - matrix.dot(principal_component.T) * principal_component\n    \ndef sif_cos(text_group1, text_group2, embed_model, frequency_map, smoothing_factor=0.001):\n    freq_sum = sum(frequency_map.values())\n    all_embeddings = []\n\n    for group1, group2 in zip(text_group1, text_group2):\n        tokens1 = [word for word in group1.split() if word in embed_model.wv]\n        tokens2 = [word for word in group2.split() if word in embed_model.wv]\n\n        if not tokens1 or not tokens2:\n            all_embeddings.extend([np.zeros(embed_model.vector_size), np.zeros(embed_model.vector_size)])\n            continue\n\n        weights1 = [smoothing_factor / (smoothing_factor + frequency_map.get(word, 1e-5) / freq_sum) for word in tokens1]\n        weights2 = [smoothing_factor / (smoothing_factor + frequency_map.get(word, 1e-5) / freq_sum) for word in tokens2]\n\n        embedding1 = np.average([embed_model.wv[word] for word in tokens1], axis=0, weights=weights1)\n        embedding2 = np.average([embed_model.wv[word] for word in tokens2], axis=0, weights=weights2)\n\n        all_embeddings.extend([embedding1, embedding2])\n\n    all_embeddings = np.array(all_embeddings)\n    all_embeddings = eliminate_first_component(all_embeddings)\n\n    similarities = [\n        (\n            np.dot(all_embeddings[i], all_embeddings[i + 1]) /\n            (np.linalg.norm(all_embeddings[i]) * np.linalg.norm(all_embeddings[i + 1]))\n            if np.linalg.norm(all_embeddings[i]) > 0 and np.linalg.norm(all_embeddings[i + 1]) > 0 else 0\n        )\n        for i in range(0, len(all_embeddings), 2)\n    ]\n\n    return np.array(similarities).reshape(-1, 1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-20T05:25:17.517950Z","iopub.execute_input":"2025-01-20T05:25:17.518404Z","iopub.status.idle":"2025-01-20T05:25:17.530540Z","shell.execute_reply.started":"2025-01-20T05:25:17.518373Z","shell.execute_reply":"2025-01-20T05:25:17.529343Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"\n# ====== Regression Model ======\nclass RegressionModel:\n    def __init__(self, model_type=\"linear\"):\n        if model_type == \"linear\":\n            self.model = self.LinearRegressionCustom()\n        elif model_type == \"svr\":\n            self.model = SVR(kernel=\"linear\")\n        elif model_type == \"rfr\":\n            self.model = self.RandomForestCustom()\n        else:\n            raise ValueError(\"Unsupported model type.\")\n\n    class LinearRegressionCustom:\n        def __init__(self):\n            self.weights = None\n\n        def fit(self, X, y):\n            X = np.c_[np.ones(X.shape[0]), X]  # Add bias term\n            self.weights = np.linalg.pinv(X.T @ X) @ X.T @ y\n        def fit(self, X, y):\n            # Convert y to NumPy and ensure matching rows\n            y = np.array(y)\n            if X.shape[0] != y.shape[0]:\n                raise ValueError(f\"Shape mismatch: X has {X.shape[0]} rows but y has {y.shape[0]} rows.\")\n            \n            X = np.c_[np.ones(X.shape[0]), X]  # Add bias term\n            self.weights = np.linalg.pinv(X.T @ X) @ X.T @ y\n\n        def predict(self, X):\n            X = np.c_[np.ones(X.shape[0]), X]  # Add bias term\n            return X @ self.weights\n\n    class RandomForestCustom:\n        def __init__(self, n_estimators=100, max_depth=None):\n            self.n_estimators = n_estimators\n            self.max_depth = max_depth\n            self.trees = []\n    \n        def fit(self, X, y):\n            from sklearn.tree import DecisionTreeRegressor\n    \n            # Ensure y is a NumPy array\n            y = np.array(y)\n    \n            n_samples = X.shape[0]\n    \n            for _ in range(self.n_estimators):\n                # Ensure indices are integers for proper indexing\n                indices = np.random.choice(range(n_samples), size=n_samples, replace=True)\n                X_sample = X[indices]\n                y_sample = y[indices]\n                tree = DecisionTreeRegressor(max_depth=self.max_depth)\n                tree.fit(X_sample, y_sample)\n                self.trees.append(tree)\n    \n        def predict(self, X):\n            # Aggregate predictions from all trees\n            predictions = np.array([tree.predict(X) for tree in self.trees])\n            return np.mean(predictions, axis=0)\n\n    @staticmethod\n    def mean_squared_error(y_true, y_pred):\n        squared_errors = [(true - pred) ** 2 for true, pred in zip(y_true, y_pred)]\n        return sum(squared_errors) / len(squared_errors)\n\n    @staticmethod\n    def mean_absolute_error(y_true, y_pred):\n        absolute_errors = [abs(true - pred) for true, pred in zip(y_true, y_pred)]\n        return sum(absolute_errors) / len(absolute_errors)\n\n    @staticmethod\n    def pearsonr(x, y):\n        mean_x = sum(x) / len(x)\n        mean_y = sum(y) / len(y)\n        numerator = sum((xi - mean_x) * (yi - mean_y) for xi, yi in zip(x, y))\n        denominator = ((sum((xi - mean_x) ** 2 for xi in x) * sum((yi - mean_y) ** 2 for yi in y)) ** 0.5)\n        return (numerator / denominator if denominator != 0 else 0.0, None)\n\n    def train(self, x_train, y_train):\n        self.model.fit(x_train, y_train)\n\n    def evaluate(self, x, y):\n        predictions = self.model.predict(x)\n        mse = self.mean_squared_error(y, predictions)\n        mae = self.mean_absolute_error(y, predictions)\n        pearson_corr, _ = self.pearsonr(y, predictions)\n        return mse, mae, pearson_corr\n\n    def predict(self, x):\n        return self.model.predict(x)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-20T05:26:02.145685Z","iopub.execute_input":"2025-01-20T05:26:02.146068Z","iopub.status.idle":"2025-01-20T05:26:02.161113Z","shell.execute_reply.started":"2025-01-20T05:26:02.146039Z","shell.execute_reply":"2025-01-20T05:26:02.159863Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"def feature_extraction(train_set1, train_set2, val_set1, val_set2, test_set, embed_model, frequency_map, method):\n    if method == \"averageCosine\":\n        train_similarities = average_cosine_similarity(train_set1, train_set2, embed_model)\n        val_similarities = average_cosine_similarity(val_set1, val_set2, embed_model)\n        test_similarities = average_cosine_similarity(test_set[\"sentence1\"], test_set[\"sentence2\"], embed_model)\n    elif method == \"wordDis\":\n        train_similarities = word_distance(train_set1, train_set2, embed_model)\n        val_similarities = word_distance(val_set1, val_set2, embed_model)\n        test_similarities = word_distance(test_set[\"sentence1\"], test_set[\"sentence2\"], embed_model)\n    elif method == \"sifCos\":\n        train_similarities = sif_cos(train_set1, train_set2, embed_model, frequency_map)\n        val_similarities = sif_cos(val_set1, val_set2, embed_model, frequency_map)\n        test_similarities = sif_cos(test_set[\"sentence1\"], test_set[\"sentence2\"], embed_model, frequency_map)\n    else:\n        raise ValueError(f\"Feature extraction method '{method}' is not supported.\")\n\n    return np.array(train_similarities), np.array(val_similarities), np.array(test_similarities)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-20T05:28:41.627492Z","iopub.execute_input":"2025-01-20T05:28:41.627838Z","iopub.status.idle":"2025-01-20T05:28:41.634683Z","shell.execute_reply.started":"2025-01-20T05:28:41.627813Z","shell.execute_reply":"2025-01-20T05:28:41.633532Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"# ====== Testing Predictions ======\ndef print_test_predictions(model, x_test_features, x_test1, x_test2, y_test):\n    predictions = model.predict(x_test_features)\n    true_scores = np.array(y_test) * 5.0\n    predicted_scores = predictions * 5.0\n    pearson_corr, _ = RegressionModel.pearsonr(true_scores, predicted_scores)\n    results = pd.DataFrame({\n        \"Sentence 1\": x_test1,\n        \"Sentence 2\": x_test2,\n        \"True Similarity Score\": true_scores,\n        \"Predicted Similarity Score\": predicted_scores\n    })\n    print(results.head(10))\n    print(f\"Pearson Correlation: {pearson_corr:.4f}\")\n    results.to_csv(\"train_predictions.csv\", index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-20T05:28:50.858292Z","iopub.execute_input":"2025-01-20T05:28:50.858636Z","iopub.status.idle":"2025-01-20T05:28:50.864409Z","shell.execute_reply.started":"2025-01-20T05:28:50.858609Z","shell.execute_reply":"2025-01-20T05:28:50.863341Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"pip install cvxopt\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-20T05:28:52.749492Z","iopub.execute_input":"2025-01-20T05:28:52.749835Z","iopub.status.idle":"2025-01-20T05:28:57.973735Z","shell.execute_reply.started":"2025-01-20T05:28:52.749807Z","shell.execute_reply":"2025-01-20T05:28:57.972385Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: cvxopt in /usr/local/lib/python3.10/dist-packages (1.3.2)\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"pip install POT\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-20T05:28:57.975533Z","iopub.execute_input":"2025-01-20T05:28:57.976001Z","iopub.status.idle":"2025-01-20T05:29:02.959579Z","shell.execute_reply.started":"2025-01-20T05:28:57.975959Z","shell.execute_reply":"2025-01-20T05:29:02.958226Z"}},"outputs":[{"name":"stdout","text":"Collecting POT\n  Downloading POT-0.9.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (34 kB)\nRequirement already satisfied: numpy>=1.16 in /usr/local/lib/python3.10/dist-packages (from POT) (1.26.4)\nRequirement already satisfied: scipy>=1.6 in /usr/local/lib/python3.10/dist-packages (from POT) (1.13.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.16->POT) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.16->POT) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.16->POT) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.16->POT) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.16->POT) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.16->POT) (2.4.1)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.16->POT) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.16->POT) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.16->POT) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.16->POT) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.16->POT) (2024.2.0)\nDownloading POT-0.9.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (865 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m865.6/865.6 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: POT\nSuccessfully installed POT-0.9.5\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"import pandas as pd\n\n# ====== Frequency Computation for SIF ======\nfrom collections import Counter\nall_sentences = x_train1 + x_train2 + x_valid1 + x_valid2 + x_test1 + x_test2\n# Compute word frequencies for SIF\nall_tokens = [token for sentence in all_sentences for token in sentence.split()]\nfreqs = Counter(all_tokens)\n\n# ====== Feature Extraction and Model Evaluation ======\nmethods = [\"averageCosine\", \"wordDis\", \"sifCos\"]\nembedding_methods = {\"w2v\": \"Word2Vec\", \"fast\": \"FastText\"}  # Map method names to embeddings\n\n# Store results for all methods and models\nresults = []\n\n# Loop over embedding methods and feature extraction methods\nfor embedding_type in [\"w2v\", \"fast\"]:\n    print(f\"Building embedding model: {embedding_methods[embedding_type]}\")\n    embedding = build_domain_specific_word_embedding(all_sentences, method=embedding_type)\n\n    for method in methods:\n        current_method = method\n        print(f\"Using feature extraction method: {current_method}\")\n        \n        x_train_features, x_valid_features, x_test_features = feature_extraction(\n            x_train1, x_train2, x_valid1, x_valid2, {\"sentence1\": x_test1, \"sentence2\": x_test2}, embedding, freqs, current_method\n        )\n\n        for reg_model in [\"linear\", \"svr\", \"rfr\"]:\n            model = RegressionModel(model_type=reg_model)\n            model.train(x_train_features, y_train)\n\n            # Evaluate on validation and test sets\n            val_mse, val_mae, val_pearson = model.evaluate(x_valid_features, y_valid)\n            test_mse, test_mae, test_pearson = model.evaluate(x_test_features, y_test)\n\n            print(f\"Validation Performance ({embedding_type}, {current_method}, {reg_model}):\")\n            print(f\"MSE: {val_mse:.4f}, MAE: {val_mae:.4f}, Pearson Correlation: {val_pearson:.4f}\")\n\n            print(f\"Test Performance ({embedding_type}, {current_method}, {reg_model}):\")\n            print(f\"MSE: {test_mse:.4f}, MAE: {test_mae:.4f}, Pearson Correlation: {test_pearson:.4f}\")\n\n            # Store results\n            results.append({\n                \"embedding_type\": embedding_type,\n                \"method\": current_method,\n                \"model\": reg_model,\n                \"pearson\": test_pearson,\n                \"test_predictions\": model.predict(x_test_features),\n                \"test_features\": x_test_features,\n                \"x_test1\": x_test1,\n                \"x_test2\": x_test2,\n                \"y_test\": y_test\n            })\n\n# Save all results as CSV files (for both validation and test)\nfor i, result in enumerate(results, start=1):\n    # Retrieve original raw sentences for validation and test\n    raw_valid1 = [item[0] for item in valid_data]  # Original raw Sentence 1 for validation\n    raw_valid2 = [item[1] for item in valid_data]  # Original raw Sentence 2 for validation\n    raw_test1 = [item[0] for item in test_data]  # Original raw Sentence 1 for test\n    raw_test2 = [item[1] for item in test_data]  # Original raw Sentence 2 for test\n    val_predictions = model.predict(x_valid_features)  # Prediksi untuk data validasi\n    # Create DataFrame for validation predictions\n    val_df = pd.DataFrame({\n        \"Original Sentence 1\": raw_valid1,  # Append raw sentence 1\n        \"Original Sentence 2\": raw_valid2,  # Append raw sentence 2\n        \"Preprocessed Sentence 1\": x_valid1,  # Preprocessed validation sentence 1\n        \"Preprocessed Sentence 2\": x_valid2,  # Preprocessed validation sentence 2\n        \"True Similarity Score\": [y * 5 for y in y_valid],  # Rescale validation true scores\n        \"Predicted Similarity Score\": [y * 5 for y in val_predictions]  # Validation predictions rescaled\n    })\n\n    # Save validation result CSV\n    val_filename = f\"val_result_{i}_{result['embedding_type']}_{result['method']}_{result['model']}.csv\"\n    val_df.to_csv(val_filename, index=False)\n    print(f\"Saved validation result: {val_filename}\")\n\n    # Create DataFrame for test predictions\n    test_df = pd.DataFrame({\n        \"Original Sentence 1\": raw_test1,  # Append raw sentence 1\n        \"Original Sentence 2\": raw_test2,  # Append raw sentence 2\n        \"Preprocessed Sentence 1\": result[\"x_test1\"],  # Preprocessed test sentence 1\n        \"Preprocessed Sentence 2\": result[\"x_test2\"],  # Preprocessed test sentence 2\n        \"True Similarity Score\": [y * 5 for y in result[\"y_test\"]],  # Rescale to [0, 5]\n        \"Predicted Similarity Score\": [y * 5 for y in result[\"test_predictions\"]]  # Predicted scores rescaled\n    })\n\n    # Save test result CSV\n    test_filename = f\"test_result_{i}_{result['embedding_type']}_{result['method']}_{result['model']}.csv\"\n    test_df.to_csv(test_filename, index=False)\n    print(f\"Saved test result: {test_filename}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-20T05:44:42.963427Z","iopub.execute_input":"2025-01-20T05:44:42.963973Z","iopub.status.idle":"2025-01-20T05:45:18.341506Z","shell.execute_reply.started":"2025-01-20T05:44:42.963888Z","shell.execute_reply":"2025-01-20T05:45:18.340493Z"}},"outputs":[{"name":"stdout","text":"Building embedding model: Word2Vec\nBuilding vocabulary with 7292 sentences\nTraining the model for 30 epochs\n","output_type":"stream"},{"name":"stderr","text":"<ipython-input-9-6c009ec4bb5e>:38: DeprecationWarning: Call to deprecated `init_sims` (Gensim 4.0.0 implemented internal optimizations that make calls to init_sims() unnecessary. init_sims() is now obsoleted and will be completely removed in future versions. See https://github.com/RaRe-Technologies/gensim/wiki/Migrating-from-Gensim-3.x-to-4).\n  model.init_sims(replace=True)\n","output_type":"stream"},{"name":"stdout","text":"Model saved at new-param-domain_w2v.model\nUsing feature extraction method: averageCosine\nValidation Performance (w2v, averageCosine, linear):\nMSE: 0.0420, MAE: 0.1631, Pearson Correlation: 0.4196\nTest Performance (w2v, averageCosine, linear):\nMSE: 0.0404, MAE: 0.1640, Pearson Correlation: 0.5256\nValidation Performance (w2v, averageCosine, svr):\nMSE: 0.0417, MAE: 0.1566, Pearson Correlation: 0.4196\nTest Performance (w2v, averageCosine, svr):\nMSE: 0.0408, MAE: 0.1592, Pearson Correlation: 0.5256\nValidation Performance (w2v, averageCosine, rfr):\nMSE: 0.0513, MAE: 0.1696, Pearson Correlation: 0.4024\nTest Performance (w2v, averageCosine, rfr):\nMSE: 0.0545, MAE: 0.1751, Pearson Correlation: 0.4006\nUsing feature extraction method: wordDis\nValidation Performance (w2v, wordDis, linear):\nMSE: 0.0438, MAE: 0.1651, Pearson Correlation: 0.3796\nTest Performance (w2v, wordDis, linear):\nMSE: 0.0445, MAE: 0.1641, Pearson Correlation: 0.4436\nValidation Performance (w2v, wordDis, svr):\nMSE: 0.0436, MAE: 0.1603, Pearson Correlation: 0.3796\nTest Performance (w2v, wordDis, svr):\nMSE: 0.0458, MAE: 0.1644, Pearson Correlation: 0.4436\nValidation Performance (w2v, wordDis, rfr):\nMSE: 0.0545, MAE: 0.1763, Pearson Correlation: 0.3679\nTest Performance (w2v, wordDis, rfr):\nMSE: 0.0584, MAE: 0.1824, Pearson Correlation: 0.3522\nUsing feature extraction method: sifCos\nValidation Performance (w2v, sifCos, linear):\nMSE: 0.0491, MAE: 0.1761, Pearson Correlation: 0.2445\nTest Performance (w2v, sifCos, linear):\nMSE: 0.0458, MAE: 0.1685, Pearson Correlation: 0.4158\nValidation Performance (w2v, sifCos, svr):\nMSE: 0.0481, MAE: 0.1693, Pearson Correlation: 0.2445\nTest Performance (w2v, sifCos, svr):\nMSE: 0.0466, MAE: 0.1653, Pearson Correlation: 0.4158\nValidation Performance (w2v, sifCos, rfr):\nMSE: 0.0668, MAE: 0.1948, Pearson Correlation: 0.2126\nTest Performance (w2v, sifCos, rfr):\nMSE: 0.0635, MAE: 0.1939, Pearson Correlation: 0.2592\nBuilding embedding model: FastText\nBuilding vocabulary with 7292 sentences\nTraining the model for 30 epochs\n","output_type":"stream"},{"name":"stderr","text":"<ipython-input-9-6c009ec4bb5e>:38: DeprecationWarning: Call to deprecated `init_sims` (Gensim 4.0.0 implemented internal optimizations that make calls to init_sims() unnecessary. init_sims() is now obsoleted and will be completely removed in future versions. See https://github.com/RaRe-Technologies/gensim/wiki/Migrating-from-Gensim-3.x-to-4).\n  model.init_sims(replace=True)\n","output_type":"stream"},{"name":"stdout","text":"Model saved at new-param-domain_fasttext.model\nUsing feature extraction method: averageCosine\nValidation Performance (fast, averageCosine, linear):\nMSE: 0.0432, MAE: 0.1626, Pearson Correlation: 0.4022\nTest Performance (fast, averageCosine, linear):\nMSE: 0.0406, MAE: 0.1601, Pearson Correlation: 0.5187\nValidation Performance (fast, averageCosine, svr):\nMSE: 0.0424, MAE: 0.1585, Pearson Correlation: 0.4022\nTest Performance (fast, averageCosine, svr):\nMSE: 0.0416, MAE: 0.1587, Pearson Correlation: 0.5187\nValidation Performance (fast, averageCosine, rfr):\nMSE: 0.0519, MAE: 0.1684, Pearson Correlation: 0.4064\nTest Performance (fast, averageCosine, rfr):\nMSE: 0.0536, MAE: 0.1746, Pearson Correlation: 0.4162\nUsing feature extraction method: wordDis\nValidation Performance (fast, wordDis, linear):\nMSE: 0.0434, MAE: 0.1665, Pearson Correlation: 0.3901\nTest Performance (fast, wordDis, linear):\nMSE: 0.0458, MAE: 0.1672, Pearson Correlation: 0.4165\nValidation Performance (fast, wordDis, svr):\nMSE: 0.0431, MAE: 0.1598, Pearson Correlation: 0.3901\nTest Performance (fast, wordDis, svr):\nMSE: 0.0474, MAE: 0.1661, Pearson Correlation: 0.4165\nValidation Performance (fast, wordDis, rfr):\nMSE: 0.0544, MAE: 0.1698, Pearson Correlation: 0.3816\nTest Performance (fast, wordDis, rfr):\nMSE: 0.0514, MAE: 0.1683, Pearson Correlation: 0.4548\nUsing feature extraction method: sifCos\nValidation Performance (fast, sifCos, linear):\nMSE: 0.0486, MAE: 0.1759, Pearson Correlation: 0.2752\nTest Performance (fast, sifCos, linear):\nMSE: 0.0447, MAE: 0.1648, Pearson Correlation: 0.4400\nValidation Performance (fast, sifCos, svr):\nMSE: 0.0474, MAE: 0.1695, Pearson Correlation: 0.2752\nTest Performance (fast, sifCos, svr):\nMSE: 0.0459, MAE: 0.1627, Pearson Correlation: 0.4400\nValidation Performance (fast, sifCos, rfr):\nMSE: 0.0702, MAE: 0.1993, Pearson Correlation: 0.1120\nTest Performance (fast, sifCos, rfr):\nMSE: 0.0643, MAE: 0.1923, Pearson Correlation: 0.2313\nSaved validation result: val_result_1_w2v_averageCosine_linear.csv\nSaved test result: test_result_1_w2v_averageCosine_linear.csv\nSaved validation result: val_result_2_w2v_averageCosine_svr.csv\nSaved test result: test_result_2_w2v_averageCosine_svr.csv\nSaved validation result: val_result_3_w2v_averageCosine_rfr.csv\nSaved test result: test_result_3_w2v_averageCosine_rfr.csv\nSaved validation result: val_result_4_w2v_wordDis_linear.csv\nSaved test result: test_result_4_w2v_wordDis_linear.csv\nSaved validation result: val_result_5_w2v_wordDis_svr.csv\nSaved test result: test_result_5_w2v_wordDis_svr.csv\nSaved validation result: val_result_6_w2v_wordDis_rfr.csv\nSaved test result: test_result_6_w2v_wordDis_rfr.csv\nSaved validation result: val_result_7_w2v_sifCos_linear.csv\nSaved test result: test_result_7_w2v_sifCos_linear.csv\nSaved validation result: val_result_8_w2v_sifCos_svr.csv\nSaved test result: test_result_8_w2v_sifCos_svr.csv\nSaved validation result: val_result_9_w2v_sifCos_rfr.csv\nSaved test result: test_result_9_w2v_sifCos_rfr.csv\nSaved validation result: val_result_10_fast_averageCosine_linear.csv\nSaved test result: test_result_10_fast_averageCosine_linear.csv\nSaved validation result: val_result_11_fast_averageCosine_svr.csv\nSaved test result: test_result_11_fast_averageCosine_svr.csv\nSaved validation result: val_result_12_fast_averageCosine_rfr.csv\nSaved test result: test_result_12_fast_averageCosine_rfr.csv\nSaved validation result: val_result_13_fast_wordDis_linear.csv\nSaved test result: test_result_13_fast_wordDis_linear.csv\nSaved validation result: val_result_14_fast_wordDis_svr.csv\nSaved test result: test_result_14_fast_wordDis_svr.csv\nSaved validation result: val_result_15_fast_wordDis_rfr.csv\nSaved test result: test_result_15_fast_wordDis_rfr.csv\nSaved validation result: val_result_16_fast_sifCos_linear.csv\nSaved test result: test_result_16_fast_sifCos_linear.csv\nSaved validation result: val_result_17_fast_sifCos_svr.csv\nSaved test result: test_result_17_fast_sifCos_svr.csv\nSaved validation result: val_result_18_fast_sifCos_rfr.csv\nSaved test result: test_result_18_fast_sifCos_rfr.csv\n","output_type":"stream"}],"execution_count":25}]}