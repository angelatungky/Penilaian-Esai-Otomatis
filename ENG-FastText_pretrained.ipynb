{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pc4gpJsyvxVx",
        "outputId": "c35008e9-b0d2-4c8b-ff02-b42c28f16797"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 958.5/958.4MB downloaded\n"
          ]
        }
      ],
      "source": [
        "from gensim.downloader import load as gensim_load\n",
        "\n",
        "# Load pre-trained embeddings\n",
        "fasttext = gensim_load('fasttext-wiki-news-subwords-300')  # FastText"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.svm import SVR\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "import nltk"
      ],
      "metadata": {
        "id": "Q2AXVZr4xgzS"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure required NLTK data is downloaded\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-1YLNpCCx_cc",
        "outputId": "cae35afb-7fad-4cf3-d9d0-a04c143ccf0e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ====== Dataset Loading and Splitting ======\n",
        "def load_custom_dataset(filename):\n",
        "    data = []\n",
        "    with open(filename, \"r\") as file:\n",
        "        for line in file:\n",
        "            question, response, answer, label = line.strip().split('\\t')\n",
        "            label = float(label) / 5.0  # Normalize to [0, 1]\n",
        "            data.append((response, answer, label))\n",
        "    return data"
      ],
      "metadata": {
        "id": "KsQJNPPVyBIQ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split_dataset(data, valid_percentage, test_percentage):\n",
        "    length = len(data)\n",
        "    random.shuffle(data)\n",
        "    train = data[:int(length * (1 - valid_percentage - test_percentage))]\n",
        "    valid = data[int(length * (1 - valid_percentage - test_percentage)):int(length * (1 - test_percentage))]\n",
        "    test = data[int(length * (1 - test_percentage)):]\n",
        "    return train, valid, test"
      ],
      "metadata": {
        "id": "yGonDdPeyC6s"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ====== Text Preprocessing ======\n",
        "def preprocess_text(text):\n",
        "    # Normalize the text by replacing curly apostrophes with straight ones\n",
        "    text = text.replace(\"‘\", \"'\").replace(\"’\", \"'\").lower()  # Case folding and normalization\n",
        "\n",
        "    # Tokenize the text\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "\n",
        "    # Remove non-alphabetic tokens and stopwords\n",
        "    stop_words = set(nltk.corpus.stopwords.words(\"english\"))\n",
        "    tokens = [word for word in tokens if word.isalpha() and word not in stop_words]\n",
        "    return \" \".join(tokens)"
      ],
      "metadata": {
        "id": "XEo8aOabyGk1"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_data(data):\n",
        "    sentence1 = [preprocess_text(item[0]) for item in data]\n",
        "    sentence2 = [preprocess_text(item[1]) for item in data]\n",
        "    labels = [item[2] for item in data]\n",
        "    return sentence1, sentence2, labels"
      ],
      "metadata": {
        "id": "SFYPjPYQyHNt"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Function to save raw and preprocessed data\n",
        "def save_raw_and_preprocessed(raw_data, preprocessed_data1, preprocessed_data2, labels, filename):\n",
        "    # Convert to DataFrame\n",
        "    df = pd.DataFrame({\n",
        "        \"Raw Sentence 1\": [item[0] for item in raw_data],\n",
        "        \"Raw Sentence 2\": [item[1] for item in raw_data],\n",
        "        \"Preprocessed Sentence 1\": preprocessed_data1,\n",
        "        \"Preprocessed Sentence 2\": preprocessed_data2,\n",
        "        \"Label\": labels\n",
        "    })\n",
        "    df.to_csv(filename, index=False)\n",
        "    print(f\"Saved dataset to {filename}\")"
      ],
      "metadata": {
        "id": "UcNpAStXyLK1"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ====== Load Dataset ======\n",
        "# Load train, validation, and test datasets from CSV files\n",
        "train_file = \"/content/train_data_ENG-W2V.csv\"\n",
        "valid_file = \"/content/valid_data_ENG-W2V.csv\"\n",
        "test_file = \"/content/test_data_ENG-W2V.csv\"\n",
        "\n",
        "# Read datasets\n",
        "train_data = pd.read_csv(train_file).values\n",
        "valid_data = pd.read_csv(valid_file).values\n",
        "test_data = pd.read_csv(test_file).values\n",
        "\n",
        "# ====== Preprocessing ======\n",
        "x_train1, x_train2, y_train = preprocess_data(train_data)\n",
        "x_valid1, x_valid2, y_valid = preprocess_data(valid_data)\n",
        "x_test1, x_test2, y_test = preprocess_data(test_data)\n",
        "\n",
        "# Output shapes for verification\n",
        "print(f\"Train data: {len(x_train1)} pairs, {len(y_train)} labels\")\n",
        "print(f\"Validation data: {len(x_valid1)} pairs, {len(y_valid)} labels\")\n",
        "print(f\"Test data: {len(x_test1)} pairs, {len(y_test)} labels\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oQEQBTPayML6",
        "outputId": "7cabe943-fe77-4450-d7f3-7bcdfbd38ee4"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train data: 2916 pairs, 2916 labels\n",
            "Validation data: 365 pairs, 365 labels\n",
            "Test data: 365 pairs, 365 labels\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check a few random samples from the preprocessed training data\n",
        "print(\"Preprocessed x_train1 Samples:\")\n",
        "for i in random.sample(range(len(x_train1)), 5):  # Randomly select 5 indices\n",
        "    print(f\"Original Sentence 1: {train_data[i][0]}\")\n",
        "    print(f\"Preprocessed Sentence 1: {x_train1[i]}\")\n",
        "    print()\n",
        "\n",
        "print(\"Preprocessed x_train2 Samples:\")\n",
        "for i in random.sample(range(len(x_train2)), 5):  # Randomly select 5 indices\n",
        "    print(f\"Original Sentence 2: {train_data[i][1]}\")\n",
        "    print(f\"Preprocessed Sentence 2: {x_train2[i]}\")\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2LyOfWfHzJ42",
        "outputId": "ca919f76-4be4-43d4-9a47-6508abadb8b1"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preprocessed x_train1 Samples:\n",
            "Original Sentence 1: An alias - LRB - synonym - RRB - for the name of the object that its operand points to in memory It is the dereferencing operator\n",
            "Preprocessed Sentence 1: alias lrb synonym rrb name object operand points memory dereferencing operator\n",
            "\n",
            "Original Sentence 1: A function prototype tells the compiler the function name return type and the number and type of parameters without revealing the implementations contained in the function definition\n",
            "Preprocessed Sentence 1: function prototype tells compiler function name return type number type parameters without revealing implementations contained function definition\n",
            "\n",
            "Original Sentence 1: the type char has a null - LRB - n - RRB - element at the very end\n",
            "Preprocessed Sentence 1: type char null lrb n rrb element end\n",
            "\n",
            "Original Sentence 1: they take up twice as much memory for each node\n",
            "Preprocessed Sentence 1: take twice much memory node\n",
            "\n",
            "Original Sentence 1: It selects the minimum from an array and places it on the first position then it selects the minimum from the rest of the array and places it on the second position and so forth\n",
            "Preprocessed Sentence 1: selects minimum array places first position selects minimum rest array places second position forth\n",
            "\n",
            "Preprocessed x_train2 Samples:\n",
            "Original Sentence 2: the type string uses less storage and you have to change the string all at once with an array of characters you can make permutations of words using the characters stored in the array without needing to actually access and change the variables with an array of characters you can just change how they are accessed\n",
            "Preprocessed Sentence 2: type string uses less storage change string array characters make permutations words using characters stored array without needing actually access change variables array characters change accessed\n",
            "\n",
            "Original Sentence 2: it starts node on the left of the root and then proceeds to visits each node in a left to right order visits the root and then proceeds to repeat the previous step on the right side of the tree\n",
            "Preprocessed Sentence 2: starts node left root proceeds visits node left right order visits root proceeds repeat previous step right side tree\n",
            "\n",
            "Original Sentence 2: a string has variable size and function calls available while an character array usually has a static size\n",
            "Preprocessed Sentence 2: string variable size function calls available character array usually static size\n",
            "\n",
            "Original Sentence 2: Elaboration construction and transition\n",
            "Preprocessed Sentence 2: elaboration construction transition\n",
            "\n",
            "Original Sentence 2: gives function ability to access and modify the caller argument data directly\n",
            "Preprocessed Sentence 2: gives function ability access modify caller argument data directly\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def avgCos(sentences1, sentences2, embedding):\n",
        "    similarities = []\n",
        "    for sent1, sent2 in zip(sentences1, sentences2):\n",
        "        tokens1 = [token for token in sent1.split() if token in embedding.key_to_index]\n",
        "        tokens2 = [token for token in sent2.split() if token in embedding.key_to_index]\n",
        "\n",
        "        if not tokens1 or not tokens2:\n",
        "            similarities.append(0)\n",
        "            continue\n",
        "\n",
        "        vec1 = np.mean([embedding.get_vector(token) for token in tokens1], axis=0)\n",
        "        vec2 = np.mean([embedding.get_vector(token) for token in tokens2], axis=0)\n",
        "\n",
        "        similarities.append(np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2)))\n",
        "\n",
        "    return np.array(similarities).reshape(-1, 1)\n"
      ],
      "metadata": {
        "id": "cszVYjvbzWe3"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def word_distance(sentences1, sentences2, embedding):\n",
        "    sims = []\n",
        "    for sent1, sent2 in zip(sentences1, sentences2):\n",
        "        sent1_tokens = [token for token in sent1.split() if token in embedding.key_to_index]\n",
        "        sent2_tokens = [token for token in sent2.split() if token in embedding.key_to_index]\n",
        "\n",
        "        if not sent1_tokens or not sent2_tokens:\n",
        "            sims.append(0)\n",
        "        else:\n",
        "            sims.append(-embedding.wmdistance(sent1_tokens, sent2_tokens))  # Lower is better\n",
        "\n",
        "    return np.array(sims).reshape(-1, 1)\n"
      ],
      "metadata": {
        "id": "EtQVS5av3liT"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import TruncatedSVD\n",
        "\n",
        "def remove_first_principal_component(X):\n",
        "    svd = TruncatedSVD(n_components=1, random_state=42)\n",
        "    svd.fit(X)\n",
        "    pc = svd.components_\n",
        "    return X - X.dot(pc.T) * pc\n",
        "\n",
        "def sif_cos(sentences1, sentences2, embedding, freqs, a=0.001):\n",
        "    total_freq = sum(freqs.values())\n",
        "    embeddings = []\n",
        "\n",
        "    for sent1, sent2 in zip(sentences1, sentences2):\n",
        "        sent1_tokens = [token for token in sent1.split() if token in embedding.key_to_index]\n",
        "        sent2_tokens = [token for token in sent2.split() if token in embedding.key_to_index]\n",
        "\n",
        "        if not sent1_tokens or not sent2_tokens:\n",
        "            embeddings.extend([np.zeros(embedding.vector_size), np.zeros(embedding.vector_size)])\n",
        "            continue\n",
        "\n",
        "        weights1 = [a / (a + freqs.get(token, 1e-5) / total_freq) for token in sent1_tokens]\n",
        "        weights2 = [a / (a + freqs.get(token, 1e-5) / total_freq) for token in sent2_tokens]\n",
        "\n",
        "        embedding1 = np.average([embedding.get_vector(token) for token in sent1_tokens], axis=0, weights=weights1)\n",
        "        embedding2 = np.average([embedding.get_vector(token) for token in sent2_tokens], axis=0, weights=weights2)\n",
        "\n",
        "        embeddings.extend([embedding1, embedding2])\n",
        "\n",
        "    embeddings = np.array(embeddings)\n",
        "    embeddings = remove_first_principal_component(embeddings)\n",
        "\n",
        "    sims = [\n",
        "        (np.dot(embeddings[i], embeddings[i + 1]) /\n",
        "         (np.linalg.norm(embeddings[i]) * np.linalg.norm(embeddings[i + 1]))\n",
        "         if np.linalg.norm(embeddings[i]) > 0 and np.linalg.norm(embeddings[i + 1]) > 0 else 0)\n",
        "        for i in range(0, len(embeddings), 2)\n",
        "    ]\n",
        "\n",
        "    return np.array(sims).reshape(-1, 1)\n"
      ],
      "metadata": {
        "id": "zWBIrTXM3nN-"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ====== Regression Model ======\n",
        "class RegressionModel:\n",
        "    def __init__(self, model_type=\"linear\"):\n",
        "        if model_type == \"linear\":\n",
        "            self.model = self.LinearRegressionCustom()\n",
        "        elif model_type == \"svr\":\n",
        "            self.model = SVR(kernel=\"linear\")\n",
        "        elif model_type == \"rfr\":\n",
        "            self.model = self.RandomForestCustom()\n",
        "        else:\n",
        "            raise ValueError(\"Unsupported model type.\")\n",
        "\n",
        "    class LinearRegressionCustom:\n",
        "        def __init__(self):\n",
        "            self.weights = None\n",
        "\n",
        "        def fit(self, X, y):\n",
        "            X = np.c_[np.ones(X.shape[0]), X]  # Add bias term\n",
        "            self.weights = np.linalg.pinv(X.T @ X) @ X.T @ y\n",
        "        def fit(self, X, y):\n",
        "            # Convert y to NumPy and ensure matching rows\n",
        "            y = np.array(y)\n",
        "            if X.shape[0] != y.shape[0]:\n",
        "                raise ValueError(f\"Shape mismatch: X has {X.shape[0]} rows but y has {y.shape[0]} rows.\")\n",
        "\n",
        "            X = np.c_[np.ones(X.shape[0]), X]  # Add bias term\n",
        "            self.weights = np.linalg.pinv(X.T @ X) @ X.T @ y\n",
        "\n",
        "        def predict(self, X):\n",
        "            X = np.c_[np.ones(X.shape[0]), X]  # Add bias term\n",
        "            return X @ self.weights\n",
        "\n",
        "    class RandomForestCustom:\n",
        "        def __init__(self, n_estimators=100, max_depth=None):\n",
        "            self.n_estimators = n_estimators\n",
        "            self.max_depth = max_depth\n",
        "            self.trees = []\n",
        "\n",
        "        def fit(self, X, y):\n",
        "            from sklearn.tree import DecisionTreeRegressor\n",
        "\n",
        "            # Ensure y is a NumPy array\n",
        "            y = np.array(y)\n",
        "\n",
        "            n_samples = X.shape[0]\n",
        "\n",
        "            for _ in range(self.n_estimators):\n",
        "                # Ensure indices are integers for proper indexing\n",
        "                indices = np.random.choice(range(n_samples), size=n_samples, replace=True)\n",
        "                X_sample = X[indices]\n",
        "                y_sample = y[indices]\n",
        "                tree = DecisionTreeRegressor(max_depth=self.max_depth)\n",
        "                tree.fit(X_sample, y_sample)\n",
        "                self.trees.append(tree)\n",
        "\n",
        "        def predict(self, X):\n",
        "            # Aggregate predictions from all trees\n",
        "            predictions = np.array([tree.predict(X) for tree in self.trees])\n",
        "            return np.mean(predictions, axis=0)\n",
        "\n",
        "    @staticmethod\n",
        "    def mean_squared_error(y_true, y_pred):\n",
        "        squared_errors = [(true - pred) ** 2 for true, pred in zip(y_true, y_pred)]\n",
        "        return sum(squared_errors) / len(squared_errors)\n",
        "\n",
        "    @staticmethod\n",
        "    def mean_absolute_error(y_true, y_pred):\n",
        "        absolute_errors = [abs(true - pred) for true, pred in zip(y_true, y_pred)]\n",
        "        return sum(absolute_errors) / len(absolute_errors)\n",
        "\n",
        "    @staticmethod\n",
        "    def pearsonr(x, y):\n",
        "        mean_x = sum(x) / len(x)\n",
        "        mean_y = sum(y) / len(y)\n",
        "        numerator = sum((xi - mean_x) * (yi - mean_y) for xi, yi in zip(x, y))\n",
        "        denominator = ((sum((xi - mean_x) ** 2 for xi in x) * sum((yi - mean_y) ** 2 for yi in y)) ** 0.5)\n",
        "        return (numerator / denominator if denominator != 0 else 0.0, None)\n",
        "\n",
        "    def train(self, x_train, y_train):\n",
        "        self.model.fit(x_train, y_train)\n",
        "\n",
        "    def evaluate(self, x, y):\n",
        "        predictions = self.model.predict(x)\n",
        "        mse = self.mean_squared_error(y, predictions)\n",
        "        mae = self.mean_absolute_error(y, predictions)\n",
        "        pearson_corr, _ = self.pearsonr(y, predictions)\n",
        "        return mse, mae, pearson_corr\n",
        "\n",
        "    def predict(self, x):\n",
        "        return self.model.predict(x)"
      ],
      "metadata": {
        "id": "APRtR-L51iF7"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def feature_extraction(train_set1, train_set2, val_set1, val_set2, test_set, embed_model, frequency_map, method):\n",
        "    if method == \"averageCosine\":\n",
        "        train_similarities = average_cosine_similarity(train_set1, train_set2, embed_model)\n",
        "        val_similarities = average_cosine_similarity(val_set1, val_set2, embed_model)\n",
        "        test_similarities = average_cosine_similarity(test_set[\"sentence1\"], test_set[\"sentence2\"], embed_model)\n",
        "    elif method == \"wordDis\":\n",
        "        train_similarities = word_distance(train_set1, train_set2, embed_model)\n",
        "        val_similarities = word_distance(val_set1, val_set2, embed_model)\n",
        "        test_similarities = word_distance(test_set[\"sentence1\"], test_set[\"sentence2\"], embed_model)\n",
        "    elif method == \"sifCos\":\n",
        "        train_similarities = sif_cos(train_set1, train_set2, embed_model, frequency_map)\n",
        "        val_similarities = sif_cos(val_set1, val_set2, embed_model, frequency_map)\n",
        "        test_similarities = sif_cos(test_set[\"sentence1\"], test_set[\"sentence2\"], embed_model, frequency_map)\n",
        "    else:\n",
        "        raise ValueError(f\"Feature extraction method '{method}' is not supported.\")\n",
        "\n",
        "    return np.array(train_similarities), np.array(val_similarities), np.array(test_similarities)"
      ],
      "metadata": {
        "id": "VEvQjrob10j-"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ====== Testing Predictions ======\n",
        "def print_test_predictions(model, x_test_features, x_test1, x_test2, y_test):\n",
        "    predictions = model.predict(x_test_features)\n",
        "    true_scores = np.array(y_test) * 5.0\n",
        "    predicted_scores = predictions * 5.0\n",
        "    pearson_corr, _ = RegressionModel.pearsonr(true_scores, predicted_scores)\n",
        "    results = pd.DataFrame({\n",
        "        \"Sentence 1\": x_test1,\n",
        "        \"Sentence 2\": x_test2,\n",
        "        \"True Similarity Score\": true_scores,\n",
        "        \"Predicted Similarity Score\": predicted_scores\n",
        "    })\n",
        "    print(results.head(10))\n",
        "    print(f\"Pearson Correlation: {pearson_corr:.4f}\")\n",
        "    results.to_csv(\"train_predictions.csv\", index=False)"
      ],
      "metadata": {
        "id": "cATyqebu12--"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install cvxopt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CQGogbya15B6",
        "outputId": "771fc7a9-a39f-462a-811c-b07a60084a0c"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: cvxopt in /usr/local/lib/python3.11/dist-packages (1.3.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install POT"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BmuewC4I16d5",
        "outputId": "178b035a-285b-42f4-8a84-7085135b1ece"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting POT\n",
            "  Downloading POT-0.9.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (34 kB)\n",
            "Requirement already satisfied: numpy>=1.16 in /usr/local/lib/python3.11/dist-packages (from POT) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.6 in /usr/local/lib/python3.11/dist-packages (from POT) (1.13.1)\n",
            "Downloading POT-0.9.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (897 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.5/897.5 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: POT\n",
            "Successfully installed POT-0.9.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# ====== Frequency Computation for SIF ======\n",
        "from collections import Counter\n",
        "all_sentences = x_train1 + x_train2 + x_valid1 + x_valid2 + x_test1 + x_test2\n",
        "# Compute word frequencies for SIF\n",
        "all_tokens = [token for sentence in all_sentences for token in sentence.split()]\n",
        "freqs = Counter(all_tokens)"
      ],
      "metadata": {
        "id": "j2eLL5sT1752"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fasttext[\"king\"]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i2VIikv53TOX",
        "outputId": "cb3a02c7-e59e-46bf-bb68-458db0ddf7cb"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-1.2063e-01,  5.1695e-03, -1.2447e-02, -7.8528e-03, -2.3738e-02,\n",
              "       -8.2595e-02,  4.5790e-02, -1.5382e-01,  6.4550e-02,  1.2893e-01,\n",
              "        2.7643e-02,  1.5958e-02,  7.7559e-02,  6.0516e-02,  1.2737e-01,\n",
              "        8.4766e-02,  6.3890e-02, -1.7687e-01,  4.3017e-02, -1.8031e-02,\n",
              "       -3.3041e-02,  2.1930e-02, -1.1328e-02,  6.6453e-02,  1.5826e-01,\n",
              "       -2.3008e-02, -4.3616e-03, -2.2379e-02,  4.4891e-02,  3.0103e-03,\n",
              "       -1.5565e-02, -7.6785e-02, -9.2186e-02,  5.7907e-02, -2.7658e-02,\n",
              "        5.4500e-03,  1.8975e-02,  4.2939e-02,  3.4704e-03,  4.0449e-02,\n",
              "       -4.0245e-03, -1.1594e-01, -5.8337e-03,  3.2509e-02, -8.6535e-02,\n",
              "        7.2000e-02, -2.2299e-02,  1.3079e-02, -3.9515e-02,  6.8996e-02,\n",
              "        9.2300e-02, -7.5371e-02,  5.9412e-03, -3.4945e-02, -3.3417e-02,\n",
              "       -9.9982e-02,  1.6438e-02,  6.3739e-02, -6.2391e-02,  7.8285e-04,\n",
              "       -2.9210e-02, -9.6416e-02,  7.2910e-02,  4.5905e-02, -8.3387e-02,\n",
              "        7.1969e-02,  4.0932e-02, -5.6454e-03,  1.3709e-01, -1.1793e-01,\n",
              "       -7.1011e-02, -7.1963e-02,  6.5600e-02, -4.6315e-02, -1.7200e-02,\n",
              "        3.4434e-02,  4.4218e-02, -9.6354e-03, -6.8105e-02,  3.0810e-02,\n",
              "        1.5424e-02,  5.6398e-02,  4.4225e-02,  8.0547e-02, -5.2413e-02,\n",
              "       -3.6509e-02,  2.6141e-02,  2.5574e-02, -3.4346e-02, -4.5879e-02,\n",
              "       -1.7031e-02,  5.1450e-02, -1.2766e-01, -8.6838e-02,  1.1084e-02,\n",
              "        1.3282e-01,  2.0850e-02,  7.0881e-02, -5.9277e-03,  2.2612e-02,\n",
              "        4.8919e-02, -1.2490e-02,  1.5460e-01, -6.1251e-03, -8.9369e-02,\n",
              "       -2.3707e-01,  2.0696e-02, -3.7604e-02, -8.3793e-02, -2.5512e-03,\n",
              "       -4.0426e-02,  1.0575e-01,  9.7514e-02,  4.4101e-02,  4.1732e-02,\n",
              "        7.4080e-02,  6.3560e-02,  3.1801e-02, -1.4961e-02, -4.3675e-03,\n",
              "       -1.4893e-02,  8.6208e-02, -2.0204e-02, -2.0797e-03,  7.7648e-02,\n",
              "       -1.9620e-03,  3.2115e-02, -1.5615e-01, -3.6702e-02,  1.2009e-01,\n",
              "       -8.0633e-02,  4.2894e-02, -3.5265e-02,  2.2693e-02, -3.3743e-02,\n",
              "        1.7573e-02, -7.5089e-02,  9.8873e-02,  2.7042e-02, -1.7185e-02,\n",
              "        1.7489e-02, -1.1096e-01,  7.5456e-02, -4.2234e-02, -3.7115e-02,\n",
              "       -1.2356e-02,  1.1243e-02, -4.6907e-02, -5.5681e-02, -6.5216e-02,\n",
              "        5.4923e-02,  3.7514e-02,  5.0259e-02, -7.4453e-02, -2.0440e-02,\n",
              "       -8.3293e-02, -2.3010e-02, -4.2105e-02, -2.8792e-02, -1.9139e-02,\n",
              "        3.6758e-02,  7.7620e-02, -6.3909e-02, -2.9304e-02,  3.1128e-02,\n",
              "       -1.2056e-02, -3.0854e-02, -2.3162e-02, -4.4762e-02,  1.2797e-01,\n",
              "       -7.7709e-03, -7.7466e-02, -2.7976e-02,  5.1038e-02, -5.5217e-02,\n",
              "        7.5312e-02,  3.4093e-02, -3.4833e-03,  9.7360e-03,  5.8273e-02,\n",
              "        9.3454e-02, -4.3781e-02, -4.5870e-02, -7.3544e-02, -4.1269e-02,\n",
              "       -9.1712e-02, -1.5840e-01,  1.1790e-01,  3.4210e-02, -2.4719e-02,\n",
              "        6.1251e-02,  8.2068e-02, -1.1710e-01,  2.9949e-02, -7.1442e-02,\n",
              "        2.2185e-02, -2.4418e-02, -2.5316e-02, -5.3970e-02,  1.1615e-01,\n",
              "       -1.9979e-01,  6.8714e-02, -6.1776e-03, -3.9478e-02, -1.8856e-02,\n",
              "        7.8819e-02,  3.0709e-02, -4.7448e-02, -5.0356e-02, -4.0706e-02,\n",
              "        1.4722e-01, -4.6420e-02,  1.1976e-05,  9.2290e-02, -6.1358e-02,\n",
              "        6.0161e-05,  1.4491e-02, -2.4847e-02,  5.6051e-02,  1.9206e-02,\n",
              "        3.2446e-02,  5.0245e-03,  1.9242e-02,  1.3482e-01,  7.3311e-03,\n",
              "       -1.0219e-01,  7.6724e-02,  9.7512e-02, -4.9655e-02, -7.2788e-03,\n",
              "       -1.1748e-01, -3.5783e-02, -6.9954e-02, -8.8086e-03, -1.5677e-02,\n",
              "        6.4489e-02, -7.2463e-02, -5.0428e-03,  7.5461e-02, -6.0999e-02,\n",
              "        9.2653e-02, -5.3002e-02, -9.8853e-02,  4.4468e-02,  1.5699e-03,\n",
              "        1.0594e-02,  5.4306e-02,  2.1943e-02, -1.4941e-02, -2.9272e-02,\n",
              "        1.0173e-01, -2.7459e-02, -1.7016e-02,  3.7454e-02,  8.5015e-02,\n",
              "        8.6834e-02, -7.6342e-02,  9.5069e-02,  4.6912e-02, -2.2718e-02,\n",
              "       -7.9839e-02,  6.6125e-02,  6.2540e-02,  2.5836e-02,  2.4580e-02,\n",
              "        5.1879e-02, -1.8032e-04,  4.8657e-02, -1.1875e-01, -2.4103e-02,\n",
              "        1.5130e-03,  8.0515e-02, -1.0280e-01, -1.3489e-02,  7.1108e-02,\n",
              "       -6.0643e-02, -2.3006e-02, -9.8232e-03, -8.7159e-02,  8.5388e-02,\n",
              "        5.3778e-02, -8.4714e-02,  5.4218e-02, -4.1406e-02,  1.0716e-02,\n",
              "        6.9728e-02, -8.9833e-03, -8.0539e-02, -3.0566e-02,  1.0912e-01,\n",
              "       -3.9061e-02, -6.3893e-02, -3.3986e-02, -2.0095e-02, -6.0904e-02,\n",
              "        1.5957e-02, -1.0371e-02,  6.7261e-02, -3.0458e-02, -3.1992e-02],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "# Compute word frequencies\n",
        "all_sentences = x_train1 + x_train2 + x_valid1 + x_valid2 + x_test1 + x_test2\n",
        "all_tokens = [token for sentence in all_sentences for token in sentence.split()]\n",
        "freqs = Counter(all_tokens)\n"
      ],
      "metadata": {
        "id": "jix5qgSa2MWo"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract Features\n",
        "x_train_features_avg = avgCos(x_train1, x_train2, fasttext)\n",
        "x_valid_features_avg = avgCos(x_valid1, x_valid2, fasttext)\n",
        "x_test_features_avg = avgCos(x_test1, x_test2, fasttext)\n",
        "\n",
        "x_train_features_wmd = word_distance(x_train1, x_train2, fasttext)\n",
        "x_valid_features_wmd = word_distance(x_valid1, x_valid2, fasttext)\n",
        "x_test_features_wmd = word_distance(x_test1, x_test2, fasttext)\n",
        "\n",
        "x_train_features_sif = sif_cos(x_train1, x_train2, fasttext, freqs)\n",
        "x_valid_features_sif = sif_cos(x_valid1, x_valid2, fasttext, freqs)\n",
        "x_test_features_sif = sif_cos(x_test1, x_test2, fasttext, freqs)\n"
      ],
      "metadata": {
        "id": "_KGsEnMY3z1R"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from scipy.stats import pearsonr\n",
        "from gensim.downloader import load as gensim_load\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "from collections import Counter\n",
        "from sklearn.decomposition import TruncatedSVD"
      ],
      "metadata": {
        "id": "EXH2ugtU4eyy"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature Extraction Methods\n",
        "feature_methods = {\n",
        "    \"avgCos\": {\"train\": x_train_features_avg, \"valid\": x_valid_features_avg, \"test\": x_test_features_avg},\n",
        "    \"wordDis\": {\"train\": x_train_features_wmd, \"valid\": x_valid_features_wmd, \"test\": x_test_features_wmd},\n",
        "    \"sifCos\": {\"train\": x_train_features_sif, \"valid\": x_valid_features_sif, \"test\": x_test_features_sif}\n",
        "}\n",
        "\n",
        "# Train and Evaluate Models\n",
        "models = {\"linear\": RegressionModel(\"linear\"), \"svr\": RegressionModel(\"svr\"), \"rfr\": RegressionModel(\"rfr\")}\n",
        "results = []\n",
        "\n",
        "for feature_name, feature_data in feature_methods.items():\n",
        "    for model_name, model in models.items():\n",
        "        model.train(feature_data[\"train\"], y_train)\n",
        "        val_mse, val_mae, val_pearson = model.evaluate(feature_data[\"valid\"], y_valid)\n",
        "        test_mse, test_mae, test_pearson = model.evaluate(feature_data[\"test\"], y_test)\n",
        "\n",
        "        print(f\"Validation MSE ({feature_name}-{model_name}): {val_mse:.4f}, MAE: {val_mae:.4f}, Pearson: {val_pearson:.4f}\")\n",
        "        print(f\"Test MSE ({feature_name}-{model_name}): {test_mse:.4f}, MAE: {test_mae:.4f}, Pearson: {test_pearson:.4f}\")\n",
        "\n",
        "        predictions = model.predict(feature_data[\"test\"])\n",
        "        results.append({\n",
        "            \"feature\": feature_name,\n",
        "            \"model\": model_name,\n",
        "            \"test_predictions\": predictions,\n",
        "            \"x_test1\": x_test1,\n",
        "            \"x_test2\": x_test2,\n",
        "            \"y_test\": y_test\n",
        "        })\n",
        "\n",
        "# Save all results as CSV files\n",
        "for i, result in enumerate(results, start=1):\n",
        "    raw_test1 = [item[0] for item in test_data]\n",
        "    raw_test2 = [item[1] for item in test_data]\n",
        "    test_df = pd.DataFrame({\n",
        "        \"Original Sentence 1\": raw_test1,\n",
        "        \"Original Sentence 2\": raw_test2,\n",
        "        \"Preprocessed Sentence 1\": result[\"x_test1\"],\n",
        "        \"Preprocessed Sentence 2\": result[\"x_test2\"],\n",
        "        \"True Similarity Score\": [y * 5 for y in result[\"y_test\"]],\n",
        "        \"Predicted Similarity Score\": [y * 5 for y in result[\"test_predictions\"]]\n",
        "    })\n",
        "    test_filename = f\"test_result_{i}_{result['feature']}_{result['model']}.csv\"\n",
        "    test_df.to_csv(test_filename, index=False)\n",
        "    print(f\"Saved test result: {test_filename}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QmS9W5Lg4PKD",
        "outputId": "f7d8015f-594c-4a21-e642-cd5c7e618a36"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation MSE (avgCos-linear): 0.0442, MAE: 0.1717, Pearson: 0.3674\n",
            "Test MSE (avgCos-linear): 0.0448, MAE: 0.1716, Pearson: 0.4414\n",
            "Validation MSE (avgCos-svr): 0.0440, MAE: 0.1636, Pearson: 0.3674\n",
            "Test MSE (avgCos-svr): 0.0463, MAE: 0.1679, Pearson: 0.4414\n",
            "Validation MSE (avgCos-rfr): 0.0523, MAE: 0.1680, Pearson: 0.3860\n",
            "Test MSE (avgCos-rfr): 0.0560, MAE: 0.1767, Pearson: 0.3928\n",
            "Validation MSE (wordDis-linear): 0.0435, MAE: 0.1677, Pearson: 0.3871\n",
            "Test MSE (wordDis-linear): 0.0481, MAE: 0.1715, Pearson: 0.3676\n",
            "Validation MSE (wordDis-svr): 0.0431, MAE: 0.1612, Pearson: 0.3871\n",
            "Test MSE (wordDis-svr): 0.0498, MAE: 0.1704, Pearson: 0.3676\n",
            "Validation MSE (wordDis-rfr): 0.0805, MAE: 0.2536, Pearson: 0.4177\n",
            "Test MSE (wordDis-rfr): 0.0744, MAE: 0.2387, Pearson: 0.3718\n",
            "Validation MSE (sifCos-linear): 0.0479, MAE: 0.1745, Pearson: 0.2694\n",
            "Test MSE (sifCos-linear): 0.0487, MAE: 0.1693, Pearson: 0.3476\n",
            "Validation MSE (sifCos-svr): 0.0473, MAE: 0.1665, Pearson: 0.2694\n",
            "Test MSE (sifCos-svr): 0.0501, MAE: 0.1670, Pearson: 0.3476\n",
            "Validation MSE (sifCos-rfr): 0.0548, MAE: 0.1900, Pearson: 0.2237\n",
            "Test MSE (sifCos-rfr): 0.0573, MAE: 0.1939, Pearson: 0.2063\n",
            "Saved test result: test_result_1_avgCos_linear.csv\n",
            "Saved test result: test_result_2_avgCos_svr.csv\n",
            "Saved test result: test_result_3_avgCos_rfr.csv\n",
            "Saved test result: test_result_4_wordDis_linear.csv\n",
            "Saved test result: test_result_5_wordDis_svr.csv\n",
            "Saved test result: test_result_6_wordDis_rfr.csv\n",
            "Saved test result: test_result_7_sifCos_linear.csv\n",
            "Saved test result: test_result_8_sifCos_svr.csv\n",
            "Saved test result: test_result_9_sifCos_rfr.csv\n"
          ]
        }
      ]
    }
  ]
}