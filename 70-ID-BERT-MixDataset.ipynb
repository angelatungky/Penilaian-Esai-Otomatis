{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10448883,"sourceType":"datasetVersion","datasetId":6467689},{"sourceId":10475641,"sourceType":"datasetVersion","datasetId":6486533},{"sourceId":10477219,"sourceType":"datasetVersion","datasetId":6486932}],"dockerImageVersionId":30840,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\n# Load your custom dataset from a CSV file\ndef load_indo_dataset(filename):\n    # Read the CSV file\n    df = pd.read_csv(filename)\n    print(df.head())\n    # Extract columns 'answer', 'response', and 'label'\n    # Normalize the label to [0, 1]\n    data = [\n        (row['answer'], row['response'], row['label'] / 5.0)\n        for _, row in df.iterrows()\n    ]\n\n    return data","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-02-08T03:14:47.673389Z","iopub.execute_input":"2025-02-08T03:14:47.673598Z","iopub.status.idle":"2025-02-08T03:14:49.545702Z","shell.execute_reply.started":"2025-02-08T03:14:47.673578Z","shell.execute_reply":"2025-02-08T03:14:49.544967Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\n\n# Load the CSV file\nfile_path = '/kaggle/input/indo-datanew/indodataset.csv'\ndf = pd.read_csv(file_path)\n\n# Check the first few rows\nprint(df.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T12:44:15.564479Z","iopub.execute_input":"2025-01-19T12:44:15.564993Z","iopub.status.idle":"2025-01-19T12:44:15.605596Z","shell.execute_reply.started":"2025-01-19T12:44:15.564958Z","shell.execute_reply":"2025-01-19T12:44:15.604733Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.info()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T12:44:15.606611Z","iopub.execute_input":"2025-01-19T12:44:15.606974Z","iopub.status.idle":"2025-01-19T12:44:15.627777Z","shell.execute_reply.started":"2025-01-19T12:44:15.606887Z","shell.execute_reply":"2025-01-19T12:44:15.626651Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\n\n# Load your dataset\nfile_path = '/kaggle/input/indo-datanew/indodataset.csv'\ndf = pd.read_csv(file_path)\n\n# Find rows with missing values\nmissing_values = df[df.isnull().any(axis=1)]\n\n# Display the rows and their indices\nprint(\"Rows with missing values:\")\nprint(missing_values)\n\n# Display the row numbers\nprint(\"\\nIndices of rows with missing values:\")\nprint(missing_values.index.tolist())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T12:44:15.630023Z","iopub.execute_input":"2025-01-19T12:44:15.630255Z","iopub.status.idle":"2025-01-19T12:44:15.660493Z","shell.execute_reply.started":"2025-01-19T12:44:15.63023Z","shell.execute_reply":"2025-01-19T12:44:15.659526Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Check for missing or infinite values\nprint(df.isnull().sum())  # Ensure no missing values\nprint((df == float('inf')).sum())  # Check for infinite values\nprint((df == float('-inf')).sum())  # Check for negative infinite values","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T12:44:15.662161Z","iopub.execute_input":"2025-01-19T12:44:15.662515Z","iopub.status.idle":"2025-01-19T12:44:15.671724Z","shell.execute_reply.started":"2025-01-19T12:44:15.662486Z","shell.execute_reply":"2025-01-19T12:44:15.670762Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install transformers sentence-transformers datasets","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-08T03:17:00.276740Z","iopub.execute_input":"2025-02-08T03:17:00.277101Z","iopub.status.idle":"2025-02-08T03:17:05.763670Z","shell.execute_reply.started":"2025-02-08T03:17:00.277075Z","shell.execute_reply":"2025-02-08T03:17:05.762478Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.47.0)\nRequirement already satisfied: sentence-transformers in /usr/local/lib/python3.10/dist-packages (3.3.1)\nRequirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.2.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.27.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.0)\nRequirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.67.1)\nRequirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.5.1+cu121)\nRequirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.2.2)\nRequirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.13.1)\nRequirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (11.0.0)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.10)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.4)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.2)\nRequirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.18.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.12.14)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.5.0)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->transformers) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport datetime\nimport time\nimport random\nfrom transformers import BertTokenizer\nfrom sentence_transformers import SentenceTransformer, models\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\nfrom scipy.stats import pearsonr","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-08T03:17:05.765135Z","iopub.execute_input":"2025-02-08T03:17:05.765487Z","iopub.status.idle":"2025-02-08T03:17:39.077494Z","shell.execute_reply.started":"2025-02-08T03:17:05.765453Z","shell.execute_reply":"2025-02-08T03:17:39.076829Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# Split dataset into train, validation, and test\ndef split_dataset(data, valid_percentage, test_percentage):\n    length = len(data)\n    np.random.shuffle(data)\n    train = data[:int(length * (1 - valid_percentage - test_percentage))]\n    valid = data[int(length * (1 - valid_percentage - test_percentage)):int(length * (1 - test_percentage))]\n    test = data[int(length * (1 - test_percentage)):]\n    return train, valid, test","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-08T03:17:39.078625Z","iopub.execute_input":"2025-02-08T03:17:39.079311Z","iopub.status.idle":"2025-02-08T03:17:39.083631Z","shell.execute_reply.started":"2025-02-08T03:17:39.079274Z","shell.execute_reply":"2025-02-08T03:17:39.082861Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# Split the dataset into train, validation, and test sets\n# train_data, val_data, test_data = split_dataset(data, valid_percentage=0.1, test_percentage=0.1)\n# ====== Load Dataset ======\n# Load train, validation, and test datasets from CSV files\ntrain_file = \"/kaggle/input/data-with-unseen/70-train_data (1).csv\"\nvalid_file = \"/kaggle/input/data-with-unseen/15-val_data (1).csv\"\ntest_file = \"/kaggle/input/data-with-unseen/15-test_data (1).csv\"\n\n# Read datasets\ntrain_data = pd.read_csv(train_file).values\nval_data = pd.read_csv(valid_file).values\ntest_data = pd.read_csv(test_file).values","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-08T03:20:37.346536Z","iopub.execute_input":"2025-02-08T03:20:37.346870Z","iopub.status.idle":"2025-02-08T03:20:37.368242Z","shell.execute_reply.started":"2025-02-08T03:20:37.346843Z","shell.execute_reply":"2025-02-08T03:20:37.367320Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"# Highlighted: Use the IndoBERT tokenizer\ntokenizer = BertTokenizer.from_pretrained('indobenchmark/indobert-base-p1')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-08T03:20:12.890564Z","iopub.execute_input":"2025-02-08T03:20:12.890874Z","iopub.status.idle":"2025-02-08T03:20:14.402909Z","shell.execute_reply.started":"2025-02-08T03:20:12.890848Z","shell.execute_reply":"2025-02-08T03:20:14.402070Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/2.00 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eeacade115be46758746532e8ac42274"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/229k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"de4d28e6c87048bd94e40bdfdfe23a78"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"634fb1fc04934ddc80677202ec62c313"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.53k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d87d5b9fee5d471b910a4bac5d5973cc"}},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"def format_time(elapsed):\n    # Round to the nearest second.\n    elapsed_rounded = int(round((elapsed)))\n    \n    # Format as hh:mm:ss\n    return str(datetime.timedelta(seconds=elapsed_rounded))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-08T03:20:16.692863Z","iopub.execute_input":"2025-02-08T03:20:16.693211Z","iopub.status.idle":"2025-02-08T03:20:16.696971Z","shell.execute_reply.started":"2025-02-08T03:20:16.693174Z","shell.execute_reply":"2025-02-08T03:20:16.696235Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"# Correct the CustomDataset __getitem__ method\nclass CustomDataset(torch.utils.data.Dataset):\n    def __init__(self, data):\n        self.first_sentences = [pair[0] for pair in data]\n        self.second_sentences = [pair[1] for pair in data]\n        self.labels = [pair[2] for pair in data]\n\n    def __len__(self):\n        return len(self.first_sentences)\n\n    def __getitem__(self, idx):\n        texts = tokenizer(\n            self.first_sentences[idx],\n            self.second_sentences[idx],\n            padding=\"max_length\",\n            max_length=128,\n            truncation=True,\n            return_tensors=\"pt\"\n        )\n        label = torch.tensor(self.labels[idx], dtype=torch.float32)\n        return {\n            'input_ids': texts['input_ids'].squeeze(0),\n            'attention_mask': texts['attention_mask'].squeeze(0),\n        }, label","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-08T03:20:18.422522Z","iopub.execute_input":"2025-02-08T03:20:18.422856Z","iopub.status.idle":"2025-02-08T03:20:18.429861Z","shell.execute_reply.started":"2025-02-08T03:20:18.422834Z","shell.execute_reply":"2025-02-08T03:20:18.428778Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"# Create DataLoader\nbatch_size = 8\ntrain_ds = CustomDataset(train_data)\nval_ds = CustomDataset(val_data)\ntest_ds = CustomDataset(test_data)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-08T03:20:40.760996Z","iopub.execute_input":"2025-02-08T03:20:40.761310Z","iopub.status.idle":"2025-02-08T03:20:40.766221Z","shell.execute_reply.started":"2025-02-08T03:20:40.761287Z","shell.execute_reply":"2025-02-08T03:20:40.765393Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"train_dataloader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\nvalidation_dataloader = DataLoader(val_ds, batch_size=batch_size)\ntest_dataloader = DataLoader(test_ds, batch_size=batch_size)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-08T03:20:44.798105Z","iopub.execute_input":"2025-02-08T03:20:44.798409Z","iopub.status.idle":"2025-02-08T03:20:44.802562Z","shell.execute_reply.started":"2025-02-08T03:20:44.798385Z","shell.execute_reply":"2025-02-08T03:20:44.801740Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"# Enhanced Model with IndoBERT\nclass EnhancedBertModel(nn.Module):\n    def __init__(self):\n        super(EnhancedBertModel, self).__init__()\n        # Highlighted: Use IndoBERT as the transformer\n        self.bert = models.Transformer('indobenchmark/indobert-base-p1', max_seq_length=128)\n        self.pooling_layer = models.Pooling(self.bert.get_word_embedding_dimension())\n\n        # Freeze BERT layers\n        for param in self.bert.parameters():\n            param.requires_grad = False\n\n        self.bi_lstm = nn.LSTM(\n            input_size=self.bert.get_word_embedding_dimension(),\n            hidden_size=64,\n            num_layers=1,\n            bidirectional=True,\n            batch_first=True\n        )\n\n        self.fc_dropout = nn.Dropout(0.3)\n        self.fc = nn.Linear(256, 1)\n\n    def forward(self, input_data):\n        bert_output = self.bert(input_data)\n        sequence_output = bert_output['token_embeddings']\n\n        lstm_output, _ = self.bi_lstm(sequence_output)\n\n        avg_pool = torch.mean(lstm_output, dim=1)\n        max_pool, _ = torch.max(lstm_output, dim=1)\n\n        pooled_output = torch.cat((avg_pool, max_pool), dim=1)\n\n        output = self.fc_dropout(pooled_output)\n        output = self.fc(output)\n\n        return output.squeeze(-1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-08T03:20:46.988306Z","iopub.execute_input":"2025-02-08T03:20:46.988625Z","iopub.status.idle":"2025-02-08T03:20:46.994778Z","shell.execute_reply.started":"2025-02-08T03:20:46.988599Z","shell.execute_reply":"2025-02-08T03:20:46.993963Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"# Check for GPU availability\nif torch.cuda.is_available():\n    device = torch.device(\"cuda\")\n    print(f'There are {torch.cuda.device_count()} GPU(s) available.')\n    print('We will use the GPU:', torch.cuda.get_device_name(0))\nelse:\n    print('No GPU available, using the CPU instead.')\n    device = torch.device(\"cpu\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-08T03:20:50.040426Z","iopub.execute_input":"2025-02-08T03:20:50.040747Z","iopub.status.idle":"2025-02-08T03:20:50.165134Z","shell.execute_reply.started":"2025-02-08T03:20:50.040722Z","shell.execute_reply":"2025-02-08T03:20:50.164283Z"}},"outputs":[{"name":"stdout","text":"There are 2 GPU(s) available.\nWe will use the GPU: Tesla T4\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"# Instantiate and move the model to device\nmodel = EnhancedBertModel()\nmodel.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-08T03:20:53.431521Z","iopub.execute_input":"2025-02-08T03:20:53.431825Z","iopub.status.idle":"2025-02-08T03:20:58.640150Z","shell.execute_reply.started":"2025-02-08T03:20:53.431798Z","shell.execute_reply":"2025-02-08T03:20:58.639292Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/498M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bbbb142ad79d41e698644361a53f268f"}},"metadata":{}},{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"EnhancedBertModel(\n  (bert): Transformer({'max_seq_length': 128, 'do_lower_case': False}) with Transformer model: BertModel \n  (pooling_layer): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n  (bi_lstm): LSTM(768, 64, batch_first=True, bidirectional=True)\n  (fc_dropout): Dropout(p=0.3, inplace=False)\n  (fc): Linear(in_features=256, out_features=1, bias=True)\n)"},"metadata":{}}],"execution_count":19},{"cell_type":"code","source":"# Loss function, optimizer, and scheduler\ncriterion = nn.MSELoss()\nepochs = 8\noptimizer = optim.Adam(model.parameters(), lr=1e-5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-08T03:20:58.641354Z","iopub.execute_input":"2025-02-08T03:20:58.641661Z","iopub.status.idle":"2025-02-08T03:20:58.646697Z","shell.execute_reply.started":"2025-02-08T03:20:58.641637Z","shell.execute_reply":"2025-02-08T03:20:58.645931Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"# Training Loop\ndef train_model():\n    training_stats = []\n    total_t0 = time.time()\n\n    for epoch_i in range(epochs):\n        print(f\"\\n======== Epoch {epoch_i + 1} / {epochs} ========\")\n        print(\"Training...\")\n\n        t0 = time.time()\n        total_train_loss = 0\n        model.train()\n\n        for batch in tqdm(train_dataloader):\n            train_data, train_labels = batch\n            train_data['input_ids'] = train_data['input_ids'].to(device)\n            train_data['attention_mask'] = train_data['attention_mask'].to(device)\n            train_labels = train_labels.to(device)\n\n            optimizer.zero_grad()\n            outputs = model({\n                'input_ids': train_data['input_ids'],\n                'attention_mask': train_data['attention_mask']\n            })\n            loss = criterion(outputs, train_labels)\n            total_train_loss += loss.item()\n\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n            optimizer.step()\n\n        avg_train_loss = total_train_loss / len(train_dataloader)\n        training_time = format_time(time.time() - t0)\n\n        print(f\"  Average training loss: {avg_train_loss:.5f}\")\n        print(f\"  Training epoch took: {training_time}\")\n\n        # Validation\n        print(\"Running Validation...\")\n        t0 = time.time()\n\n        model.eval()\n        total_val_loss = 0\n\n        for batch in tqdm(validation_dataloader):\n            val_data, val_labels = batch\n            val_data['input_ids'] = val_data['input_ids'].to(device)\n            val_data['attention_mask'] = val_data['attention_mask'].to(device)\n            val_labels = val_labels.to(device)\n\n            with torch.no_grad():\n                outputs = model({\n                    'input_ids': val_data['input_ids'],\n                    'attention_mask': val_data['attention_mask']\n                })\n                loss = criterion(outputs, val_labels)\n                total_val_loss += loss.item()\n\n        avg_val_loss = total_val_loss / len(validation_dataloader)\n        validation_time = format_time(time.time() - t0)\n\n        print(f\"  Validation Loss: {avg_val_loss:.5f}\")\n        print(f\"  Validation took: {validation_time}\")\n\n        training_stats.append({\n            'epoch': epoch_i + 1,\n            'Training Loss': avg_train_loss,\n            'Validation Loss': avg_val_loss,\n            'Training Time': training_time,\n            'Validation Time': validation_time\n        })\n\n    print(\"Training complete!\")\n    print(f\"Total training took {format_time(time.time() - total_t0)}\")\n    return model, training_stats\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-08T03:21:04.997232Z","iopub.execute_input":"2025-02-08T03:21:04.997530Z","iopub.status.idle":"2025-02-08T03:21:05.005850Z","shell.execute_reply.started":"2025-02-08T03:21:04.997506Z","shell.execute_reply":"2025-02-08T03:21:05.005111Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"# Train the model\nmodel, training_stats = train_model()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-08T03:21:11.538216Z","iopub.execute_input":"2025-02-08T03:21:11.538625Z","iopub.status.idle":"2025-02-08T03:22:51.695777Z","shell.execute_reply.started":"2025-02-08T03:21:11.538588Z","shell.execute_reply":"2025-02-08T03:22:51.694933Z"}},"outputs":[{"name":"stdout","text":"\n======== Epoch 1 / 8 ========\nTraining...\n","output_type":"stream"},{"name":"stderr","text":"  3%|▎         | 5/164 [00:02<00:49,  3.22it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n  5%|▌         | 9/164 [00:02<00:23,  6.69it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n 12%|█▏        | 19/164 [00:03<00:10, 14.25it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n 23%|██▎       | 37/164 [00:04<00:07, 17.21it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n 40%|███▉      | 65/164 [00:05<00:05, 17.29it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n 62%|██████▏   | 101/164 [00:07<00:03, 17.27it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n 65%|██████▌   | 107/164 [00:08<00:03, 17.16it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n 69%|██████▉   | 113/164 [00:08<00:02, 17.08it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n 80%|███████▉  | 131/164 [00:09<00:01, 17.14it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n 82%|████████▏ | 135/164 [00:09<00:01, 17.18it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n 86%|████████▌ | 141/164 [00:10<00:01, 16.91it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n100%|██████████| 164/164 [00:11<00:00, 14.29it/s]\n","output_type":"stream"},{"name":"stdout","text":"  Average training loss: 0.19173\n  Training epoch took: 0:00:11\nRunning Validation...\n","output_type":"stream"},{"name":"stderr","text":" 28%|██▊       | 10/36 [00:00<00:01, 18.43it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n 94%|█████████▍| 34/36 [00:01<00:00, 18.01it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n100%|██████████| 36/36 [00:01<00:00, 18.46it/s]\n","output_type":"stream"},{"name":"stdout","text":"  Validation Loss: 0.13719\n  Validation took: 0:00:02\n\n======== Epoch 2 / 8 ========\nTraining...\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/164 [00:00<?, ?it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n 28%|██▊       | 46/164 [00:02<00:06, 17.02it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n 34%|███▍      | 56/164 [00:03<00:06, 16.97it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n 37%|███▋      | 60/164 [00:03<00:06, 16.88it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n 41%|████▏     | 68/164 [00:04<00:05, 16.72it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n 54%|█████▎    | 88/164 [00:05<00:04, 16.86it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n 60%|█████▉    | 98/164 [00:05<00:03, 16.60it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n 72%|███████▏  | 118/164 [00:07<00:02, 16.84it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n 74%|███████▍  | 122/164 [00:07<00:02, 16.65it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n 85%|████████▌ | 140/164 [00:08<00:01, 16.83it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n 94%|█████████▍| 154/164 [00:09<00:00, 16.62it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n100%|██████████| 164/164 [00:09<00:00, 16.85it/s]\n","output_type":"stream"},{"name":"stdout","text":"  Average training loss: 0.17498\n  Training epoch took: 0:00:10\nRunning Validation...\n","output_type":"stream"},{"name":"stderr","text":" 28%|██▊       | 10/36 [00:00<00:01, 17.91it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n 94%|█████████▍| 34/36 [00:01<00:00, 17.57it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n100%|██████████| 36/36 [00:01<00:00, 18.06it/s]\n","output_type":"stream"},{"name":"stdout","text":"  Validation Loss: 0.12124\n  Validation took: 0:00:02\n\n======== Epoch 3 / 8 ========\nTraining...\n","output_type":"stream"},{"name":"stderr","text":" 15%|█▍        | 24/164 [00:01<00:08, 16.49it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n 29%|██▉       | 48/164 [00:02<00:07, 16.46it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n 30%|███       | 50/164 [00:03<00:06, 16.37it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n 49%|████▉     | 80/164 [00:04<00:05, 16.48it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n 51%|█████     | 84/164 [00:05<00:04, 16.52it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n 52%|█████▏    | 86/164 [00:05<00:04, 16.46it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n 70%|██████▉   | 114/164 [00:06<00:03, 16.50it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n 71%|███████   | 116/164 [00:07<00:02, 16.47it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n 74%|███████▍  | 122/164 [00:07<00:02, 16.41it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n 80%|████████  | 132/164 [00:08<00:01, 16.51it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n 87%|████████▋ | 142/164 [00:08<00:01, 16.49it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n 90%|█████████ | 148/164 [00:08<00:00, 16.47it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n100%|██████████| 164/164 [00:09<00:00, 16.53it/s]\n","output_type":"stream"},{"name":"stdout","text":"  Average training loss: 0.15686\n  Training epoch took: 0:00:10\nRunning Validation...\n","output_type":"stream"},{"name":"stderr","text":" 28%|██▊       | 10/36 [00:00<00:01, 17.41it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n 94%|█████████▍| 34/36 [00:01<00:00, 17.04it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n100%|██████████| 36/36 [00:02<00:00, 17.54it/s]\n","output_type":"stream"},{"name":"stdout","text":"  Validation Loss: 0.11042\n  Validation took: 0:00:02\n\n======== Epoch 4 / 8 ========\nTraining...\n","output_type":"stream"},{"name":"stderr","text":"  5%|▍         | 8/164 [00:00<00:09, 16.65it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n  9%|▊         | 14/164 [00:00<00:08, 16.76it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n 10%|▉         | 16/164 [00:00<00:08, 16.62it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n 26%|██▌       | 42/164 [00:02<00:07, 16.49it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n 37%|███▋      | 60/164 [00:03<00:06, 16.50it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n 46%|████▋     | 76/164 [00:04<00:05, 16.42it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n 49%|████▉     | 80/164 [00:04<00:05, 16.44it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n 70%|██████▉   | 114/164 [00:06<00:03, 16.45it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n 80%|████████  | 132/164 [00:08<00:01, 16.38it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n 84%|████████▍ | 138/164 [00:08<00:01, 16.43it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n 93%|█████████▎| 152/164 [00:09<00:00, 16.52it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n100%|██████████| 164/164 [00:09<00:00, 16.53it/s]\n","output_type":"stream"},{"name":"stdout","text":"  Average training loss: 0.15094\n  Training epoch took: 0:00:10\nRunning Validation...\n","output_type":"stream"},{"name":"stderr","text":" 28%|██▊       | 10/36 [00:00<00:01, 16.82it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n 94%|█████████▍| 34/36 [00:02<00:00, 16.57it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n100%|██████████| 36/36 [00:02<00:00, 17.22it/s]\n","output_type":"stream"},{"name":"stdout","text":"  Validation Loss: 0.10229\n  Validation took: 0:00:02\n\n======== Epoch 5 / 8 ========\nTraining...\n","output_type":"stream"},{"name":"stderr","text":" 20%|█▉        | 32/164 [00:01<00:08, 16.39it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n 22%|██▏       | 36/164 [00:02<00:07, 16.35it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n 28%|██▊       | 46/164 [00:02<00:07, 16.09it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n 39%|███▉      | 64/164 [00:03<00:06, 16.31it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n 43%|████▎     | 70/164 [00:04<00:05, 16.31it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n 44%|████▍     | 72/164 [00:04<00:05, 16.30it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n 46%|████▋     | 76/164 [00:04<00:05, 16.29it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n 61%|██████    | 100/164 [00:06<00:04, 15.97it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n 62%|██████▏   | 102/164 [00:06<00:03, 16.02it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n 70%|██████▉   | 114/164 [00:07<00:03, 16.10it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n 95%|█████████▌| 156/164 [00:09<00:00, 15.98it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n 98%|█████████▊| 160/164 [00:09<00:00, 15.99it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n100%|██████████| 164/164 [00:10<00:00, 16.17it/s]\n","output_type":"stream"},{"name":"stdout","text":"  Average training loss: 0.14149\n  Training epoch took: 0:00:10\nRunning Validation...\n","output_type":"stream"},{"name":"stderr","text":" 28%|██▊       | 10/36 [00:00<00:01, 16.57it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n 94%|█████████▍| 34/36 [00:02<00:00, 16.37it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n100%|██████████| 36/36 [00:02<00:00, 16.85it/s]\n","output_type":"stream"},{"name":"stdout","text":"  Validation Loss: 0.09550\n  Validation took: 0:00:02\n\n======== Epoch 6 / 8 ========\nTraining...\n","output_type":"stream"},{"name":"stderr","text":"  5%|▍         | 8/164 [00:00<00:09, 15.68it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n  6%|▌         | 10/164 [00:00<00:09, 15.84it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n 21%|██        | 34/164 [00:02<00:08, 16.00it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n 23%|██▎       | 38/164 [00:02<00:07, 15.82it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n 43%|████▎     | 70/164 [00:04<00:05, 15.76it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n 46%|████▋     | 76/164 [00:04<00:05, 15.69it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n 49%|████▉     | 80/164 [00:05<00:05, 15.79it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n 70%|██████▉   | 114/164 [00:07<00:03, 15.77it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n 76%|███████▌  | 124/164 [00:07<00:02, 15.64it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n 84%|████████▍ | 138/164 [00:08<00:01, 15.51it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n 98%|█████████▊| 160/164 [00:10<00:00, 15.72it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n100%|██████████| 164/164 [00:10<00:00, 15.77it/s]\n","output_type":"stream"},{"name":"stdout","text":"  Average training loss: 0.13697\n  Training epoch took: 0:00:10\nRunning Validation...\n","output_type":"stream"},{"name":"stderr","text":" 28%|██▊       | 10/36 [00:00<00:01, 16.62it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n 94%|█████████▍| 34/36 [00:02<00:00, 16.43it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n100%|██████████| 36/36 [00:02<00:00, 16.86it/s]\n","output_type":"stream"},{"name":"stdout","text":"  Validation Loss: 0.08877\n  Validation took: 0:00:02\n\n======== Epoch 7 / 8 ========\nTraining...\n","output_type":"stream"},{"name":"stderr","text":" 15%|█▍        | 24/164 [00:01<00:09, 15.47it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n 21%|██        | 34/164 [00:02<00:08, 15.38it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n 29%|██▉       | 48/164 [00:03<00:07, 15.25it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n 34%|███▍      | 56/164 [00:03<00:07, 15.34it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n 51%|█████     | 84/164 [00:05<00:05, 15.42it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n 55%|█████▍    | 90/164 [00:05<00:04, 15.45it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n 59%|█████▊    | 96/164 [00:06<00:04, 15.21it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n 71%|███████   | 116/164 [00:07<00:03, 15.29it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n 72%|███████▏  | 118/164 [00:07<00:03, 15.22it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n 80%|████████  | 132/164 [00:08<00:02, 15.30it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n 88%|████████▊ | 144/164 [00:09<00:01, 15.19it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n100%|██████████| 164/164 [00:10<00:00, 15.36it/s]\n","output_type":"stream"},{"name":"stdout","text":"  Average training loss: 0.12961\n  Training epoch took: 0:00:11\nRunning Validation...\n","output_type":"stream"},{"name":"stderr","text":" 28%|██▊       | 10/36 [00:00<00:01, 16.22it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n 94%|█████████▍| 34/36 [00:02<00:00, 15.78it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n100%|██████████| 36/36 [00:02<00:00, 16.35it/s]\n","output_type":"stream"},{"name":"stdout","text":"  Validation Loss: 0.08350\n  Validation took: 0:00:02\n\n======== Epoch 8 / 8 ========\nTraining...\n","output_type":"stream"},{"name":"stderr","text":" 21%|██        | 34/164 [00:02<00:08, 15.05it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n 39%|███▉      | 64/164 [00:04<00:06, 14.94it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n 41%|████▏     | 68/164 [00:04<00:06, 14.91it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n 44%|████▍     | 72/164 [00:04<00:06, 14.96it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n 48%|████▊     | 78/164 [00:05<00:05, 14.85it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n 52%|█████▏    | 86/164 [00:05<00:05, 14.97it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n 55%|█████▍    | 90/164 [00:06<00:04, 14.93it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n 60%|█████▉    | 98/164 [00:06<00:04, 14.87it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n 61%|██████    | 100/164 [00:06<00:04, 14.81it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n 70%|██████▉   | 114/164 [00:07<00:03, 14.94it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n 71%|███████   | 116/164 [00:07<00:03, 14.88it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n 78%|███████▊  | 128/164 [00:08<00:02, 14.71it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n100%|██████████| 164/164 [00:10<00:00, 14.96it/s]\n","output_type":"stream"},{"name":"stdout","text":"  Average training loss: 0.13152\n  Training epoch took: 0:00:11\nRunning Validation...\n","output_type":"stream"},{"name":"stderr","text":" 28%|██▊       | 10/36 [00:00<00:01, 15.56it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n 94%|█████████▍| 34/36 [00:02<00:00, 15.16it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n100%|██████████| 36/36 [00:02<00:00, 15.68it/s]","output_type":"stream"},{"name":"stdout","text":"  Validation Loss: 0.08010\n  Validation took: 0:00:02\nTraining complete!\nTotal training took 0:01:40\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"# Define custom evaluation functions\ndef mean_squared_error(y_true, y_pred):\n    squared_errors = [(true - pred) ** 2 for true, pred in zip(y_true, y_pred)]\n    return sum(squared_errors) / len(squared_errors)\n\ndef mean_absolute_error(y_true, y_pred):\n    absolute_errors = [abs(true - pred) for true, pred in zip(y_true, y_pred)]\n    return sum(absolute_errors) / len(absolute_errors)\n\ndef root_mean_squared_error(y_true, y_pred):\n    mse = mean_squared_error(y_true, y_pred)\n    return mse ** 0.5\n\ndef pearsonr(x, y):\n    mean_x = sum(x) / len(x)\n    mean_y = sum(y) / len(y)\n    numerator = sum((xi - mean_x) * (yi - mean_y) for xi, yi in zip(x, y))\n    denominator = ((sum((xi - mean_x) ** 2 for xi in x) * sum((yi - mean_y) ** 2 for yi in y)) ** 0.5)\n    return numerator / denominator if denominator != 0 else 0.0","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-08T03:22:51.697033Z","iopub.execute_input":"2025-02-08T03:22:51.697340Z","iopub.status.idle":"2025-02-08T03:22:51.703140Z","shell.execute_reply.started":"2025-02-08T03:22:51.697305Z","shell.execute_reply":"2025-02-08T03:22:51.702327Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"# Evaluate Model with custom functions\ndef evaluate_model(model, dataloader):\n    model.eval()\n    true_labels = []\n    predicted_scores = []\n\n    with torch.no_grad():\n        for batch in dataloader:\n            data, labels = batch\n            data['input_ids'] = data['input_ids'].to(device)\n            data['attention_mask'] = data['attention_mask'].to(device)\n            predictions = model({\n                'input_ids': data['input_ids'],\n                'attention_mask': data['attention_mask']\n            })\n\n            true_labels.extend(labels.cpu().numpy())\n            predicted_scores.extend(predictions.cpu().numpy())\n\n    mse = mean_squared_error(true_labels, predicted_scores)\n    mae = mean_absolute_error(true_labels, predicted_scores)\n    rmse = root_mean_squared_error(true_labels, predicted_scores)\n    pearson_corr = pearsonr(true_labels, predicted_scores)\n\n    print(f\"Mean Squared Error (MSE): {mse:.4f}\")\n    print(f\"Mean Absolute Error (MAE): {mae:.4f}\")\n    print(f\"Root Mean Squared Error (RMSE): {rmse:.4f}\")\n    print(f\"Pearson Correlation: {pearson_corr:.4f}\")\n\n    return mse, mae, rmse, pearson_corr\n\n# Evaluate on validation set\nevaluate_model(model, validation_dataloader)\n\n# Optional: Evaluate on the test set if available\ntest_ds = CustomDataset(test_data)\ntest_dataloader = DataLoader(test_ds, batch_size=batch_size)\nprint(\"this is the evaluation on the test set:\")\nevaluate_model(model, test_dataloader)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-08T03:22:51.704966Z","iopub.execute_input":"2025-02-08T03:22:51.705251Z","iopub.status.idle":"2025-02-08T03:22:56.339710Z","shell.execute_reply.started":"2025-02-08T03:22:51.705230Z","shell.execute_reply":"2025-02-08T03:22:56.339046Z"}},"outputs":[{"name":"stderr","text":"Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","output_type":"stream"},{"name":"stdout","text":"Mean Squared Error (MSE): 0.0810\nMean Absolute Error (MAE): 0.2371\nRoot Mean Squared Error (RMSE): 0.2846\nPearson Correlation: 0.6651\nthis is the evaluation on the test set:\n","output_type":"stream"},{"name":"stderr","text":"Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","output_type":"stream"},{"name":"stdout","text":"Mean Squared Error (MSE): 0.0791\nMean Absolute Error (MAE): 0.2390\nRoot Mean Squared Error (RMSE): 0.2813\nPearson Correlation: 0.7001\n","output_type":"stream"},{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"(0.07910466631199645,\n 0.23903706051119647,\n 0.2812555178338666,\n 0.7001037946271274)"},"metadata":{}}],"execution_count":24},{"cell_type":"code","source":"# Save the trained model to a .pt file\nmodel_save_path = \"/kaggle/working/indobert_similarity_model.pt\"\ntorch.save(model.state_dict(), model_save_path)\nprint(f\"Model saved to {model_save_path}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-08T03:22:56.340701Z","iopub.execute_input":"2025-02-08T03:22:56.341012Z","iopub.status.idle":"2025-02-08T03:22:57.077367Z","shell.execute_reply.started":"2025-02-08T03:22:56.340987Z","shell.execute_reply":"2025-02-08T03:22:57.076489Z"}},"outputs":[{"name":"stdout","text":"Model saved to /kaggle/working/indobert_similarity_model.pt\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"# Reinitialize the model architecture\nmodel = EnhancedBertModel()\nmodel.to(device)\n\n# Load the model state dictionary\nmodel.load_state_dict(torch.load(model_save_path))\nprint(\"Model loaded successfully!\")\n\n# Set the model to evaluation mode if testing\nmodel.eval()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-08T03:22:57.078233Z","iopub.execute_input":"2025-02-08T03:22:57.078525Z","iopub.status.idle":"2025-02-08T03:22:58.413479Z","shell.execute_reply.started":"2025-02-08T03:22:57.078501Z","shell.execute_reply":"2025-02-08T03:22:58.412344Z"}},"outputs":[{"name":"stderr","text":"<ipython-input-26-a5f5a58a1944>:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load(model_save_path))\n","output_type":"stream"},{"name":"stdout","text":"Model loaded successfully!\n","output_type":"stream"},{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"EnhancedBertModel(\n  (bert): Transformer({'max_seq_length': 128, 'do_lower_case': False}) with Transformer model: BertModel \n  (pooling_layer): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n  (bi_lstm): LSTM(768, 64, batch_first=True, bidirectional=True)\n  (fc_dropout): Dropout(p=0.3, inplace=False)\n  (fc): Linear(in_features=256, out_features=1, bias=True)\n)"},"metadata":{}}],"execution_count":26},{"cell_type":"code","source":"def evaluate_and_save_results(model, dataloader, csv_filename, original_data):\n    model.eval()\n    results = []  # To store all prediction results\n    index = 0  # Track the original dataset index\n\n    with torch.no_grad():\n        for batch in tqdm(dataloader):\n            data, labels = batch\n            data['input_ids'] = data['input_ids'].to(device)\n            data['attention_mask'] = data['attention_mask'].to(device)\n            predictions = model({\n                'input_ids': data['input_ids'],\n                'attention_mask': data['attention_mask']\n            }).cpu().numpy()\n\n            batch_size = len(labels)\n            for i in range(batch_size):\n                # Get original sentences (response, answer)\n                response, answer, true_label = original_data[index]\n                index += 1  # Move to the next pair\n\n                results.append({\n                    'Response': response,\n                    'Answer': answer,\n                    'True Label': true_label,\n                    'Predicted Score': predictions[i]\n                })\n\n    # Save results to CSV\n    df = pd.DataFrame(results)\n    df.to_csv(csv_filename, index=False)\n    print(f\"Results saved to {csv_filename}\")\n    \n    return df\n\n# Save validation results\nvalidation_results_csv = \"/kaggle/working/validation_results.csv\"\ndf_validation = evaluate_and_save_results(model, validation_dataloader, validation_results_csv, val_data)\n\n# Load test data for testing\ntest_ds = CustomDataset(test_data)\ntest_dataloader = DataLoader(test_ds, batch_size=batch_size)\n\n# Save test results\ntest_results_csv = \"/kaggle/working/test_results.csv\"\ndf_test = evaluate_and_save_results(model, test_dataloader, test_results_csv, test_data)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-08T03:22:58.414845Z","iopub.execute_input":"2025-02-08T03:22:58.415203Z","iopub.status.idle":"2025-02-08T03:23:03.233021Z","shell.execute_reply.started":"2025-02-08T03:22:58.415170Z","shell.execute_reply":"2025-02-08T03:23:03.232060Z"}},"outputs":[{"name":"stderr","text":" 28%|██▊       | 10/36 [00:00<00:01, 15.07it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n 94%|█████████▍| 34/36 [00:02<00:00, 15.10it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n100%|██████████| 36/36 [00:02<00:00, 15.34it/s]\n","output_type":"stream"},{"name":"stdout","text":"Results saved to /kaggle/working/validation_results.csv\n","output_type":"stream"},{"name":"stderr","text":" 61%|██████    | 22/36 [00:01<00:00, 14.60it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n100%|██████████| 36/36 [00:02<00:00, 14.85it/s]","output_type":"stream"},{"name":"stdout","text":"Results saved to /kaggle/working/test_results.csv\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":27},{"cell_type":"code","source":"def test_single_data_point(model, tokenizer, sentence1, sentence2):\n    # Prepare input using the tokenizer\n    encoded_input = tokenizer(\n        sentence1,\n        sentence2,\n        padding=\"max_length\",\n        max_length=128,\n        truncation=True,\n        return_tensors=\"pt\"\n    ).to(device)\n\n    # Set the model to evaluation mode\n    model.eval()\n\n    with torch.no_grad():\n        # Perform inference\n        predicted_score = model({\n            'input_ids': encoded_input['input_ids'],\n            'attention_mask': encoded_input['attention_mask']\n        })\n\n    # Since the model output is normalized to [0, 1], rescale it to [0, 5]\n    predicted_score_rescaled = predicted_score.item() * 5.0\n\n    print(f\"Sentence 1: {sentence1}\")\n    print(f\"Sentence 2: {sentence2}\")\n    print(f\"Predicted Similarity Score: {predicted_score_rescaled:.4f}\")\n\n    return predicted_score_rescaled","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T12:52:28.313555Z","iopub.execute_input":"2025-01-19T12:52:28.313876Z","iopub.status.idle":"2025-01-19T12:52:28.319122Z","shell.execute_reply.started":"2025-01-19T12:52:28.313844Z","shell.execute_reply":"2025-01-19T12:52:28.318166Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Example test data point from the test set\ntest_sentence1 = test_data[0][0]  # Replace with the first sentence from your test data\ntest_sentence2 = test_data[0][1]  # Replace with the second sentence from your test data\ntrue_score = test_data[0][2] * 5.0  # Rescale the true label to the [0, 5] range for comparison\n\nprint(f\"True Similarity Score: {true_score:.4f}\")\npredicted_score = test_single_data_point(model, tokenizer, test_sentence1, test_sentence2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T12:52:28.320192Z","iopub.execute_input":"2025-01-19T12:52:28.320498Z","iopub.status.idle":"2025-01-19T12:52:28.350844Z","shell.execute_reply.started":"2025-01-19T12:52:28.320468Z","shell.execute_reply":"2025-01-19T12:52:28.350015Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Example test data point from the test set\ntest_sentence1 = test_data[5][0]  # Replace with the first sentence from your test data\ntest_sentence2 = test_data[5][1]  # Replace with the second sentence from your test data\ntrue_score = test_data[5][2] * 5.0  # Rescale the true label to the [0, 5] range for comparison\n\nprint(f\"True Similarity Score: {true_score:.4f}\")\npredicted_score = test_single_data_point(model, tokenizer, test_sentence1, test_sentence2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T12:52:28.35172Z","iopub.execute_input":"2025-01-19T12:52:28.352048Z","iopub.status.idle":"2025-01-19T12:52:28.369799Z","shell.execute_reply.started":"2025-01-19T12:52:28.352024Z","shell.execute_reply":"2025-01-19T12:52:28.368833Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import csv\n\n# Evaluate model on the unseen dataset and save results to CSV\ndef evaluate_and_save_results(model, dataloader, output_csv_path):\n    model.eval()\n    true_labels = []\n    predicted_scores = []\n    responses = []\n    answers = []\n\n    with torch.no_grad():\n        for batch in dataloader:\n            data, labels = batch\n            data['input_ids'] = data['input_ids'].to(device)\n            data['attention_mask'] = data['attention_mask'].to(device)\n            predictions = model({\n                'input_ids': data['input_ids'],\n                'attention_mask': data['attention_mask']\n            })\n\n            true_labels.extend(labels.cpu().numpy())\n            predicted_scores.extend(predictions.cpu().numpy())\n            responses.extend(data['input_ids'].cpu().numpy())  # Add the actual `response`\n            answers.extend(data['attention_mask'].cpu().numpy())  # Add the `answer`\n\n    # Calculate metrics\n    mse = mean_squared_error(true_labels, predicted_scores)\n    mae = mean_absolute_error(true_labels, predicted_scores)\n    rmse = root_mean_squared_error(true_labels, predicted_scores)\n    pearson_corr = pearsonr(true_labels, predicted_scores)\n\n    print(f\"\\nEvaluation on Unseen Dataset:\")\n    print(f\"Mean Squared Error (MSE): {mse:.4f}\")\n    print(f\"Mean Absolute Error (MAE): {mae:.4f}\")\n    print(f\"Root Mean Squared Error (RMSE): {rmse:.4f}\")\n    print(f\"Pearson Correlation: {pearson_corr:.4f}\")\n\n    # Save results to CSV\n    results = {\n        'answer': [tokenizer.decode(answers[i]) for i in range(len(answers))],\n        'response': [tokenizer.decode(responses[i]) for i in range(len(responses))],\n        'label': true_labels,\n        'predicted_label': predicted_scores,\n    }\n\n    df_results = pd.DataFrame(results)\n    df_results.to_csv(output_csv_path, index=False)\n    print(f\"Results saved to {output_csv_path}\")\n\n# Load unseen dataset and create DataLoader\nunseen_data = load_indo_dataset(\"/kaggle/input/testi-data/test-BuIng.csv\")\nunseen_dataset = CustomDataset(unseen_data)\nunseen_dataloader = DataLoader(unseen_dataset, batch_size=batch_size)\n\n# Evaluate and save to CSV\noutput_csv_path = \"/kaggle/working/unseen_dataset_results.csv\"\nevaluate_and_save_results(model, unseen_dataloader, output_csv_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-08T03:23:03.233874Z","iopub.execute_input":"2025-02-08T03:23:03.234264Z","iopub.status.idle":"2025-02-08T03:23:03.546701Z","shell.execute_reply.started":"2025-02-08T03:23:03.234224Z","shell.execute_reply":"2025-02-08T03:23:03.545821Z"}},"outputs":[{"name":"stdout","text":"                                              answer  \\\n0  animasi adalah sebuah proses merekam dan memai...   \n1  animasi adalah menghidupkan, yaitu usaha untuk...   \n2  animasi adalah sebuah proses merekam dan memai...   \n3  animasi adalah sebuah proses merekam dan memai...   \n4  animasi adalah menghidupkan, yaitu usaha untuk...   \n\n                                            response  label  \n0  animasi komputer adalah pembuatan atau pemrose...    2.5  \n1  animasi komputer merupakan sebuah bentuk seni ...    4.5  \n2  animasi yang dibuat pada saat sekarang dan dib...    2.5  \n3  sebuah animasi dimana animasi ini sebuah perge...    4.0  \n4    proses menciptakan gerakan menggunakan komputer    5.0  \n","output_type":"stream"},{"name":"stderr","text":"Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","output_type":"stream"},{"name":"stdout","text":"\nEvaluation on Unseen Dataset:\nMean Squared Error (MSE): 0.1031\nMean Absolute Error (MAE): 0.2725\nRoot Mean Squared Error (RMSE): 0.3211\nPearson Correlation: -0.4117\nResults saved to /kaggle/working/unseen_dataset_results.csv\n","output_type":"stream"}],"execution_count":28}]}