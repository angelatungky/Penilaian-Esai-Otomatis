{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pc4gpJsyvxVx",
        "outputId": "7a93bdee-adb8-48fa-d4c9-c15e8881b5d9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 1662.8/1662.8MB downloaded\n"
          ]
        }
      ],
      "source": [
        "from gensim.downloader import load as gensim_load\n",
        "\n",
        "# Load pre-trained embeddings\n",
        "word2vec = gensim_load('word2vec-google-news-300')  # Google News Word2Vec"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.svm import SVR\n",
        "from gensim.models import Word2Vec, FastText\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "import nltk"
      ],
      "metadata": {
        "id": "Q2AXVZr4xgzS"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure required NLTK data is downloaded\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-1YLNpCCx_cc",
        "outputId": "ff35d727-38df-45c8-f2b0-b39c8b237c6c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ====== Dataset Loading and Splitting ======\n",
        "def load_custom_dataset(filename):\n",
        "    data = []\n",
        "    with open(filename, \"r\") as file:\n",
        "        for line in file:\n",
        "            question, response, answer, label = line.strip().split('\\t')\n",
        "            label = float(label) / 5.0  # Normalize to [0, 1]\n",
        "            data.append((response, answer, label))\n",
        "    return data"
      ],
      "metadata": {
        "id": "KsQJNPPVyBIQ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split_dataset(data, valid_percentage, test_percentage):\n",
        "    length = len(data)\n",
        "    random.shuffle(data)\n",
        "    train = data[:int(length * (1 - valid_percentage - test_percentage))]\n",
        "    valid = data[int(length * (1 - valid_percentage - test_percentage)):int(length * (1 - test_percentage))]\n",
        "    test = data[int(length * (1 - test_percentage)):]\n",
        "    return train, valid, test"
      ],
      "metadata": {
        "id": "yGonDdPeyC6s"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ====== Text Preprocessing ======\n",
        "def preprocess_text(text):\n",
        "    # Normalize the text by replacing curly apostrophes with straight ones\n",
        "    text = text.replace(\"‘\", \"'\").replace(\"’\", \"'\").lower()  # Case folding and normalization\n",
        "\n",
        "    # Tokenize the text\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "\n",
        "    # Remove non-alphabetic tokens and stopwords\n",
        "    stop_words = set(nltk.corpus.stopwords.words(\"english\"))\n",
        "    tokens = [word for word in tokens if word.isalpha() and word not in stop_words]\n",
        "    return \" \".join(tokens)"
      ],
      "metadata": {
        "id": "XEo8aOabyGk1"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_data(data):\n",
        "    sentence1 = [preprocess_text(item[0]) for item in data]\n",
        "    sentence2 = [preprocess_text(item[1]) for item in data]\n",
        "    labels = [item[2] for item in data]\n",
        "    return sentence1, sentence2, labels"
      ],
      "metadata": {
        "id": "SFYPjPYQyHNt"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Function to save raw and preprocessed data\n",
        "def save_raw_and_preprocessed(raw_data, preprocessed_data1, preprocessed_data2, labels, filename):\n",
        "    # Convert to DataFrame\n",
        "    df = pd.DataFrame({\n",
        "        \"Raw Sentence 1\": [item[0] for item in raw_data],\n",
        "        \"Raw Sentence 2\": [item[1] for item in raw_data],\n",
        "        \"Preprocessed Sentence 1\": preprocessed_data1,\n",
        "        \"Preprocessed Sentence 2\": preprocessed_data2,\n",
        "        \"Label\": labels\n",
        "    })\n",
        "    df.to_csv(filename, index=False)\n",
        "    print(f\"Saved dataset to {filename}\")"
      ],
      "metadata": {
        "id": "UcNpAStXyLK1"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ====== Load Dataset ======\n",
        "# Load train, validation, and test datasets from CSV files\n",
        "train_file = \"/content/train_data_ENG-W2V.csv\"\n",
        "valid_file = \"/content/valid_data_ENG-W2V.csv\"\n",
        "test_file = \"/content/test_data_ENG-W2V.csv\"\n",
        "\n",
        "# Read datasets\n",
        "train_data = pd.read_csv(train_file).values\n",
        "valid_data = pd.read_csv(valid_file).values\n",
        "test_data = pd.read_csv(test_file).values\n",
        "\n",
        "# ====== Preprocessing ======\n",
        "x_train1, x_train2, y_train = preprocess_data(train_data)\n",
        "x_valid1, x_valid2, y_valid = preprocess_data(valid_data)\n",
        "x_test1, x_test2, y_test = preprocess_data(test_data)\n",
        "\n",
        "# Output shapes for verification\n",
        "print(f\"Train data: {len(x_train1)} pairs, {len(y_train)} labels\")\n",
        "print(f\"Validation data: {len(x_valid1)} pairs, {len(y_valid)} labels\")\n",
        "print(f\"Test data: {len(x_test1)} pairs, {len(y_test)} labels\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oQEQBTPayML6",
        "outputId": "61a44fb0-4fa0-427f-ba55-801857cfde89"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train data: 2916 pairs, 2916 labels\n",
            "Validation data: 365 pairs, 365 labels\n",
            "Test data: 365 pairs, 365 labels\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check a few random samples from the preprocessed training data\n",
        "print(\"Preprocessed x_train1 Samples:\")\n",
        "for i in random.sample(range(len(x_train1)), 5):  # Randomly select 5 indices\n",
        "    print(f\"Original Sentence 1: {train_data[i][0]}\")\n",
        "    print(f\"Preprocessed Sentence 1: {x_train1[i]}\")\n",
        "    print()\n",
        "\n",
        "print(\"Preprocessed x_train2 Samples:\")\n",
        "for i in random.sample(range(len(x_train2)), 5):  # Randomly select 5 indices\n",
        "    print(f\"Original Sentence 2: {train_data[i][1]}\")\n",
        "    print(f\"Preprocessed Sentence 2: {x_train2[i]}\")\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2LyOfWfHzJ42",
        "outputId": "682bef9f-57ac-456f-a8c7-a8a74639232d"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preprocessed x_train1 Samples:\n",
            "Original Sentence 1: a static array will store the new values that were assigned to each of its elements meaning if you call a function twice it will use the last values that were returned the first time if you do not declare it static then the new values will not be stored and will be reset to their original value\n",
            "Preprocessed Sentence 1: static array store new values assigned elements meaning call function twice use last values returned first time declare static new values stored reset original value\n",
            "\n",
            "Original Sentence 1: The address of the location in memory where the function code resides\n",
            "Preprocessed Sentence 1: address location memory function code resides\n",
            "\n",
            "Original Sentence 1: The compiler creates a default constructor when we do not define one in our class file\n",
            "Preprocessed Sentence 1: compiler creates default constructor define one class file\n",
            "\n",
            "Original Sentence 1: Insertion sort divides the list into sorted and unsorted regions then takes each item from the unsorted region and inserts it into its correct order in the sorted region\n",
            "Preprocessed Sentence 1: insertion sort divides list sorted unsorted regions takes item unsorted region inserts correct order sorted region\n",
            "\n",
            "Original Sentence 1: Overall the program has better performance - LRB - means it is faster - RRB - because it does not have to copy large amounts of data\n",
            "Preprocessed Sentence 1: overall program better performance lrb means faster rrb copy large amounts data\n",
            "\n",
            "Preprocessed x_train2 Samples:\n",
            "Original Sentence 2: not answered\n",
            "Preprocessed Sentence 2: answered\n",
            "\n",
            "Original Sentence 2: A pointer based implementation of a queue could use a linear linked list with two external pointers one to the front and one to the back\n",
            "Preprocessed Sentence 2: pointer based implementation queue could use linear linked list two external pointers one front one back\n",
            "\n",
            "Original Sentence 2: the insertion sort creates a new array and inserts each item in its place with respect to the new array\n",
            "Preprocessed Sentence 2: insertion sort creates new array inserts item place respect new array\n",
            "\n",
            "Original Sentence 2: Globally for an entire program and locally for individual functions - LRB - including FOR statements - RRB -\n",
            "Preprocessed Sentence 2: globally entire program locally individual functions lrb including statements rrb\n",
            "\n",
            "Original Sentence 2: A method with access to a linked list head pointer as access to the entire list\n",
            "Preprocessed Sentence 2: method access linked list head pointer access entire list\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def avgCos(sentences1, sentences2, embedding):\n",
        "    similarities = []\n",
        "    for sent1, sent2 in zip(sentences1, sentences2):\n",
        "        tokens1 = [token for token in sent1.split() if token in embedding.key_to_index]\n",
        "        tokens2 = [token for token in sent2.split() if token in embedding.key_to_index]\n",
        "\n",
        "        if not tokens1 or not tokens2:\n",
        "            similarities.append(0)\n",
        "            continue\n",
        "\n",
        "        vec1 = np.mean([embedding.get_vector(token) for token in tokens1], axis=0)\n",
        "        vec2 = np.mean([embedding.get_vector(token) for token in tokens2], axis=0)\n",
        "\n",
        "        similarities.append(np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2)))\n",
        "\n",
        "    return np.array(similarities).reshape(-1, 1)\n"
      ],
      "metadata": {
        "id": "cszVYjvbzWe3"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def word_distance(sentences1, sentences2, embedding):\n",
        "    sims = []\n",
        "    for sent1, sent2 in zip(sentences1, sentences2):\n",
        "        sent1_tokens = [token for token in sent1.split() if token in embedding.key_to_index]\n",
        "        sent2_tokens = [token for token in sent2.split() if token in embedding.key_to_index]\n",
        "\n",
        "        if not sent1_tokens or not sent2_tokens:\n",
        "            sims.append(0)\n",
        "        else:\n",
        "            sims.append(-embedding.wmdistance(sent1_tokens, sent2_tokens))  # Lower is better\n",
        "\n",
        "    return np.array(sims).reshape(-1, 1)\n"
      ],
      "metadata": {
        "id": "EtQVS5av3liT"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import TruncatedSVD\n",
        "\n",
        "def remove_first_principal_component(X):\n",
        "    svd = TruncatedSVD(n_components=1, random_state=42)\n",
        "    svd.fit(X)\n",
        "    pc = svd.components_\n",
        "    return X - X.dot(pc.T) * pc\n",
        "\n",
        "def sif_cos(sentences1, sentences2, embedding, freqs, a=0.001):\n",
        "    total_freq = sum(freqs.values())\n",
        "    embeddings = []\n",
        "\n",
        "    for sent1, sent2 in zip(sentences1, sentences2):\n",
        "        sent1_tokens = [token for token in sent1.split() if token in embedding.key_to_index]\n",
        "        sent2_tokens = [token for token in sent2.split() if token in embedding.key_to_index]\n",
        "\n",
        "        if not sent1_tokens or not sent2_tokens:\n",
        "            embeddings.extend([np.zeros(embedding.vector_size), np.zeros(embedding.vector_size)])\n",
        "            continue\n",
        "\n",
        "        weights1 = [a / (a + freqs.get(token, 1e-5) / total_freq) for token in sent1_tokens]\n",
        "        weights2 = [a / (a + freqs.get(token, 1e-5) / total_freq) for token in sent2_tokens]\n",
        "\n",
        "        embedding1 = np.average([embedding.get_vector(token) for token in sent1_tokens], axis=0, weights=weights1)\n",
        "        embedding2 = np.average([embedding.get_vector(token) for token in sent2_tokens], axis=0, weights=weights2)\n",
        "\n",
        "        embeddings.extend([embedding1, embedding2])\n",
        "\n",
        "    embeddings = np.array(embeddings)\n",
        "    embeddings = remove_first_principal_component(embeddings)\n",
        "\n",
        "    sims = [\n",
        "        (np.dot(embeddings[i], embeddings[i + 1]) /\n",
        "         (np.linalg.norm(embeddings[i]) * np.linalg.norm(embeddings[i + 1]))\n",
        "         if np.linalg.norm(embeddings[i]) > 0 and np.linalg.norm(embeddings[i + 1]) > 0 else 0)\n",
        "        for i in range(0, len(embeddings), 2)\n",
        "    ]\n",
        "\n",
        "    return np.array(sims).reshape(-1, 1)\n"
      ],
      "metadata": {
        "id": "zWBIrTXM3nN-"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ====== Regression Model ======\n",
        "class RegressionModel:\n",
        "    def __init__(self, model_type=\"linear\"):\n",
        "        if model_type == \"linear\":\n",
        "            self.model = self.LinearRegressionCustom()\n",
        "        elif model_type == \"svr\":\n",
        "            self.model = SVR(kernel=\"linear\")\n",
        "        elif model_type == \"rfr\":\n",
        "            self.model = self.RandomForestCustom()\n",
        "        else:\n",
        "            raise ValueError(\"Unsupported model type.\")\n",
        "\n",
        "    class LinearRegressionCustom:\n",
        "        def __init__(self):\n",
        "            self.weights = None\n",
        "\n",
        "        def fit(self, X, y):\n",
        "            X = np.c_[np.ones(X.shape[0]), X]  # Add bias term\n",
        "            self.weights = np.linalg.pinv(X.T @ X) @ X.T @ y\n",
        "        def fit(self, X, y):\n",
        "            # Convert y to NumPy and ensure matching rows\n",
        "            y = np.array(y)\n",
        "            if X.shape[0] != y.shape[0]:\n",
        "                raise ValueError(f\"Shape mismatch: X has {X.shape[0]} rows but y has {y.shape[0]} rows.\")\n",
        "\n",
        "            X = np.c_[np.ones(X.shape[0]), X]  # Add bias term\n",
        "            self.weights = np.linalg.pinv(X.T @ X) @ X.T @ y\n",
        "\n",
        "        def predict(self, X):\n",
        "            X = np.c_[np.ones(X.shape[0]), X]  # Add bias term\n",
        "            return X @ self.weights\n",
        "\n",
        "    class RandomForestCustom:\n",
        "        def __init__(self, n_estimators=100, max_depth=None):\n",
        "            self.n_estimators = n_estimators\n",
        "            self.max_depth = max_depth\n",
        "            self.trees = []\n",
        "\n",
        "        def fit(self, X, y):\n",
        "            from sklearn.tree import DecisionTreeRegressor\n",
        "\n",
        "            # Ensure y is a NumPy array\n",
        "            y = np.array(y)\n",
        "\n",
        "            n_samples = X.shape[0]\n",
        "\n",
        "            for _ in range(self.n_estimators):\n",
        "                # Ensure indices are integers for proper indexing\n",
        "                indices = np.random.choice(range(n_samples), size=n_samples, replace=True)\n",
        "                X_sample = X[indices]\n",
        "                y_sample = y[indices]\n",
        "                tree = DecisionTreeRegressor(max_depth=self.max_depth)\n",
        "                tree.fit(X_sample, y_sample)\n",
        "                self.trees.append(tree)\n",
        "\n",
        "        def predict(self, X):\n",
        "            # Aggregate predictions from all trees\n",
        "            predictions = np.array([tree.predict(X) for tree in self.trees])\n",
        "            return np.mean(predictions, axis=0)\n",
        "\n",
        "    @staticmethod\n",
        "    def mean_squared_error(y_true, y_pred):\n",
        "        squared_errors = [(true - pred) ** 2 for true, pred in zip(y_true, y_pred)]\n",
        "        return sum(squared_errors) / len(squared_errors)\n",
        "\n",
        "    @staticmethod\n",
        "    def mean_absolute_error(y_true, y_pred):\n",
        "        absolute_errors = [abs(true - pred) for true, pred in zip(y_true, y_pred)]\n",
        "        return sum(absolute_errors) / len(absolute_errors)\n",
        "\n",
        "    @staticmethod\n",
        "    def pearsonr(x, y):\n",
        "        mean_x = sum(x) / len(x)\n",
        "        mean_y = sum(y) / len(y)\n",
        "        numerator = sum((xi - mean_x) * (yi - mean_y) for xi, yi in zip(x, y))\n",
        "        denominator = ((sum((xi - mean_x) ** 2 for xi in x) * sum((yi - mean_y) ** 2 for yi in y)) ** 0.5)\n",
        "        return (numerator / denominator if denominator != 0 else 0.0, None)\n",
        "\n",
        "    def train(self, x_train, y_train):\n",
        "        self.model.fit(x_train, y_train)\n",
        "\n",
        "    def evaluate(self, x, y):\n",
        "        predictions = self.model.predict(x)\n",
        "        mse = self.mean_squared_error(y, predictions)\n",
        "        mae = self.mean_absolute_error(y, predictions)\n",
        "        pearson_corr, _ = self.pearsonr(y, predictions)\n",
        "        return mse, mae, pearson_corr\n",
        "\n",
        "    def predict(self, x):\n",
        "        return self.model.predict(x)"
      ],
      "metadata": {
        "id": "APRtR-L51iF7"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def feature_extraction(train_set1, train_set2, val_set1, val_set2, test_set, embed_model, frequency_map, method):\n",
        "    if method == \"averageCosine\":\n",
        "        train_similarities = average_cosine_similarity(train_set1, train_set2, embed_model)\n",
        "        val_similarities = average_cosine_similarity(val_set1, val_set2, embed_model)\n",
        "        test_similarities = average_cosine_similarity(test_set[\"sentence1\"], test_set[\"sentence2\"], embed_model)\n",
        "    elif method == \"wordDis\":\n",
        "        train_similarities = word_distance(train_set1, train_set2, embed_model)\n",
        "        val_similarities = word_distance(val_set1, val_set2, embed_model)\n",
        "        test_similarities = word_distance(test_set[\"sentence1\"], test_set[\"sentence2\"], embed_model)\n",
        "    elif method == \"sifCos\":\n",
        "        train_similarities = sif_cos(train_set1, train_set2, embed_model, frequency_map)\n",
        "        val_similarities = sif_cos(val_set1, val_set2, embed_model, frequency_map)\n",
        "        test_similarities = sif_cos(test_set[\"sentence1\"], test_set[\"sentence2\"], embed_model, frequency_map)\n",
        "    else:\n",
        "        raise ValueError(f\"Feature extraction method '{method}' is not supported.\")\n",
        "\n",
        "    return np.array(train_similarities), np.array(val_similarities), np.array(test_similarities)"
      ],
      "metadata": {
        "id": "VEvQjrob10j-"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ====== Testing Predictions ======\n",
        "def print_test_predictions(model, x_test_features, x_test1, x_test2, y_test):\n",
        "    predictions = model.predict(x_test_features)\n",
        "    true_scores = np.array(y_test) * 5.0\n",
        "    predicted_scores = predictions * 5.0\n",
        "    pearson_corr, _ = RegressionModel.pearsonr(true_scores, predicted_scores)\n",
        "    results = pd.DataFrame({\n",
        "        \"Sentence 1\": x_test1,\n",
        "        \"Sentence 2\": x_test2,\n",
        "        \"True Similarity Score\": true_scores,\n",
        "        \"Predicted Similarity Score\": predicted_scores\n",
        "    })\n",
        "    print(results.head(10))\n",
        "    print(f\"Pearson Correlation: {pearson_corr:.4f}\")\n",
        "    results.to_csv(\"train_predictions.csv\", index=False)"
      ],
      "metadata": {
        "id": "cATyqebu12--"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install cvxopt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CQGogbya15B6",
        "outputId": "2a9ed398-ce04-4b37-fb66-71fb71fe0ff9"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: cvxopt in /usr/local/lib/python3.11/dist-packages (1.3.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install POT"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BmuewC4I16d5",
        "outputId": "ff4251d8-131f-40b0-9774-6e346adc7d8d"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting POT\n",
            "  Downloading POT-0.9.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (34 kB)\n",
            "Requirement already satisfied: numpy>=1.16 in /usr/local/lib/python3.11/dist-packages (from POT) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.6 in /usr/local/lib/python3.11/dist-packages (from POT) (1.13.1)\n",
            "Downloading POT-0.9.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (897 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.5/897.5 kB\u001b[0m \u001b[31m23.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: POT\n",
            "Successfully installed POT-0.9.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# ====== Frequency Computation for SIF ======\n",
        "from collections import Counter\n",
        "all_sentences = x_train1 + x_train2 + x_valid1 + x_valid2 + x_test1 + x_test2\n",
        "# Compute word frequencies for SIF\n",
        "all_tokens = [token for sentence in all_sentences for token in sentence.split()]\n",
        "freqs = Counter(all_tokens)"
      ],
      "metadata": {
        "id": "j2eLL5sT1752"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if \"king\" in word2vec.key_to_index:\n",
        "    print(\"Word exists in Word2Vec model!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bryLcmTP3SOf",
        "outputId": "e3f11f82-c328-4795-e7be-3f830edd502a"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word exists in Word2Vec model!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word2vec[\"king\"]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i2VIikv53TOX",
        "outputId": "57e769a8-2a9d-451f-f276-f260fd7a661d"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 1.25976562e-01,  2.97851562e-02,  8.60595703e-03,  1.39648438e-01,\n",
              "       -2.56347656e-02, -3.61328125e-02,  1.11816406e-01, -1.98242188e-01,\n",
              "        5.12695312e-02,  3.63281250e-01, -2.42187500e-01, -3.02734375e-01,\n",
              "       -1.77734375e-01, -2.49023438e-02, -1.67968750e-01, -1.69921875e-01,\n",
              "        3.46679688e-02,  5.21850586e-03,  4.63867188e-02,  1.28906250e-01,\n",
              "        1.36718750e-01,  1.12792969e-01,  5.95703125e-02,  1.36718750e-01,\n",
              "        1.01074219e-01, -1.76757812e-01, -2.51953125e-01,  5.98144531e-02,\n",
              "        3.41796875e-01, -3.11279297e-02,  1.04492188e-01,  6.17675781e-02,\n",
              "        1.24511719e-01,  4.00390625e-01, -3.22265625e-01,  8.39843750e-02,\n",
              "        3.90625000e-02,  5.85937500e-03,  7.03125000e-02,  1.72851562e-01,\n",
              "        1.38671875e-01, -2.31445312e-01,  2.83203125e-01,  1.42578125e-01,\n",
              "        3.41796875e-01, -2.39257812e-02, -1.09863281e-01,  3.32031250e-02,\n",
              "       -5.46875000e-02,  1.53198242e-02, -1.62109375e-01,  1.58203125e-01,\n",
              "       -2.59765625e-01,  2.01416016e-02, -1.63085938e-01,  1.35803223e-03,\n",
              "       -1.44531250e-01, -5.68847656e-02,  4.29687500e-02, -2.46582031e-02,\n",
              "        1.85546875e-01,  4.47265625e-01,  9.58251953e-03,  1.31835938e-01,\n",
              "        9.86328125e-02, -1.85546875e-01, -1.00097656e-01, -1.33789062e-01,\n",
              "       -1.25000000e-01,  2.83203125e-01,  1.23046875e-01,  5.32226562e-02,\n",
              "       -1.77734375e-01,  8.59375000e-02, -2.18505859e-02,  2.05078125e-02,\n",
              "       -1.39648438e-01,  2.51464844e-02,  1.38671875e-01, -1.05468750e-01,\n",
              "        1.38671875e-01,  8.88671875e-02, -7.51953125e-02, -2.13623047e-02,\n",
              "        1.72851562e-01,  4.63867188e-02, -2.65625000e-01,  8.91113281e-03,\n",
              "        1.49414062e-01,  3.78417969e-02,  2.38281250e-01, -1.24511719e-01,\n",
              "       -2.17773438e-01, -1.81640625e-01,  2.97851562e-02,  5.71289062e-02,\n",
              "       -2.89306641e-02,  1.24511719e-02,  9.66796875e-02, -2.31445312e-01,\n",
              "        5.81054688e-02,  6.68945312e-02,  7.08007812e-02, -3.08593750e-01,\n",
              "       -2.14843750e-01,  1.45507812e-01, -4.27734375e-01, -9.39941406e-03,\n",
              "        1.54296875e-01, -7.66601562e-02,  2.89062500e-01,  2.77343750e-01,\n",
              "       -4.86373901e-04, -1.36718750e-01,  3.24218750e-01, -2.46093750e-01,\n",
              "       -3.03649902e-03, -2.11914062e-01,  1.25000000e-01,  2.69531250e-01,\n",
              "        2.04101562e-01,  8.25195312e-02, -2.01171875e-01, -1.60156250e-01,\n",
              "       -3.78417969e-02, -1.20117188e-01,  1.15234375e-01, -4.10156250e-02,\n",
              "       -3.95507812e-02, -8.98437500e-02,  6.34765625e-03,  2.03125000e-01,\n",
              "        1.86523438e-01,  2.73437500e-01,  6.29882812e-02,  1.41601562e-01,\n",
              "       -9.81445312e-02,  1.38671875e-01,  1.82617188e-01,  1.73828125e-01,\n",
              "        1.73828125e-01, -2.37304688e-01,  1.78710938e-01,  6.34765625e-02,\n",
              "        2.36328125e-01, -2.08984375e-01,  8.74023438e-02, -1.66015625e-01,\n",
              "       -7.91015625e-02,  2.43164062e-01, -8.88671875e-02,  1.26953125e-01,\n",
              "       -2.16796875e-01, -1.73828125e-01, -3.59375000e-01, -8.25195312e-02,\n",
              "       -6.49414062e-02,  5.07812500e-02,  1.35742188e-01, -7.47070312e-02,\n",
              "       -1.64062500e-01,  1.15356445e-02,  4.45312500e-01, -2.15820312e-01,\n",
              "       -1.11328125e-01, -1.92382812e-01,  1.70898438e-01, -1.25000000e-01,\n",
              "        2.65502930e-03,  1.92382812e-01, -1.74804688e-01,  1.39648438e-01,\n",
              "        2.92968750e-01,  1.13281250e-01,  5.95703125e-02, -6.39648438e-02,\n",
              "        9.96093750e-02, -2.72216797e-02,  1.96533203e-02,  4.27246094e-02,\n",
              "       -2.46093750e-01,  6.39648438e-02, -2.25585938e-01, -1.68945312e-01,\n",
              "        2.89916992e-03,  8.20312500e-02,  3.41796875e-01,  4.32128906e-02,\n",
              "        1.32812500e-01,  1.42578125e-01,  7.61718750e-02,  5.98144531e-02,\n",
              "       -1.19140625e-01,  2.74658203e-03, -6.29882812e-02, -2.72216797e-02,\n",
              "       -4.82177734e-03, -8.20312500e-02, -2.49023438e-02, -4.00390625e-01,\n",
              "       -1.06933594e-01,  4.24804688e-02,  7.76367188e-02, -1.16699219e-01,\n",
              "        7.37304688e-02, -9.22851562e-02,  1.07910156e-01,  1.58203125e-01,\n",
              "        4.24804688e-02,  1.26953125e-01,  3.61328125e-02,  2.67578125e-01,\n",
              "       -1.01074219e-01, -3.02734375e-01, -5.76171875e-02,  5.05371094e-02,\n",
              "        5.26428223e-04, -2.07031250e-01, -1.38671875e-01, -8.97216797e-03,\n",
              "       -2.78320312e-02, -1.41601562e-01,  2.07031250e-01, -1.58203125e-01,\n",
              "        1.27929688e-01,  1.49414062e-01, -2.24609375e-02, -8.44726562e-02,\n",
              "        1.22558594e-01,  2.15820312e-01, -2.13867188e-01, -3.12500000e-01,\n",
              "       -3.73046875e-01,  4.08935547e-03,  1.07421875e-01,  1.06933594e-01,\n",
              "        7.32421875e-02,  8.97216797e-03, -3.88183594e-02, -1.29882812e-01,\n",
              "        1.49414062e-01, -2.14843750e-01, -1.83868408e-03,  9.91210938e-02,\n",
              "        1.57226562e-01, -1.14257812e-01, -2.05078125e-01,  9.91210938e-02,\n",
              "        3.69140625e-01, -1.97265625e-01,  3.54003906e-02,  1.09375000e-01,\n",
              "        1.31835938e-01,  1.66992188e-01,  2.35351562e-01,  1.04980469e-01,\n",
              "       -4.96093750e-01, -1.64062500e-01, -1.56250000e-01, -5.22460938e-02,\n",
              "        1.03027344e-01,  2.43164062e-01, -1.88476562e-01,  5.07812500e-02,\n",
              "       -9.37500000e-02, -6.68945312e-02,  2.27050781e-02,  7.61718750e-02,\n",
              "        2.89062500e-01,  3.10546875e-01, -5.37109375e-02,  2.28515625e-01,\n",
              "        2.51464844e-02,  6.78710938e-02, -1.21093750e-01, -2.15820312e-01,\n",
              "       -2.73437500e-01, -3.07617188e-02, -3.37890625e-01,  1.53320312e-01,\n",
              "        2.33398438e-01, -2.08007812e-01,  3.73046875e-01,  8.20312500e-02,\n",
              "        2.51953125e-01, -7.61718750e-02, -4.66308594e-02, -2.23388672e-02,\n",
              "        2.99072266e-02, -5.93261719e-02, -4.66918945e-03, -2.44140625e-01,\n",
              "       -2.09960938e-01, -2.87109375e-01, -4.54101562e-02, -1.77734375e-01,\n",
              "       -2.79296875e-01, -8.59375000e-02,  9.13085938e-02,  2.51953125e-01],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "# Compute word frequencies\n",
        "all_sentences = x_train1 + x_train2 + x_valid1 + x_valid2 + x_test1 + x_test2\n",
        "all_tokens = [token for sentence in all_sentences for token in sentence.split()]\n",
        "freqs = Counter(all_tokens)\n"
      ],
      "metadata": {
        "id": "jix5qgSa2MWo"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract Features\n",
        "x_train_features_avg = avgCos(x_train1, x_train2, word2vec)\n",
        "x_valid_features_avg = avgCos(x_valid1, x_valid2, word2vec)\n",
        "x_test_features_avg = avgCos(x_test1, x_test2, word2vec)\n",
        "\n",
        "x_train_features_wmd = word_distance(x_train1, x_train2, word2vec)\n",
        "x_valid_features_wmd = word_distance(x_valid1, x_valid2, word2vec)\n",
        "x_test_features_wmd = word_distance(x_test1, x_test2, word2vec)\n",
        "\n",
        "x_train_features_sif = sif_cos(x_train1, x_train2, word2vec, freqs)\n",
        "x_valid_features_sif = sif_cos(x_valid1, x_valid2, word2vec, freqs)\n",
        "x_test_features_sif = sif_cos(x_test1, x_test2, word2vec, freqs)\n"
      ],
      "metadata": {
        "id": "_KGsEnMY3z1R"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from scipy.stats import pearsonr\n",
        "from gensim.downloader import load as gensim_load\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "from collections import Counter\n",
        "from sklearn.decomposition import TruncatedSVD"
      ],
      "metadata": {
        "id": "EXH2ugtU4eyy"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature Extraction Methods\n",
        "feature_methods = {\n",
        "    \"avgCos\": {\"train\": x_train_features_avg, \"valid\": x_valid_features_avg, \"test\": x_test_features_avg},\n",
        "    \"wordDis\": {\"train\": x_train_features_wmd, \"valid\": x_valid_features_wmd, \"test\": x_test_features_wmd},\n",
        "    \"sifCos\": {\"train\": x_train_features_sif, \"valid\": x_valid_features_sif, \"test\": x_test_features_sif}\n",
        "}\n",
        "\n",
        "# Train and Evaluate Models\n",
        "models = {\"linear\": RegressionModel(\"linear\"), \"svr\": RegressionModel(\"svr\"), \"rfr\": RegressionModel(\"rfr\")}\n",
        "results = []\n",
        "\n",
        "for feature_name, feature_data in feature_methods.items():\n",
        "    for model_name, model in models.items():\n",
        "        model.train(feature_data[\"train\"], y_train)\n",
        "        val_mse, val_mae, val_pearson = model.evaluate(feature_data[\"valid\"], y_valid)\n",
        "        test_mse, test_mae, test_pearson = model.evaluate(feature_data[\"test\"], y_test)\n",
        "\n",
        "        print(f\"Validation MSE ({feature_name}-{model_name}): {val_mse:.4f}, MAE: {val_mae:.4f}, Pearson: {val_pearson:.4f}\")\n",
        "        print(f\"Test MSE ({feature_name}-{model_name}): {test_mse:.4f}, MAE: {test_mae:.4f}, Pearson: {test_pearson:.4f}\")\n",
        "\n",
        "        predictions = model.predict(feature_data[\"test\"])\n",
        "        results.append({\n",
        "            \"feature\": feature_name,\n",
        "            \"model\": model_name,\n",
        "            \"test_predictions\": predictions,\n",
        "            \"x_test1\": x_test1,\n",
        "            \"x_test2\": x_test2,\n",
        "            \"y_test\": y_test\n",
        "        })\n",
        "\n",
        "# Save all results as CSV files\n",
        "for i, result in enumerate(results, start=1):\n",
        "    raw_test1 = [item[0] for item in test_data]\n",
        "    raw_test2 = [item[1] for item in test_data]\n",
        "    test_df = pd.DataFrame({\n",
        "        \"Original Sentence 1\": raw_test1,\n",
        "        \"Original Sentence 2\": raw_test2,\n",
        "        \"Preprocessed Sentence 1\": result[\"x_test1\"],\n",
        "        \"Preprocessed Sentence 2\": result[\"x_test2\"],\n",
        "        \"True Similarity Score\": [y * 5 for y in result[\"y_test\"]],\n",
        "        \"Predicted Similarity Score\": [y * 5 for y in result[\"test_predictions\"]]\n",
        "    })\n",
        "    test_filename = f\"test_result_{i}_{result['feature']}_{result['model']}.csv\"\n",
        "    test_df.to_csv(test_filename, index=False)\n",
        "    print(f\"Saved test result: {test_filename}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QmS9W5Lg4PKD",
        "outputId": "50da03e0-9cc5-4cf2-c29e-b9d15608d38a"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation MSE (avgCos-linear): 0.0427, MAE: 0.1633, Pearson: 0.4024\n",
            "Test MSE (avgCos-linear): 0.0462, MAE: 0.1672, Pearson: 0.4103\n",
            "Validation MSE (avgCos-svr): 0.0425, MAE: 0.1574, Pearson: 0.4024\n",
            "Test MSE (avgCos-svr): 0.0474, MAE: 0.1649, Pearson: 0.4103\n",
            "Validation MSE (avgCos-rfr): 0.0486, MAE: 0.1661, Pearson: 0.4275\n",
            "Test MSE (avgCos-rfr): 0.0504, MAE: 0.1672, Pearson: 0.4521\n",
            "Validation MSE (wordDis-linear): 0.0438, MAE: 0.1681, Pearson: 0.3811\n",
            "Test MSE (wordDis-linear): 0.0483, MAE: 0.1710, Pearson: 0.3619\n",
            "Validation MSE (wordDis-svr): 0.0433, MAE: 0.1615, Pearson: 0.3811\n",
            "Test MSE (wordDis-svr): 0.0501, MAE: 0.1703, Pearson: 0.3619\n",
            "Validation MSE (wordDis-rfr): 0.0446, MAE: 0.1748, Pearson: 0.3875\n",
            "Test MSE (wordDis-rfr): 0.0482, MAE: 0.1801, Pearson: 0.3705\n",
            "Validation MSE (sifCos-linear): 0.0480, MAE: 0.1734, Pearson: 0.2639\n",
            "Test MSE (sifCos-linear): 0.0509, MAE: 0.1728, Pearson: 0.2917\n",
            "Validation MSE (sifCos-svr): 0.0475, MAE: 0.1651, Pearson: 0.2639\n",
            "Test MSE (sifCos-svr): 0.0530, MAE: 0.1717, Pearson: 0.2917\n",
            "Validation MSE (sifCos-rfr): 0.0612, MAE: 0.2003, Pearson: 0.0609\n",
            "Test MSE (sifCos-rfr): 0.0558, MAE: 0.1824, Pearson: 0.2371\n",
            "Saved test result: test_result_1_avgCos_linear.csv\n",
            "Saved test result: test_result_2_avgCos_svr.csv\n",
            "Saved test result: test_result_3_avgCos_rfr.csv\n",
            "Saved test result: test_result_4_wordDis_linear.csv\n",
            "Saved test result: test_result_5_wordDis_svr.csv\n",
            "Saved test result: test_result_6_wordDis_rfr.csv\n",
            "Saved test result: test_result_7_sifCos_linear.csv\n",
            "Saved test result: test_result_8_sifCos_svr.csv\n",
            "Saved test result: test_result_9_sifCos_rfr.csv\n"
          ]
        }
      ]
    }
  ]
}